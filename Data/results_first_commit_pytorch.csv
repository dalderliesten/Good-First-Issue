First Commit Hash (SHA),Commit Message,Author (Github Name),Link to Commit
053065ba239e3421d1b2cbec55e3f177a21d6829,initial revamp of torch7 tree,andresy,https://api.github.com/repos/pytorch/pytorch/git/commits/053065ba239e3421d1b2cbec55e3f177a21d6829
ade0d0e1f22811cd42af33dd7c6afe4e3c16ea60,First basic skeleton for package manager,clementfarabet,https://api.github.com/repos/pytorch/pytorch/git/commits/ade0d0e1f22811cd42af33dd7c6afe4e3c16ea60
426a2b19670c7e95b3c59008970ca12e331908fb,"added maskedSelect method to select elements using a mask vector
documentation changes for Tensor",koraykv,https://api.github.com/repos/pytorch/pytorch/git/commits/426a2b19670c7e95b3c59008970ca12e331908fb
9e099237d3e863ef1b56aa82fc75a547ffae6aa0,Add NEON assembly routine for ARM processor,arnaudchauveur,https://api.github.com/repos/pytorch/pytorch/git/commits/9e099237d3e863ef1b56aa82fc75a547ffae6aa0
9766daa1b211b0dd403d9764b0e05fceb03ddf5c,"Fix bad string.format usage in pkg/torch/File

* .. should be ,
* wrapped className with tostring, in case it is nil",pflaquerre,https://api.github.com/repos/pytorch/pytorch/git/commits/9766daa1b211b0dd403d9764b0e05fceb03ddf5c
43fead371b4ecf844ec0c69003ee908c8e30dc5c,added cuda convolutionMap,soumith,https://api.github.com/repos/pytorch/pytorch/git/commits/43fead371b4ecf844ec0c69003ee908c8e30dc5c
84a5bcb17a07bd28659b85ad50901217894c77b5,Allow running subset of tests,jtbates,https://api.github.com/repos/pytorch/pytorch/git/commits/84a5bcb17a07bd28659b85ad50901217894c77b5
87ecea3154168d53295b141a8b4ff8e66df8d83c,"Added missing form of CudaTensor:add

This adds the form that performs x:add(y,s,z), which was previously missing.",akfidjeland,https://api.github.com/repos/pytorch/pytorch/git/commits/87ecea3154168d53295b141a8b4ff8e66df8d83c
8ed97e6961ed7247bc785120b42cf03af2ccf7f2,Using a local var.,fidlej,https://api.github.com/repos/pytorch/pytorch/git/commits/8ed97e6961ed7247bc785120b42cf03af2ccf7f2
526def20a0f38c799b1a2dd78d7e32c49f2120f8,Allow negative step with range,jucor,https://api.github.com/repos/pytorch/pytorch/git/commits/526def20a0f38c799b1a2dd78d7e32c49f2120f8
962e1df4a7035141ee115e7d2543dc2db0f92fce,luaopen_xxx functions need LUA_EXTERNC,leonbottou,https://api.github.com/repos/pytorch/pytorch/git/commits/962e1df4a7035141ee115e7d2543dc2db0f92fce
6cf948cd0ff036f1d0ce3c43641731b823f8ad01,Add more tests for torch.sort(),d11,https://api.github.com/repos/pytorch/pytorch/git/commits/6cf948cd0ff036f1d0ce3c43641731b823f8ad01
1dc0c1812178455d8c3fcf88fbf2c942a3bf7c10,"Refactored TH to not use global state for random number generation.

* Style / naming needs work.
* Breaks other layers within torch",timharley,https://api.github.com/repos/pytorch/pytorch/git/commits/1dc0c1812178455d8c3fcf88fbf2c942a3bf7c10
2bf674bebd069c70bb38eba95cfd6475e8a97926,Implement method abs() for LongTensor and IntTensor,sergomezcol,https://api.github.com/repos/pytorch/pytorch/git/commits/2bf674bebd069c70bb38eba95cfd6475e8a97926
b5744ef1a34df3b7256f6876d59c0fcab4b1174c,multinomial with replacement,nicholas-leonard,https://api.github.com/repos/pytorch/pytorch/git/commits/b5744ef1a34df3b7256f6876d59c0fcab4b1174c
60b537444044378d929ba5681c3a7d31c2a876a1,Updated,ajtulloch,https://api.github.com/repos/pytorch/pytorch/git/commits/60b537444044378d929ba5681c3a7d31c2a876a1
81c58afbde774a129845ac785fdf3a5908a78b57,Support copy of large non contiguous tensors,IoannisAntonoglou,https://api.github.com/repos/pytorch/pytorch/git/commits/81c58afbde774a129845ac785fdf3a5908a78b57
b8cbe0700eb9f2369d5bd8a06042024b051c7103,optimized indexSelect,hycis,https://api.github.com/repos/pytorch/pytorch/git/commits/b8cbe0700eb9f2369d5bd8a06042024b051c7103
328c4d930cbdcf26548a5af2c9feac236ea32c9d,No need to search for gfortran,tudor,https://api.github.com/repos/pytorch/pytorch/git/commits/328c4d930cbdcf26548a5af2c9feac236ea32c9d
1e156b6ba7de522e8101ceae48dc215f6ad1d364,"Tensor:cdiv with three arguments

Torch tensors support the cdiv operation with both two and three arguments.
cutorch only supports the two-argument (in-place) version. This patch extends it
to also support the three-argument version and adds a test for it. The
implementation is very similar to cmul which already supports both two and three
arguments.",dominikgrewe,https://api.github.com/repos/pytorch/pytorch/git/commits/1e156b6ba7de522e8101ceae48dc215f6ad1d364
8cae227d6bee69344e2cb01a7d7a3a5a9a156108,peer to peer access,szagoruyko,https://api.github.com/repos/pytorch/pytorch/git/commits/8cae227d6bee69344e2cb01a7d7a3a5a9a156108
ce8ba6dc824558340c99da7526f0b251870cab08,Added logical any and all (similar to numpy),jjh42,https://api.github.com/repos/pytorch/pytorch/git/commits/ce8ba6dc824558340c99da7526f0b251870cab08
0b614eeb25b5f0e2a73ba9c6ceb9784773c0e823,Added torch.round function,GeorgOstrovski,https://api.github.com/repos/pytorch/pytorch/git/commits/0b614eeb25b5f0e2a73ba9c6ceb9784773c0e823
bfa4a88eda44aee41df6ce51b3c566d5dd117927,added a clamp method for tensors.,jonathantompson,https://api.github.com/repos/pytorch/pytorch/git/commits/bfa4a88eda44aee41df6ce51b3c566d5dd117927
45a3be0a762493ce9b80985cd1040e90c3d760ce,"Add missing header file

nvcc is complaining: THCTensorConv.cu(340): error: identifier ""THCudaTensor_copy"" is undefined",santazhang,https://api.github.com/repos/pytorch/pytorch/git/commits/45a3be0a762493ce9b80985cd1040e90c3d760ce
992798504c41effa0c06321366db3eb2b08b5fa1,Fix FindSSE SSE4* check crash on MSVC.,wishstudio,https://api.github.com/repos/pytorch/pytorch/git/commits/992798504c41effa0c06321366db3eb2b08b5fa1
fc81f4be42ec5ec07fe81d652d3ab199af16ef41,Add file and line number to THError.,adamlerer,https://api.github.com/repos/pytorch/pytorch/git/commits/fc81f4be42ec5ec07fe81d652d3ab199af16ef41
4795c660a07d94edb842edb2c52e77c3d83bd8da,Speed up Tensor:index for contiguous tensors,colesbury,https://api.github.com/repos/pytorch/pytorch/git/commits/4795c660a07d94edb842edb2c52e77c3d83bd8da
67f8eb429c52629ad2e2664e7b3cafa5ba15fb1a,Adding support for Storage views and exposing Storage type.,zakattacktwitter,https://api.github.com/repos/pytorch/pytorch/git/commits/67f8eb429c52629ad2e2664e7b3cafa5ba15fb1a
3e849586a0da9fe797c98dd63038db2fbbadbe12,"FindBLAS.cmake: fix missing stdlib.h include

This is to make sure BLAS F2C auto detection does not fail because of
compiler warnings:

    error: implicit declaration of function 'exit'",deltheil,https://api.github.com/repos/pytorch/pytorch/git/commits/3e849586a0da9fe797c98dd63038db2fbbadbe12
2ed1077a8313c783da4098bdde6c69cde3290329,"A clean init for Caffe2, removing my earlier hacky
commits.",Yangqing,https://api.github.com/repos/pytorch/pytorch/git/commits/2ed1077a8313c783da4098bdde6c69cde3290329
d8e3ce8ef47fb84a0705969d0debe4a0cb38991d,[build] flush output from subprocesses,longjon,https://api.github.com/repos/pytorch/pytorch/git/commits/d8e3ce8ef47fb84a0705969d0debe4a0cb38991d
8ba95403fdc573c0f7f485318255e0d9368a5b94,minor format issue,xiaoyunwu,https://api.github.com/repos/pytorch/pytorch/git/commits/8ba95403fdc573c0f7f485318255e0d9368a5b94
0bc9dd6bec270cfa63ffc354cb8459025f5834b3,fixed possibly wrong pointers coming from a saved state,mys007,https://api.github.com/repos/pytorch/pytorch/git/commits/0bc9dd6bec270cfa63ffc354cb8459025f5834b3
eefc497b9ea2b898ba4901cde9d91acbc31d12da,"Do not set -Werror for MSVC
TH_API removed from function definitions to allow for static library compilation",diz-vara,https://api.github.com/repos/pytorch/pytorch/git/commits/eefc497b9ea2b898ba4901cde9d91acbc31d12da
ba62d19b6d3dd81d3f5a338641c8e097c6c841b5,Add support for compilation on Windows using mingw32.,pkulchenko,https://api.github.com/repos/pytorch/pytorch/git/commits/ba62d19b6d3dd81d3f5a338641c8e097c6c841b5
50df65bda79e81d0930070f9d870913442e4737e,Fix for gemm with zero strides plus add unit test for torch.mm,fmassa,https://api.github.com/repos/pytorch/pytorch/git/commits/50df65bda79e81d0930070f9d870913442e4737e
0c6938faa9ed6b6cb58e452e7f089ac25fd358ca,Work under windows,samehkhamis,https://api.github.com/repos/pytorch/pytorch/git/commits/0c6938faa9ed6b6cb58e452e7f089ac25fd358ca
17fb6c489edc49813a539bc707aac67deb868afe,Fix SSE 4.1 bug,massimobernava,https://api.github.com/repos/pytorch/pytorch/git/commits/17fb6c489edc49813a539bc707aac67deb868afe
6440362c08932e5427dc677dd12e2b85d8e23c7e,"build_env.py: add /usr/include, /usr/lib to the default include, library
dirs",jeffdonahue,https://api.github.com/repos/pytorch/pytorch/git/commits/6440362c08932e5427dc677dd12e2b85d8e23c7e
c40f6b6ead4d91280bf94a7880836dd1edba25cd,LAPACK ormqr routine,bartvm,https://api.github.com/repos/pytorch/pytorch/git/commits/c40f6b6ead4d91280bf94a7880836dd1edba25cd
bc9c0c159894ead7da579d2e9497bde41c992c16,"Rewrite indexSelect kernel for correctness

The generic indexSelect kernel (for non-contiguous tensors) was broken;
totally rewrite it for correctness.

Closes #240.",oyamauchi,https://api.github.com/repos/pytorch/pytorch/git/commits/bc9c0c159894ead7da579d2e9497bde41c992c16
384668bbc80c92b3b1bdd13a8724b931e05c31a2,"adding potrs and uplo option to potrf

adding tests for torch.potrs and (modified) torch.potrf",j-wilson,https://api.github.com/repos/pytorch/pytorch/git/commits/384668bbc80c92b3b1bdd13a8724b931e05c31a2
35c2754c88896c7742d7bc7e59e1936f19f04776,static libraries no longer built by default,hughperkins,https://api.github.com/repos/pytorch/pytorch/git/commits/35c2754c88896c7742d7bc7e59e1936f19f04776
439bc9bddcc36a301ced0b81cc5a6b1cecb8311d,faster norm,wickedfoo,https://api.github.com/repos/pytorch/pytorch/git/commits/439bc9bddcc36a301ced0b81cc5a6b1cecb8311d
7cf666f0fc2d8443fc08e612ecc13f61f85faf65,add indexAccum,ajabri,https://api.github.com/repos/pytorch/pytorch/git/commits/7cf666f0fc2d8443fc08e612ecc13f61f85faf65
264102d24594e62fc452d77c84129265b0f6ab14,Add torch.mode and fix median/kthvalue docs,kosklain,https://api.github.com/repos/pytorch/pytorch/git/commits/264102d24594e62fc452d77c84129265b0f6ab14
15e2939f7a82acd041f257d201c7420f77239857,"Fixing large data reading/writing/memory storing

When data size starts to exceed around 4GB variables of type long canâ€™t
keep this size anymore so the type needs to be replaced",anastasiuspernat,https://api.github.com/repos/pytorch/pytorch/git/commits/15e2939f7a82acd041f257d201c7420f77239857
b1fa9d2b06714de099e3ae1141d15dcbaba78dd3,Fixed unsigned overflows in THFile,apaszke,https://api.github.com/repos/pytorch/pytorch/git/commits/b1fa9d2b06714de099e3ae1141d15dcbaba78dd3
56f18e35d442606997cd34797fb0c7d654fc81aa,Detecting LAPACK on the system with LAPACKless OpenBLAS,borisfom,https://api.github.com/repos/pytorch/pytorch/git/commits/56f18e35d442606997cd34797fb0c7d654fc81aa
0673d5f44f6829dbf1392e19994e299f6f2cc33b,Initial release.,nluehr,https://api.github.com/repos/pytorch/pytorch/git/commits/0673d5f44f6829dbf1392e19994e299f6f2cc33b
b5f299cc176b699029d898b579b74fbbf145d40a,"Add isSetTo: simple check for shared storage.

Returns true iff Tensor is set to argument Tensor. Specifically, this is
true iff tensor shares same storage as argument tensor, with same
storage offset and identical sizes and strides.",dm-jrae,https://api.github.com/repos/pytorch/pytorch/git/commits/b5f299cc176b699029d898b579b74fbbf145d40a
3df4750a37bf02d554027e9f95917df0f9e14947,Add isSetTo to cutorch,dmjlm,https://api.github.com/repos/pytorch/pytorch/git/commits/3df4750a37bf02d554027e9f95917df0f9e14947
1850c54b8c85f2ac8671abb8d8a6ff590e5bf184,"New function longSize for files

This function allows binary files compatibility between 32 and 64 bit
architectures",mvitez,https://api.github.com/repos/pytorch/pytorch/git/commits/1850c54b8c85f2ac8671abb8d8a6ff590e5bf184
41ce4ca9fc748186190e63823b4f1cf5c365b220,Add int64 and uint64 types for all algorithms and tests,slayton58,https://api.github.com/repos/pytorch/pytorch/git/commits/41ce4ca9fc748186190e63823b4f1cf5c365b220
7b6339cdd280085bb634046c19d003ae9326344b,Make nBufferSize signed for writing longs,alschua,https://api.github.com/repos/pytorch/pytorch/git/commits/7b6339cdd280085bb634046c19d003ae9326344b
e1634ca6cb5ff26ffd0ba6a7dd5a4ea80f6a400e,Use semantic versioning,lukeyeager,https://api.github.com/repos/pytorch/pytorch/git/commits/e1634ca6cb5ff26ffd0ba6a7dd5a4ea80f6a400e
d332c41e715cd06f93e5ccb88f597f1e2efc2581,fix a typo in README.md,yangky11,https://api.github.com/repos/pytorch/pytorch/git/commits/d332c41e715cd06f93e5ccb88f597f1e2efc2581
1423a171088c09006e5886ca859f2c7a0ac39666,Add THNN/ffi conversion of Abs,andreaskoepf,https://api.github.com/repos/pytorch/pytorch/git/commits/1423a171088c09006e5886ca859f2c7a0ac39666
c05312f1517bab34586bb74a37bb60dedf097d30,"Moved tests to separate dir and improved MPI test

test sources moved to test/ directory.
MPI test displays PASS/FAIL and returns code accordingly.

Change-Id: I058ebd1bd5202d8f38cc9787898b2480100c102b
Reviewed-on: http://git-master/r/936086
Reviewed-by: Przemek Tredak <ptredak@nvidia.com>
Tested-by: Przemek Tredak <ptredak@nvidia.com>",sjeaugey,https://api.github.com/repos/pytorch/pytorch/git/commits/c05312f1517bab34586bb74a37bb60dedf097d30
a0b433c8e1ddeac8b7217bf1f3bb6d7713133908,Readding the argmin argmax as second argument,mevGDM,https://api.github.com/repos/pytorch/pytorch/git/commits/a0b433c8e1ddeac8b7217bf1f3bb6d7713133908
1f9a43488132a0aa1b1153492ffb3aaa379ff426,"Initialise longSize in PipeFile

The previously added longSize field was only initialised in DiskFile and
not in PipeFile. This meant uninitialized memory was used in PipeFile
construction",kieranmilan,https://api.github.com/repos/pytorch/pytorch/git/commits/1f9a43488132a0aa1b1153492ffb3aaa379ff426
da0634c124503f901acda66f64b966077fac2e82,"Fix for issue #517: torch.all

torch.all only returns true if all numbers in the tensor are odd #517

Update THTensorMath.c

Respective test",ivpopov,https://api.github.com/repos/pytorch/pytorch/git/commits/da0634c124503f901acda66f64b966077fac2e82
8e4e40557a64598d4a4a1fd8e669629cafdd94b6,THGeneral.h.in: add generic cleanup mechanism for error macros that do not return,tkoeppe,https://api.github.com/repos/pytorch/pytorch/git/commits/8e4e40557a64598d4a4a1fd8e669629cafdd94b6
85c277e6013e1db864e6cb66d4efd8cec2698548,Added torch.equal function which performs a tensor equality check,fbesse,https://api.github.com/repos/pytorch/pytorch/git/commits/85c277e6013e1db864e6cb66d4efd8cec2698548
e3677059ae720e4f939882176cf5e5c0e7303ade,Remove junk value columns from right singular vectors matrix in SVD,yf225,https://api.github.com/repos/pytorch/pytorch/git/commits/e3677059ae720e4f939882176cf5e5c0e7303ade
aef48d21c4308bd3abf1ceae0adc1878b173e1f1,Making margin parameterizable in nn.MultiMarginCriterion,nybbles,https://api.github.com/repos/pytorch/pytorch/git/commits/aef48d21c4308bd3abf1ceae0adc1878b173e1f1
8dff00a7b894a25e5b8a7681f6cc1b8bb1c779aa,Fix csub #60,JoostvDoorn,https://api.github.com/repos/pytorch/pytorch/git/commits/8dff00a7b894a25e5b8a7681f6cc1b8bb1c779aa
59c25ebe036a69ee86fe6cb1ec2a7f5ea54807f7,"Fix a bug in :isSetTo

The case of self->storage == NULL was not checked, so any two Tensors
with no storage would effectively appear to be shared with each other.
Added a test to prevent regressions.",malcolmreynolds,https://api.github.com/repos/pytorch/pytorch/git/commits/59c25ebe036a69ee86fe6cb1ec2a7f5ea54807f7
a3b4b6c23cd41312ce01ff8b419933d4e1e4dca2,In-place ELU,anibali,https://api.github.com/repos/pytorch/pytorch/git/commits/a3b4b6c23cd41312ce01ff8b419933d4e1e4dca2
300b0d3e6a581b60d60d77acd798eb1ef69c3535,"Fix bad size evaluation

xmax, xmin and step are accreal. Therefore, computations are carried out
in both int and double logic. The separate division does introduce a bug
when int logic is used.",Atcold,https://api.github.com/repos/pytorch/pytorch/git/commits/300b0d3e6a581b60d60d77acd798eb1ef69c3535
2dbc0dcb50edb9a10fbd311a21e84ad22362891b,Remove unnecessary rank check in potrs call.,bamos,https://api.github.com/repos/pytorch/pytorch/git/commits/2dbc0dcb50edb9a10fbd311a21e84ad22362891b
a81f87b9f1b469c37f9bf9318d2825b8bd857559,Fix multinomial regression.,nkoumchatzky,https://api.github.com/repos/pytorch/pytorch/git/commits/a81f87b9f1b469c37f9bf9318d2825b8bd857559
0da5e29063a3a7473f475401e6d2b199b71ba99b,Sparse Linear now does sparse updates from the last input,ebetica,https://api.github.com/repos/pytorch/pytorch/git/commits/0da5e29063a3a7473f475401e6d2b199b71ba99b
8047e43d4603bd150103c0828ffb2f2648f59edb,make margin parameterizable,caldweln,https://api.github.com/repos/pytorch/pytorch/git/commits/8047e43d4603bd150103c0828ffb2f2648f59edb
80e413e9229dc902699d982dad2e5ee5076a8337,"Improvements to torch.{linspace, logspace, range}.

Allow end < start in linspace and logspace (no mathematical reason to forbid
this, and it makes it match numpy behaviour).

Also don't raw-resize tensor unnecessarily. This makes it work with
non-contiguous tensors (e.g., can now do a range on a slice of a tensor).",davidsaxton,https://api.github.com/repos/pytorch/pytorch/git/commits/80e413e9229dc902699d982dad2e5ee5076a8337
95ffcb67d3c0a2a0d7d3b876550bc35656c3f4dd,"Add fmod(), cfmod()",liboyue,https://api.github.com/repos/pytorch/pytorch/git/commits/95ffcb67d3c0a2a0d7d3b876550bc35656c3f4dd
0bf2fc46501e64ab5ebda729d14d4f87687d8887,[LookupTable] Add Max-norm constraints to LookupTable (#739),xianjiec,https://api.github.com/repos/pytorch/pytorch/git/commits/0bf2fc46501e64ab5ebda729d14d4f87687d8887
e30bf9598919787b2c851158f1e2e1ad4d4c2432,Enable compilation with old g++ when the default g++ is not supported (+5.0),Hopobcn,https://api.github.com/repos/pytorch/pytorch/git/commits/e30bf9598919787b2c851158f1e2e1ad4d4c2432
5a08f4999528747dfafdb248bdf68cae91e88a57,"Fix uninitialized pointers in THCState during init.

An out-of-memory error during THCudaInit will cause
a jump to an uninitialized address.",lukealonso,https://api.github.com/repos/pytorch/pytorch/git/commits/5a08f4999528747dfafdb248bdf68cae91e88a57
a12cb07327f653657b9436e1561438357914c541,"Fix _msize crash on windows xp

It will crash application when calling _msize(NULL) on windows xp.",clcarwin,https://api.github.com/repos/pytorch/pytorch/git/commits/a12cb07327f653657b9436e1561438357914c541
c772fb79765e57694c5e0289f5b8f80f4bf59dfc,few changes as required for cudnn fp16 support,lukasc-ch,https://api.github.com/repos/pytorch/pytorch/git/commits/c772fb79765e57694c5e0289f5b8f80f4bf59dfc
a97ae66a1db36bdbdeebae3234d99a16a4a91ee9,remove some warning when compiling TH,albanD,https://api.github.com/repos/pytorch/pytorch/git/commits/a97ae66a1db36bdbdeebae3234d99a16a4a91ee9
e1921929addce4b1c6e4f46c3eed576f0d7f4992,Fix - added check to avoid invalid memory access in indexSelect_long,htwaijry,https://api.github.com/repos/pytorch/pytorch/git/commits/e1921929addce4b1c6e4f46c3eed576f0d7f4992
6db57b8a4d0ae1a9efe542fc7952dda499862a93,added correct inclusion for malloc_usable_size(),tpltnt,https://api.github.com/repos/pytorch/pytorch/git/commits/6db57b8a4d0ae1a9efe542fc7952dda499862a93
88bc821d79463d69d4aedbbb4fdc67f41ba792cb,added THAtomicSetLong,lake4790k,https://api.github.com/repos/pytorch/pytorch/git/commits/88bc821d79463d69d4aedbbb4fdc67f41ba792cb
01b17a2dccaa352b8d3f5f3eddfd32e97c4f31d8,"Limit registers when LRNFillScale runs on TK1.

Fix for torch@cunn#271, to make openface works on Jetson TK1.",verdimrc,https://api.github.com/repos/pytorch/pytorch/git/commits/01b17a2dccaa352b8d3f5f3eddfd32e97c4f31d8
7e5a1d91405b509e143b24a2f62ae7245dcaf512,Visual Studio doesn't allow in-loop declaration in the 'omp parallel for' construct,elikosan,https://api.github.com/repos/pytorch/pytorch/git/commits/7e5a1d91405b509e143b24a2f62ae7245dcaf512
6a1e0c3127170bb3b2a896b38c983295d56bca31,"Added CUDA version of VolumetricMaxUnpooling
modified:   THCUNN.h
new file:   VolumetricMaxUnpooling.cu
modified:   ../../test.lua

Freed input storage
modified:   VolumetricMaxUnpooling.cu",kmul00,https://api.github.com/repos/pytorch/pytorch/git/commits/6a1e0c3127170bb3b2a896b38c983295d56bca31
3af4b939567b050b971fd2f4d7c83c030ce4544c,nobias in spatial full conv,PraveerSINGH,https://api.github.com/repos/pytorch/pytorch/git/commits/3af4b939567b050b971fd2f4d7c83c030ce4544c
30df3ea23a6333d80f4a1e7bd8d604ab103840a2,"cmake: add soversion for libTH

Imported from Debian package.",cdluminate,https://api.github.com/repos/pytorch/pytorch/git/commits/30df3ea23a6333d80f4a1e7bd8d604ab103840a2
ed401cc29bede009c9a3227fdc6171604a177c83,link library with -lrt; otherwise there is undefined reference to shm_open,jia-kai,https://api.github.com/repos/pytorch/pytorch/git/commits/ed401cc29bede009c9a3227fdc6171604a177c83
234c8c9ef390b2aeccfec09a574dfc3ab5ab9779,Update LICENSE.txt,cliffwoolley,https://api.github.com/repos/pytorch/pytorch/git/commits/234c8c9ef390b2aeccfec09a574dfc3ab5ab9779
4bbe5a095d5ac5d2efe8b4eaf6cdacf6e7dd070e,"HardTanh does not use inclusive bounds in inplace mode

HardTanh does not use inclusive bounds in inplace mode, which makes it
inconsistent with the cuda version, as well as Threshold, ReLU, etc.",gchanan,https://api.github.com/repos/pytorch/pytorch/git/commits/4bbe5a095d5ac5d2efe8b4eaf6cdacf6e7dd070e
76ac35cdaa0b3dd92f0d7c77f38e3eb6f7c344b9,"Added dyanamic dispatch for x86 (#755)

Added dynamic dispatch for x86",rguthrie3,https://api.github.com/repos/pytorch/pytorch/git/commits/76ac35cdaa0b3dd92f0d7c77f38e3eb6f7c344b9
0703e0e8971b63e781773edd635258562cd1f50e,"BCECriterion THCUNN + Weights (#331)

BCE Criterion CUDA implementation",nitsky,https://api.github.com/repos/pytorch/pytorch/git/commits/0703e0e8971b63e781773edd635258562cd1f50e
f56f06d88d77d1d12527aa47832c0dec0398c4a8,fix cpuid ecx; change to compile with msvc,BTNC,https://api.github.com/repos/pytorch/pytorch/git/commits/f56f06d88d77d1d12527aa47832c0dec0398c4a8
94b35312d00a0c8a369ca28ee931a9bccc53909e,"Compile fixes for picky compilers / stl versions (#518)

* Compile fixes for picky compilers/stl versions",rsfbarreira,https://api.github.com/repos/pytorch/pytorch/git/commits/94b35312d00a0c8a369ca28ee931a9bccc53909e
c2e3bf214547b76682db5ba666a1e75596ec05d1,"[cutorch refactor] move meanall function into generic/, update cwrap for lua mean",killeent,https://api.github.com/repos/pytorch/pytorch/git/commits/c2e3bf214547b76682db5ba666a1e75596ec05d1
38cb3d02270b9e558a891a9a2bef01a75d1bd9e1,Fix build when NEON is supported,howard0su,https://api.github.com/repos/pytorch/pytorch/git/commits/38cb3d02270b9e558a891a9a2bef01a75d1bd9e1
aa6f6117b750acc43a3bd4d42472bc0213881496,Ported Linear module to THNN,lantiga,https://api.github.com/repos/pytorch/pytorch/git/commits/aa6f6117b750acc43a3bd4d42472bc0213881496
f16f68e103dfc22921f6106ec7136ddc7a0ab087,CMake: Install generic/THCTensorMathScan.h,dbolkensteyn,https://api.github.com/repos/pytorch/pytorch/git/commits/f16f68e103dfc22921f6106ec7136ddc7a0ab087
4d03d96e8b888b94cfcd63bba2a6fdbcb4ac676c,"fix: cunn can't find cutorch sources

https://github.com/torch/distro/issues/138#issuecomment-259133935",joker512,https://api.github.com/repos/pytorch/pytorch/git/commits/4d03d96e8b888b94cfcd63bba2a6fdbcb4ac676c
c82537462baa715b2c70726f7da8f734b2ad3a3f,"[cutorch] remove syncing point from baddbmm

This change removes HtoD copies inside baddbmm. These copies
introduce a syncing point which causes slow downs in a multi
gpu training.

Test plan: Run unittests for baddbmm.",denisyarats,https://api.github.com/repos/pytorch/pytorch/git/commits/c82537462baa715b2c70726f7da8f734b2ad3a3f
5f2b32e45b7d31ec942de27369a1308a0afe8fb0,Add Fortran bindings,kylefernandes,https://api.github.com/repos/pytorch/pytorch/git/commits/5f2b32e45b7d31ec942de27369a1308a0afe8fb0
61c0bcf91dbf3d50f3a30dfc92f540301b078aa4,"removed deprecated readme info

Summary: Removed deprecated information.

Reviewed By: Yangqing

Differential Revision: D4208320

fbshipit-source-id: 6906e9c56b9f0abf0582c2ba1bb8e6a5a9f89a84",bwasti,https://api.github.com/repos/pytorch/pytorch/git/commits/61c0bcf91dbf3d50f3a30dfc92f540301b078aa4
e3f440b1d058d348b689629c01630d206bd1631b,Make torch.backends.cudnn work on OSX,Maratyszcza,https://api.github.com/repos/pytorch/pytorch/git/commits/e3f440b1d058d348b689629c01630d206bd1631b
5765d608cc2fc602c038dd8aaf5e747986fac25b,"Add a static library target ""staticlib"" to the Makefile.

Rename the static library ""libnccl_static.a"" to disambiguate from the
dynamic libraries.",peterhj,https://api.github.com/repos/pytorch/pytorch/git/commits/5765d608cc2fc602c038dd8aaf5e747986fac25b
9da60c39ce16fc0acf1b1b81ae387c01a48b91f4,Fix batch_first in AutogradRNN (#255),jekbradbury,https://api.github.com/repos/pytorch/pytorch/git/commits/9da60c39ce16fc0acf1b1b81ae387c01a48b91f4
c41f0d27c42fc8b3d558f20069342c1750143c8c,"adding more things to the list of known operators in model_helper

Summary: This is so they don't generate spurious warning messages in the logs

Reviewed By: dzhulgakov

Differential Revision: D4205610

fbshipit-source-id: f764b51565430f4057898ab929372bc7943e0495",ajaech,https://api.github.com/repos/pytorch/pytorch/git/commits/c41f0d27c42fc8b3d558f20069342c1750143c8c
b5613d7a3dd02f685e6119ce5b83752909e7ac73,"report offending blob name when blob in wrong device scope

Summary:
Another recurrent problem is that some blob is in CPU scope while operator expects CUDA scope (or other way round).
The exception is only partially helpful, as it tells the operator but not the offending blob name. This diff adds the blob name
to the exception message, helping debug.

Reviewed By: prigoyal

Differential Revision: D4208584

fbshipit-source-id: 5aeac5c3efeed8d6c995bea166ed534855007945",akyrola,https://api.github.com/repos/pytorch/pytorch/git/commits/b5613d7a3dd02f685e6119ce5b83752909e7ac73
c48551409c0c7ea0f815ced409e9be92768b334d,"Proper error message if passing NoneType value for kwargs

Summary:
I got a weird error about NoneType not being iterable which made me think
it was some error in the C2 core, whereas it was an error in my code.

Reviewed By: Yangqing

Differential Revision: D4192799

fbshipit-source-id: 0122f13e205c1c6a0766545f0ad6296228d3a3d9",pietern,https://api.github.com/repos/pytorch/pytorch/git/commits/c48551409c0c7ea0f815ced409e9be92768b334d
6b437708ad8b42a7362178576af0924f3ad6c856,"caffe2/caffe2/operators/softmax_with_loss_op.cc: avoid shadowing warnings

Summary:
Fix warnings exposed by gcc-4.9.x's -Wshadow-compatible-local
(and/or the stricter -Wshadow-local) options.  Note that these
are both less onerous than -Wshadow.
I plan to enable one of them for all of fbcode, soon.

Rename inner ""idx"" to ""k"".

Differential Revision: D4216556

fbshipit-source-id: 5ee48751efd07838db24f56390730718ea031772",meyering,https://api.github.com/repos/pytorch/pytorch/git/commits/6b437708ad8b42a7362178576af0924f3ad6c856
301ab97e41191aa2360896a60f1b188dc2f75ecb,"Fix few more operators to handle empty batches correctly.

Summary:
If we go to prod some of the sparse features might be empty or for some reason
batch might be empty. It's a good idea to be sure that we can run empty
batches.

Reviewed By: dzhulgakov

Differential Revision: D4197297

fbshipit-source-id: 1a154ebf625d1a39fd15354a154cf100f525ae9a",kennyhorror,https://api.github.com/repos/pytorch/pytorch/git/commits/301ab97e41191aa2360896a60f1b188dc2f75ecb
a7df0e6724cf03042b5c6244739783138ef30413,"Clone model net to avoid hard-coded inputs

Summary:
Previously DPER was quite broken - we couldn't change loaders on the fly because serialized model had blob names hard-coded, e.g. ""nn_loader/dense"". In fact, the tests worked only by accident as both trainer and evaluator used the same loader type.

This diff does the following:
1) when writing out model, remap input blobs to be 'inputs/<field_name>'
2) when loading eval model, remap them back to the current loader

This diff uses Net.input_schema() for convenience, in particular the schema format is implicitly serialized in input blobs names. From our discussion with Andrey this type of hardcoding is actually acceptible since the schema of HiveReader on python side is inferred via the same string-parsing procedure

It also modifies model saving a bit so that we don't pollute global namespace with shape_provider net.

Overall code in mlp.py is pretty terrible. But I'd leave refactoring to xianjiec as a part of Layers migration.

Reviewed By: xianjiec

Differential Revision: D4218902

fbshipit-source-id: 6cd19f0343ec1be6ddaa3581512e61879957749e",dzhulgakov,https://api.github.com/repos/pytorch/pytorch/git/commits/a7df0e6724cf03042b5c6244739783138ef30413
e8b7ec1393428c1cfaa5623b82accd177ba5d250,"disable local update for sparse features

Summary:
With parameter server, sparse features are updated on the parameter server.
Local update for sparse features are disabled. But that logic is removed in
D4144922. This diff is to add this logic back in a slightly different way.

Previously, in trainer_example, I did that in a hacky way just avoid adding
sparse weight to model.params. It will still generate grad, but will not add
optimization operators. At the same time, it is always registered directly in
the sparse_mapping, so the parameter server is aware of this parameter.
But with the new change for ParameterInfo. I can not do it in that way anymore.
Because the param registry and params are bind together in ParameterInfo.

For dper, there is a option in dper model helper to disable all of the sparse
parameter optimizer.

To combine these two together, I directly changed the ModelHelperBase in this
diff. It is not quite ideal. It is better to do it in Layer. But to fix the old
one, this seems to be more reasonable place to cover both cases.

With this diff, there is no spike anymore. So probably this is the root cause
for the convergence issue we have seen in D4144922. It explains that why the
model can recover, which is because adagrad decays local learning rate and
local updates cause less change.

Reviewed By: dzhulgakov

Differential Revision: D4229684

fbshipit-source-id: da1241d43d7c52cbf13560f9bb83e09897d8d56f",kingohm,https://api.github.com/repos/pytorch/pytorch/git/commits/e8b7ec1393428c1cfaa5623b82accd177ba5d250
1aafeb3565af1f17bf3e58e5022eec9fff7e34be,"clean up memory of c2/sigrid predictor

Summary: trying to optimize c2 predictor memory usage. mainly to remove unsed dbreader and dper metadata.

Differential Revision: D4232595

fbshipit-source-id: dcd7aa7dd09587ec9811a9e5ec725e0c22757665",lgxgfb,https://api.github.com/repos/pytorch/pytorch/git/commits/1aafeb3565af1f17bf3e58e5022eec9fff7e34be
5f7d1f02f22e5f9ab5d6ed482389de41c2763baa,"Use native reader for evaluation

Summary:
Since hashing is different.

This should be ready to commit now. Running ads nn canaries.

Differential Revision: D4264009

fbshipit-source-id: 3aa16b0c47c61f9a442b0375524c5f1580af5892",aazzolini,https://api.github.com/repos/pytorch/pytorch/git/commits/5f7d1f02f22e5f9ab5d6ed482389de41c2763baa
ab5f26545b5c565990b5c81f372a175dda0bd656,"Correct documentation to be in line with #237 (#303)

.parameter_dict was renamed to .state_dict in #237

This documentation change reflects that.",y0ast,https://api.github.com/repos/pytorch/pytorch/git/commits/ab5f26545b5c565990b5c81f372a175dda0bd656
6d12185cc99daf62851ef79958728b731b89bc08,Fixed compilation on Raspberry PI without NEON,vfonov,https://api.github.com/repos/pytorch/pytorch/git/commits/6d12185cc99daf62851ef79958728b731b89bc08
0d0f197682f6be142a8634450f6473cef9bdfb2c,Add note on Huber loss (#310),Kaixhin,https://api.github.com/repos/pytorch/pytorch/git/commits/0d0f197682f6be142a8634450f6473cef9bdfb2c
cb918ac727c87e59d6f7caebdf9e2d2e282878c1,"Implementation of ResNets on imagenet dataset

Summary:
adding imagenet dataset as well
data augmentation and model has been added, just need to add db read

Differential Revision: D4289150

fbshipit-source-id: b531d3f09e3d0efac5cda5bb75d8146e1bb693e4",prigoyal,https://api.github.com/repos/pytorch/pytorch/git/commits/cb918ac727c87e59d6f7caebdf9e2d2e282878c1
fb68be952d369d4675ecbe2faa39b07803c838ca,"Bugfix for multinomial distribution

- Ensures the index of the first bin from the cdf is returned.",pavanky,https://api.github.com/repos/pytorch/pytorch/git/commits/fb68be952d369d4675ecbe2faa39b07803c838ca
0c1c0e21b8c90545bfc451589c9ebec774361b6e,"Bugfix of type in THCTensor macro.

A fix for issue #632.",popol1991,https://api.github.com/repos/pytorch/pytorch/git/commits/0c1c0e21b8c90545bfc451589c9ebec774361b6e
78edb8295e1eff4b2990376a89e01c8ec0f186c7,"No exception for float64 in FeedBlob. Warning instead.

Summary:
The exception in FeedBlob causes many tests to fail.
Instead of exception, we log a warning message and move on.
Feeding a float64 blob should not cause any issue.
Closes https://github.com/caffe2/caffe2/pull/57

Reviewed By: bwasti

Differential Revision: D4343135

Pulled By: Yangqing

fbshipit-source-id: cd1144b94c9883fcbd8bdcd78f9f93a67debc0a6",pooyadavoodi,https://api.github.com/repos/pytorch/pytorch/git/commits/78edb8295e1eff4b2990376a89e01c8ec0f186c7
9e498c7bbac7cc14009227a16ecc7262c0a042a4,"caffe2: removing message logging in conv_transpose_op

Summary: Avoid printing message repeatedly each time the conv_transpose_op (with cudnn) is called

Reviewed By: Yangqing

Differential Revision: D4337242

fbshipit-source-id: 27b048bad8c54604d91174acd4928a1496f2f5c7",KaimingHe,https://api.github.com/repos/pytorch/pytorch/git/commits/9e498c7bbac7cc14009227a16ecc7262c0a042a4
6c3cca9bc7056194bd9719b67e7e0a4979110b66,"Build caffe2, NNPACK, FXdiv, pthreadpool for macOS

Summary: Builds caffe2 and dependencies for macOS. Not included in the MSQRD engine or elsewhere yet.

Reviewed By: Yangqing

Differential Revision: D4334013

fbshipit-source-id: 31cacf07e2b07f379e1894e51dde5103c56b8815",orionr,https://api.github.com/repos/pytorch/pytorch/git/commits/6c3cca9bc7056194bd9719b67e7e0a4979110b66
c2d28fb87498d506f71f03d33c188b6b46e718d3,"RNNs API simplification

Summary:
This is a first step in improving our RNN story. It provides a wrapper around current RecurrentNetworkOp implementation which infers most of the redundant parameters and makes API much simpler.

Also in order to support general step nets I added an extra argument to the RecurrentNetworkOp.

Future work:

1. Inferring step net output and internal blobs (scratches) sizes and type
2. Avoid accessing blobs by names in c++ part
3. Remove requirement for inputs / output 1:1 correspondence in the step net
4. Make python API support networks with operators like Sum being on the boarder of the Cell net (currently there is an issue with such networks where gradient blobs which are on the side are not explicitly created).

Differential Revision: D4268503

fbshipit-source-id: f8a66491c2b55daa730caeed7e9f2b3921541b49",urikz,https://api.github.com/repos/pytorch/pytorch/git/commits/c2d28fb87498d506f71f03d33c188b6b46e718d3
f8dee4620a0a15cb590285930114238832fd918d,add a new function histc2,KaiyuYue,https://api.github.com/repos/pytorch/pytorch/git/commits/f8dee4620a0a15cb590285930114238832fd918d
f7bd3f7932dce495a69ba3c28fe2fd61a5564aed,"added pixel shuffle layer + tests

removed duplicate save_for_backward",twitter-recsys-challenge-2020,https://api.github.com/repos/pytorch/pytorch/git/commits/f7bd3f7932dce495a69ba3c28fe2fd61a5564aed
86ec14e594888c1050f1a8ab5dd9ec72cf9c801e,"Add support for VSX vector instructions on PPC

Added support for the fill, diff, scale, mul and add functions using
PPC CPU vector instructions. These are used in place of the versions
of these functions written for x86, when compiled on PPC.

This fixes a compile failure on PPC",amrobbins,https://api.github.com/repos/pytorch/pytorch/git/commits/86ec14e594888c1050f1a8ab5dd9ec72cf9c801e
5340291addd60748674e03eac76004c966743d8d,"Update FindARM.cmake

Fix typos",temerick,https://api.github.com/repos/pytorch/pytorch/git/commits/5340291addd60748674e03eac76004c966743d8d
b2ae0544104f0a4aec42b60144d693ea840c4113,Add SpatialAdaptiveAveragePooling.,ruotianluo,https://api.github.com/repos/pytorch/pytorch/git/commits/b2ae0544104f0a4aec42b60144d693ea840c4113
7d6742f2f51320bb09833d714eeb1d61c508a4e4,"Tool to convert caffe models to c2 + fixes for xray v10

Summary:
Simple tool similar to caffe_translator_test.py for conversion from caffe to
caffe2. The differences are:

There are a couple of issues that need to be fixed as mentioned in
https://our.intern.facebook.com/intern/tasks?t=15424761, especially related to
the 'legacy_pad' field in conv op.

Differential Revision: D4407146

fbshipit-source-id: ec641f6d7e0cf6cdf2eca21f058b4451635d4a56",viswanathgs,https://api.github.com/repos/pytorch/pytorch/git/commits/7d6742f2f51320bb09833d714eeb1d61c508a4e4
99f4864674ebaae44e266baf5a3774af578471e5,"fixed RMSprop initialization (#485)

* fixed RMSprop initialization",glample,https://api.github.com/repos/pytorch/pytorch/git/commits/99f4864674ebaae44e266baf5a3774af578471e5
f2741e80380e72c651731654cb73b67da5dd7bc1,format fix (#490),vra,https://api.github.com/repos/pytorch/pytorch/git/commits/f2741e80380e72c651731654cb73b67da5dd7bc1
26a492acf3924189e79f7b952eadb22ba52028dc,"Update docstring for ConvTranspose functions

Transposed convolutions are often (but incorrectly) referred to as Deconvolutional operations. Made mention of this in the docstring to make it easier for people to search for this operation in the documentation.",ronrest,https://api.github.com/repos/pytorch/pytorch/git/commits/26a492acf3924189e79f7b952eadb22ba52028dc
173c81c2d2fef983a49deb1fc563830130ee585f,import package at the beginning,donglixp,https://api.github.com/repos/pytorch/pytorch/git/commits/173c81c2d2fef983a49deb1fc563830130ee585f
43b5be1d7877031a322a0bc465a8e424cfd56eca,added c implementation of GatedLinearUnit,huihuifan,https://api.github.com/repos/pytorch/pytorch/git/commits/43b5be1d7877031a322a0bc465a8e424cfd56eca
b740878697982cc4204860e3cdb827699a4190b1,"Updated h0,c0 shape in documentation for RNN, LSTM, GRU (#519)",MaximumEntropy,https://api.github.com/repos/pytorch/pytorch/git/commits/b740878697982cc4204860e3cdb827699a4190b1
16a09304b47353bbe2b014a24d9e574399c8a379,fix documentation of LSTM cell (#525),jsenellart,https://api.github.com/repos/pytorch/pytorch/git/commits/16a09304b47353bbe2b014a24d9e574399c8a379
3f66f66da9efba435ea664f8fab0923d16971750,"DebugMode helper for Caffe2

Summary:
It helps to develop scripts locally (when working outside of Flow). One doesn't have to rerun the script in order to catch exception in the debugger / add a print statement. (Flow does this kind of thing automatically)

Usage example:

```
if __name__ == '__main__':
  workspace.GlobalInit(['caffe2', '--caffe2_log_level=2'])
  from caffe2.python.utils import DebugMode
  DebugMode.enable()
  DebugMode.run(main)
```

Reviewed By: Yangqing

Differential Revision: D4424096

fbshipit-source-id: 73f418c80f581820e70139df7e166981e4d8c55f",salexspb,https://api.github.com/repos/pytorch/pytorch/git/commits/3f66f66da9efba435ea664f8fab0923d16971750
6a7dd236faf5318915dc8d861aeb6f6cfdb4d1e4,"instance norm

Summary: Added gradient and GPU implementation to caffe2 InstanceNorm op

Reviewed By: Yangqing

Differential Revision: D4304808

fbshipit-source-id: 6feecaed589ea9f825260a49b39b4260da6e5426",kmatzen,https://api.github.com/repos/pytorch/pytorch/git/commits/6a7dd236faf5318915dc8d861aeb6f6cfdb4d1e4
c28575a4ebdada363a27cd60d0ea4bd98f520180,Fix typo in documentation for autograd,sheng-z,https://api.github.com/repos/pytorch/pytorch/git/commits/c28575a4ebdada363a27cd60d0ea4bd98f520180
017c7efb43e23cced3ecf5174d251552cc2f9861,Fix typo in LSTMCell documentation,TomVeniat,https://api.github.com/repos/pytorch/pytorch/git/commits/017c7efb43e23cced3ecf5174d251552cc2f9861
f8e89fbe1123f6788992b70361f13ad498665327,fix docs for torch.nn.functional.conv1d (#536),alykhantejani,https://api.github.com/repos/pytorch/pytorch/git/commits/f8e89fbe1123f6788992b70361f13ad498665327
e0c90de6e6128a978ebf3d663f8579a2dc6d8346,"Speedup get_op_ids_in_path

Summary:
Perf bug report: https://www.facebook.com/groups/1405155842844877/permalink/1617904561570003/

Diagnosis:

I've done some digging into this and here's what I've found:
(1) In this use case, the call is disallowed_op_ids = get_op_ids_in_path(ssa, blob_versions, [], inputs)) where inputs = ['res4_22_sum'] is the last blob produced by the res4 stage of a ResNet101 model.
(2) get_op_ids_in_path has exponential running time in the number of blocks in the res4 stage of ResNet. This is based on empirical running times. This call should complete in 4.5 days on my devgpu.
(3) I haven't familiarized myself enough with the IR and SSA code in core.py to understand the algorithmic fix yet, but surely there's a more efficient algorithm to compute the same thing.

Reviewed By: Yangqing

Differential Revision: D4446278

fbshipit-source-id: 8bd147f92d62b865dc355d5802a53e92d64b6e21",rbgirshick,https://api.github.com/repos/pytorch/pytorch/git/commits/e0c90de6e6128a978ebf3d663f8579a2dc6d8346
e374dc1696e1e24c2d0c8656374aed5fab8c93be,"add step rate to adadelta (#568)

Scales `delta` before it is applied to the parameters in order to control the learning rate of the optimizer (inspired from climin optim lib for theano).
Also changed the link to the Adadelta paper to point to the right location.",edouardelasalles,https://api.github.com/repos/pytorch/pytorch/git/commits/e374dc1696e1e24c2d0c8656374aed5fab8c93be
945ce5cdb02c01f08d35a2bfe00fdcfec07cc1d4,"Fix math block of GRUCell in docs (#572)

Added a blank space between the beginning of the `.. math::` block, otherwise it is displayed as a code block.",jfsantos,https://api.github.com/repos/pytorch/pytorch/git/commits/945ce5cdb02c01f08d35a2bfe00fdcfec07cc1d4
0048f228cb6a081cd58e4f9fc2a8dcd32937bfe7,Add spatial test for LogSoftmax,desimone,https://api.github.com/repos/pytorch/pytorch/git/commits/0048f228cb6a081cd58e4f9fc2a8dcd32937bfe7
962b16a814b7200263612bc66c07864e0e94620a,"speedup of softmaxwithlossop

Summary: Speeds up inference in the FCIS model from 2900ms/iter for SoftmaxWithLoss layer to 230ms/iter

Differential Revision: D4456494

fbshipit-source-id: dd520d91fbe950511d198de45f34ac4cd4a676b0",amyzhang,https://api.github.com/repos/pytorch/pytorch/git/commits/962b16a814b7200263612bc66c07864e0e94620a
cc65cc64c8277407bf0c7f027a34af15dfd5715c,"Create function ParseProtobufFromLargeString to parse strings more than 64MB

Summary: Replace ParseFromString with ParseProtobufFromLargeString to get around the limitation of the 64MB limit.

Reviewed By: Yangqing

Differential Revision: D4466226

fbshipit-source-id: b68a6efc76955db294ddb0d23bbaf03b69e4952a",sf-wind,https://api.github.com/repos/pytorch/pytorch/git/commits/cc65cc64c8277407bf0c7f027a34af15dfd5715c
956d946c25ece0051931a023f45d4ee7d39997f1,"Default initial hidden states for recurrent layers (#605)

Fixes #434",goelhardik,https://api.github.com/repos/pytorch/pytorch/git/commits/956d946c25ece0051931a023f45d4ee7d39997f1
f7ab5a128a99ceef3c9f14611f92fc9d0cbc9389,"Delete extra bracket in RNNCellBase.__repr__. (#637)

This extra bracket causes a ValueError when trying to print a Module that uses RNNCellBase or any of its subclasses.",mrdrozdov,https://api.github.com/repos/pytorch/pytorch/git/commits/f7ab5a128a99ceef3c9f14611f92fc9d0cbc9389
ebe6f40fcef2b271e58b99ffccaf664b233b2a45,RPC message packing and unpacking implemented,jytug,https://api.github.com/repos/pytorch/pytorch/git/commits/ebe6f40fcef2b271e58b99ffccaf664b233b2a45
f8e94d0d8b90a762d35f59ec8a97553d3522b920,Implement DataChannel (MPI and TCP) (#8),VirrageS,https://api.github.com/repos/pytorch/pytorch/git/commits/f8e94d0d8b90a762d35f59ec8a97553d3522b920
e78aa4bb84fe8dd51f6aa6738f0aac9d859ba2ed,Implement CommandChannel with ZMQ.,0mp,https://api.github.com/repos/pytorch/pytorch/git/commits/e78aa4bb84fe8dd51f6aa6738f0aac9d859ba2ed
79c04d32dc7b2f4ca469ab11051c8be8ec380074,"add an option to use a resnet network instead of alexnet

Summary: add an option to use a resnet network instead of alexnet. Modified the resnet.create_resnet50 function slightly to allow specifying different kernel/stride parameters so we can adapt resnet to our image size.

Differential Revision: D4472535

fbshipit-source-id: ed06acf52f6425a1e04d047548eb3c70388d74aa",seansnyder,https://api.github.com/repos/pytorch/pytorch/git/commits/79c04d32dc7b2f4ca469ab11051c8be8ec380074
44196955e2adcc2ec685864b0a815ed1464db32b,"ByteTensor should be unsigned (#664)

ByteTensor should be unsigned",thuyen,https://api.github.com/repos/pytorch/pytorch/git/commits/44196955e2adcc2ec685864b0a815ed1464db32b
e4886f6589ce12c0dff631cedf2846d173795d5b,VolumetricFractionalMaxPooling like spatial,bottler,https://api.github.com/repos/pytorch/pytorch/git/commits/e4886f6589ce12c0dff631cedf2846d173795d5b
de4659659bf1564ab7f775e6ee63c3e5b5ac4f4c,The RNNCell's example can not run correctly,zhoumingjun,https://api.github.com/repos/pytorch/pytorch/git/commits/de4659659bf1564ab7f775e6ee63c3e5b5ac4f4c
79263243853f86b0974107d29e8ce70ca4ba5f51,Corrected parameter typo in Adam docstring (#697),mccajm,https://api.github.com/repos/pytorch/pytorch/git/commits/79263243853f86b0974107d29e8ce70ca4ba5f51
5eab428294d47e02853e9f1251165b9ea71bd3cc,Qualify nullptr_t with std::.,cwhipkey,https://api.github.com/repos/pytorch/pytorch/git/commits/5eab428294d47e02853e9f1251165b9ea71bd3cc
306fde233ab1f2983fc755e2ee77ce555d6d47fc,"Accept optional blob map for InferShapesAndTypes

Summary:
Shape inference allows Caffe2 to compute shapes of blobs without running a model. Update InferShapesAndTypes() to accept an optional blob:dimensions map so that external input blobs do not need to be part of the workspace.

InferShapesAndTypes() in workspace.py conditionally calls the ...from_workspace or ...from_map bindings. Note I favored a small amount of code duplication here for the sake of readability. InferShapesAndTypes() in operator.cc has been refactored into mirrored entry points, invoking a common helper.

Other minor changes to address linter warnings.

Reviewed By: dzhulgakov

Differential Revision: D4524873

fbshipit-source-id: 56f863b759c016d7f23523f06fda3aa5bba22357",andrewwdye,https://api.github.com/repos/pytorch/pytorch/git/commits/306fde233ab1f2983fc755e2ee77ce555d6d47fc
6c77fa91211f398d9cd5d9fe1a4407878b915ef5,Changes in RNNBase and Embedding for compatibility with DataParallel (#660),bmccann,https://api.github.com/repos/pytorch/pytorch/git/commits/6c77fa91211f398d9cd5d9fe1a4407878b915ef5
4d37ef878cad7391d3ddf5251602cbecf7fe9944,Remove view on data and target tensors of dim 1 in TensorDataset (#609),zhtvk,https://api.github.com/repos/pytorch/pytorch/git/commits/4d37ef878cad7391d3ddf5251602cbecf7fe9944
3e08beb75e81c53a0fca57d496d55f1d3330e6dd,"implement Float16EncodeOp and Float16DecodeOp

Summary: casting between fp16 and fp32

Reviewed By: dzhulgakov

Differential Revision: D4526415

fbshipit-source-id: ebffb00ae12c6bcba79096b13e84ce55ef3f02bb",volkhin,https://api.github.com/repos/pytorch/pytorch/git/commits/3e08beb75e81c53a0fca57d496d55f1d3330e6dd
407a92dc269aa914311f994e9a4b15f5c39ec8dc,"std::min() requires same type (#732)

* std::min() requires same type

* cast buffer instead

* declare buffer_size as int64_t",kashif,https://api.github.com/repos/pytorch/pytorch/git/commits/407a92dc269aa914311f994e9a4b15f5c39ec8dc
e52676b27243e037177bfb47a64846bd906ea636,"Delete SerializeToString() call in class Model(), workspace.py

Summary:
.In Tutorial, I found it not correct when calling Model(). After that changing, It works.
Closes https://github.com/caffe2/caffe2/pull/148

Reviewed By: bwasti

Differential Revision: D4556894

Pulled By: Yangqing

fbshipit-source-id: 949a8d0496861f19869436908ffe1ef1a0f853b1",ezineo,https://api.github.com/repos/pytorch/pytorch/git/commits/e52676b27243e037177bfb47a64846bd906ea636
524bc07973287b7a06e78ce9ff1d1f8de0822cb8,"Change the schema of IndexLoad & IndexFreeze so that state change is captured by the framework

Summary: These operators update the state of the instance and therefor should have the instance in the output list.

Reviewed By: xianjiec

Differential Revision: D4554773

fbshipit-source-id: 556d484fcf58878308aa6b0f7cd7ea2446d3f29e",kittipatv,https://api.github.com/repos/pytorch/pytorch/git/commits/524bc07973287b7a06e78ce9ff1d1f8de0822cb8
6aaa14f5fe267f9179dad8f76f26652d1507ffe2,Fix LSTMCell Doc Typo (#743),dexter1691,https://api.github.com/repos/pytorch/pytorch/git/commits/6aaa14f5fe267f9179dad8f76f26652d1507ffe2
593f867e3ef74e038c1b282a06d0bbb7c06ad607,"Fixed a simple compiling erroin mac OS #745. (#746)

Signed-off-by: Zhou Chang <achang.zhou@gmail.com>",Teaonly,https://api.github.com/repos/pytorch/pytorch/git/commits/593f867e3ef74e038c1b282a06d0bbb7c06ad607
336eeee895e480ae69b6d5917948fe6b24334c55,"kernel_size as the default stride for avg_pool1d (#744)

Following the documentation, let stride to be kernel_size if stride is not provided.",supakjk,https://api.github.com/repos/pytorch/pytorch/git/commits/336eeee895e480ae69b6d5917948fe6b24334c55
a217fefee1b1b00d44ac64a4c71cb41ac44d3bcc,"Update rnn.py

Fixed a problem with outputting the RuntimeError if arguments are incorrect in cudnn/rnn.py",bdfhjk,https://api.github.com/repos/pytorch/pytorch/git/commits/a217fefee1b1b00d44ac64a4c71cb41ac44d3bcc
93795406c5468b769d64d9701f3bfdcf70d944bd,"Adapt NLU proj code for Caffe2 RecurrentNetworkOp changes

Summary: Updates function revise_recurrent_network_op() which supports cloning recurrent networks by adding a blob-name prefix to string arguments to maintain correspondence. Previously relied on many hard-coded indices referring to the positions of arguments and inputs of RecurrentNetworkOp and its corresponding gradient operator, and therefore broke when the implementation changed. This fix should make it more general and robust

Differential Revision: D4559768

fbshipit-source-id: fb85b0b1ffb1393dc84760d6ae5dc473e8b764b0",jhcross,https://api.github.com/repos/pytorch/pytorch/git/commits/93795406c5468b769d64d9701f3bfdcf70d944bd
81d932b161bcac4fb1fe2ce5ae30adfffbc09790,"Add LeakyReluOp to caffe

Summary: Adds LeakyRelu to caffe2 with a test.

Reviewed By: bwasti

Differential Revision: D4511970

fbshipit-source-id: a7189c691ec1813b304bf04f2b73f1c61acd08e2",tullie,https://api.github.com/repos/pytorch/pytorch/git/commits/81d932b161bcac4fb1fe2ce5ae30adfffbc09790
3098bef94e36536af78dd37da89d1475ab5877be,"Getting things in sync with the internal repo

This file was moved.",JoelMarcey,https://api.github.com/repos/pytorch/pytorch/git/commits/3098bef94e36536af78dd37da89d1475ab5877be
945e75bd3a369804acd28773874ae69195498f5e,"Remove openmp parallel for in caffe2

Summary: Task L1 items: Replace CAFFE2_OMP_PARALLEL_FOR with TODO and remove macro definition.

Reviewed By: ajtulloch, Yangqing

Differential Revision: D4565679

fbshipit-source-id: 8185af2b77b230159058c0a756a0da25ebcf3d0f",pestevez,https://api.github.com/repos/pytorch/pytorch/git/commits/945e75bd3a369804acd28773874ae69195498f5e
1a5cae734013f0f18f78771597c08de256bc0bab,"Add busy-poll option in TCP transport

Summary: Ideally we would want the driver to busy-poll for us. In absence of driver support, spinning with MSG_DONTWAIT flag seems to be helping a lot too. Of course, we pay the price of burning one core for polling. Sigh.

Reviewed By: pietern

Differential Revision: D4576242

fbshipit-source-id: 85d9e1b786fbb6053864fba80f3e5ecc80fe221d",plapukhov,https://api.github.com/repos/pytorch/pytorch/git/commits/1a5cae734013f0f18f78771597c08de256bc0bab
04aba1caec28dcc7517c85b40addb5a7741683fa,Fix cuDNN dropout desc for multi-gpu (#772),csarofeen,https://api.github.com/repos/pytorch/pytorch/git/commits/04aba1caec28dcc7517c85b40addb5a7741683fa
49295ebe54511b9c2ab9a926a7ce7d89c691dbc2,Add sequential to documentation,chsasank,https://api.github.com/repos/pytorch/pytorch/git/commits/49295ebe54511b9c2ab9a926a7ce7d89c691dbc2
a25c8555ebfb54e687c6528c6980967210bc17f7,Fixed paper references,dpkingma,https://api.github.com/repos/pytorch/pytorch/git/commits/a25c8555ebfb54e687c6528c6980967210bc17f7
68c9e3f232b75785d437f5d7c51b68480d59de6c,Fixed typo in GRUCell example,keskarnitish,https://api.github.com/repos/pytorch/pytorch/git/commits/68c9e3f232b75785d437f5d7c51b68480d59de6c
5b10411c8c525684b9727650e8d2f63eaf4b813b,"Fixed some mistakes in examples

Fixed mistakes in LSTMCell and GRUCell examples.",yunjey,https://api.github.com/repos/pytorch/pytorch/git/commits/5b10411c8c525684b9727650e8d2f63eaf4b813b
240372a991f380283d99bf4638855b6fac92aa27,Fixed topk documentation for largest=True,naifrec,https://api.github.com/repos/pytorch/pytorch/git/commits/240372a991f380283d99bf4638855b6fac92aa27
6073f9b46ccc1bc42aa0b9fbe49270d124a42cbf,"update table in README.md

it removes the empty top row",edgarriba,https://api.github.com/repos/pytorch/pytorch/git/commits/6073f9b46ccc1bc42aa0b9fbe49270d124a42cbf
64419a928de0769e0827fbd509c8dfb7693f703a,"Implement EnsureDenseOp and EnsureDenseGradientOp.

Summary:
This operator can always outputs dense gradients regardless of
the input gradients. For forward pass, it passes inputs to outputs in place.

Reviewed By: xianjiec

Differential Revision: D4582511

fbshipit-source-id: 7eb2c5d2142aa05d373f06cab1e7f89d8b747d34",xianxl,https://api.github.com/repos/pytorch/pytorch/git/commits/64419a928de0769e0827fbd509c8dfb7693f703a
838842d4b26ef341ca8d0e3fe9866d79a417e998,fix documentation error. [issue #790](https://github.com/pytorch/pytorch/issues/790) (#831),chenyuntc,https://api.github.com/repos/pytorch/pytorch/git/commits/838842d4b26ef341ca8d0e3fe9866d79a417e998
61bf08ca24bd96d824bbd5de048ef33f2ff9c7e3,Fix compilation for simd tensor add,Jokeren,https://api.github.com/repos/pytorch/pytorch/git/commits/61bf08ca24bd96d824bbd5de048ef33f2ff9c7e3
cd4ea4204851eb83bf170dab652938a25d40731a,"Allowing creation of random odd length arrays in RandGaussian

Summary: curandGenerateNormal can only generate arrays of multiple of 2 lengths. MSRAFill and GaussianFill operators use RandGaussian utility method which in turn uses curandGenerateNormal. This is a test which runs the operators on both devices to generate odd sized random arrays.

Differential Revision: D4602819

fbshipit-source-id: e65f5c731e925886cfa14afff482f7053bd020a0",dpacgopinath,https://api.github.com/repos/pytorch/pytorch/git/commits/cd4ea4204851eb83bf170dab652938a25d40731a
b87c113cf4f9bebb69b04e926556743e8f2b5f94,"CUDA documentation enhancement and docs versioning (#848)

* Add more detail to CUDA documentation

Also adds better cross-linking to the pages that discuss relevant topics.

* Adds recommendation to torch.save docs

* Make the version numbers for the docs dynamic

Might need tweaks for beta, 1.0, etc.",elistevens,https://api.github.com/repos/pytorch/pytorch/git/commits/b87c113cf4f9bebb69b04e926556743e8f2b5f94
8a0ebed4c9214ac91b739e6b3ad5bdd4794f88d3,"Caffe2: Tile operator

Summary: Caffe2: Tile operator

Differential Revision: D4630698

fbshipit-source-id: 1aa5c3c9d7fcfc17f78c80fd4b752595280266a0",shen-pan,https://api.github.com/repos/pytorch/pytorch/git/commits/8a0ebed4c9214ac91b739e6b3ad5bdd4794f88d3
48f087f6ce1aa2b7309805c68a6a756391bf599e,"C99 cleanup broke MSVC (#952)

* __pragma for MSVC.",shivak,https://api.github.com/repos/pytorch/pytorch/git/commits/48f087f6ce1aa2b7309805c68a6a756391bf599e
5e7f5db3328a07d54e9fdd91fc20254331f0f1e4,add subset samplers (#888),felixgwu,https://api.github.com/repos/pytorch/pytorch/git/commits/5e7f5db3328a07d54e9fdd91fc20254331f0f1e4
cdce8f0e524e6f8c1fa99f76756d825581fb2f60,"update gflags

Reviewed By: yfeldblum

Differential Revision: D4646271

fbshipit-source-id: 5d21407e815588ae2b016001b859a4816851ab00",igorsugak,https://api.github.com/repos/pytorch/pytorch/git/commits/cdce8f0e524e6f8c1fa99f76756d825581fb2f60
aec182ae72d51dad0f46cdfe7ff9a41380d7da35,Support half precision in baddbmm,guillaumekln,https://api.github.com/repos/pytorch/pytorch/git/commits/aec182ae72d51dad0f46cdfe7ff9a41380d7da35
f17cfe42936310a2e3fd573e1f4dec8c684d4003,sparse tensor operations (#735),martinraison,https://api.github.com/repos/pytorch/pytorch/git/commits/f17cfe42936310a2e3fd573e1f4dec8c684d4003
7d58765ceeca47e32d8f8d610eb7154bcedc2667,docs: Fixed example code bug in extending module doc.,yeelan0319,https://api.github.com/repos/pytorch/pytorch/git/commits/7d58765ceeca47e32d8f8d610eb7154bcedc2667
5b171ad7c2febf9a796f8f30e2f8bf1de975203d,remove misleading guide for BCELoss (#924),fehiepsi,https://api.github.com/repos/pytorch/pytorch/git/commits/5b171ad7c2febf9a796f8f30e2f8bf1de975203d
0d179aa8db5979e8629a21ae98d70f377b80109f,"Updated datasets.rst, combined all commits (#931)

Added MNIST in the docs

Updated incomplete cifar doc

Updated the datasets.rst to include all datasets",skrish13,https://api.github.com/repos/pytorch/pytorch/git/commits/0d179aa8db5979e8629a21ae98d70f377b80109f
2b1cd919ce01db7b696ae557526c36e069ae970a,Update extending.rst (#933),alexis-jacq,https://api.github.com/repos/pytorch/pytorch/git/commits/2b1cd919ce01db7b696ae557526c36e069ae970a
9b626a8047c86f0ee19261d7caefa7242d2e8731,Fix documentation - replace 'matrix' with 'vector' (#951),ar-nowaczynski,https://api.github.com/repos/pytorch/pytorch/git/commits/9b626a8047c86f0ee19261d7caefa7242d2e8731
6830d561038c80a3fe408e275a07b07c26cc7b56,"CodeMod: google::ProgramUsage to gflags::ProgramUsage

Summary:
CodeMod: `google::ProgramUsage` to `gflags::ProgramUsage`.

For gflags, `namespace google` is deprecated in favor of `namespace gflags`.

Automated with:

```lang=bash
hg grep -lw google::ProgramUsage | xargs perl -pi -e 's,\bgoogle(::ProgramUsage)\b,gflags\1,g'
```

Reviewed By: igorsugak

Differential Revision: D4665851

fbshipit-source-id: 9790c74f1c42d74043b94ee356f8e3cc3622f132",yfeldblum,https://api.github.com/repos/pytorch/pytorch/git/commits/6830d561038c80a3fe408e275a07b07c26cc7b56
e75221e316e17a78976a920561ac1899edb03e1c,"Add eval net to two tower workflow

Summary: The evaluation part of the two tower workflow is missing. This diff is to complete it. Part of the newly added functions can be used for other workflows, eg, feed. As the eval workflow in different workflows will be overlapped, a generic eval workflow will be added in a separate diff.

Reviewed By: kennyhorror

Differential Revision: D4646880

fbshipit-source-id: 4d6eb35df10f6f613533d442f2a04dc0332386f8",chocjy,https://api.github.com/repos/pytorch/pytorch/git/commits/e75221e316e17a78976a920561ac1899edb03e1c
8de1db9eb6cbafeba8ba78fdc8f9553c62fdb073,"Implement recurrent attention in C2

Summary: Super rough implementation of recurrent attention. Planning to factor out the common code between the two functions as well as train and eval. I want to get this out and get eyes on it sooner rather than later

Differential Revision: D4647837

fbshipit-source-id: 54bc4e8ed0df6f04c86c425926decbe89f73b068",jamesr66a,https://api.github.com/repos/pytorch/pytorch/git/commits/8de1db9eb6cbafeba8ba78fdc8f9553c62fdb073
a462edd0f6696a4cac4dd04c60d1ad3c9bc0b99c,"Docs(RNN|GRU|LSTM): Note dropout applies to all layers *except* the last layer (#961)

This is an important clarification to make as otherwise users are misled as to where they may need to add dropout and to clarify the situation would need to delve into the backend implementation. 
https://github.com/pytorch/pytorch/blob/4647f753bc22d3fc4b57f326608663173109aeb0/torch/nn/_functions/rnn.py#L73",Smerity,https://api.github.com/repos/pytorch/pytorch/git/commits/a462edd0f6696a4cac4dd04c60d1ad3c9bc0b99c
965a7daf9b85c4fabe98115e9c78a26eac4888c2,"Implement MILSTM in caffe2

Summary:
Created a new function with specifics related to MI LSTM implementation in caffe2
See https://arxiv.org/pdf/1606.06630.pdf for details.
See D4478877 for the implementation of the same in tensorflow

Reviewed By: jhcross

Differential Revision: D4669882

fbshipit-source-id: 095bbcf187dbdac2cd79558ff0c8f9f67d8af639",karthikprasad,https://api.github.com/repos/pytorch/pytorch/git/commits/965a7daf9b85c4fabe98115e9c78a26eac4888c2
31b72b90048a091e32d6d34d0dd3b0779b99d961,"move reshape out of utility_ops

Summary: move reshape as individual op

Reviewed By: ajtulloch

Differential Revision: D4690919

fbshipit-source-id: a84859d738039125a4f4122365619b69d5990427",zhpzuo,https://api.github.com/repos/pytorch/pytorch/git/commits/31b72b90048a091e32d6d34d0dd3b0779b99d961
2f5c215d346f358c562970324f34bae5e3328e41,"Update setup.py (#981)

Adding `description` to `setup.py`",lowks,https://api.github.com/repos/pytorch/pytorch/git/commits/2f5c215d346f358c562970324f34bae5e3328e41
b9c816a7968966376869e1bc938b6f46c72fb9c8,Fix run_test.sh --coverage option. (#983),nicolasdespres,https://api.github.com/repos/pytorch/pytorch/git/commits/b9c816a7968966376869e1bc938b6f46c72fb9c8
997312c2339cda8d26f92587c51eab5f18ef9d4f,"Add WeightedRandomSampler (#980)

Samples elements from `[0,..,len(weights)-1]` with given probabilities (weights). So far there is no mean to either introduce sample weights in loss functions or while sampling from a dataset. This is an attempt to add the functionality for the latter issue.",DmitryUlyanov,https://api.github.com/repos/pytorch/pytorch/git/commits/997312c2339cda8d26f92587c51eab5f18ef9d4f
170d790b6643d5010531607319c2a30583cd04b1,"fix doc of conv3d in conv.py (#989)

the second dimension should be height.",wangg12,https://api.github.com/repos/pytorch/pytorch/git/commits/170d790b6643d5010531607319c2a30583cd04b1
a5a5d00b87bcaa4349749454bf85f4fcf93b6956,"Fixed a bug: 'ModelTrainerLog instance has no attribute 'external_loggers''

Summary: Fixed a bug (AttributeError: ModelTrainerLog instance has no attribute 'external_loggers', at File ""caffe2/python/experiment_util.py"", line 101) when no external_loggers is passed to ModelTrainerLog().

Differential Revision: D4697197

fbshipit-source-id: 1c770c366d87ea474bcf40ab289b67c76648d48b",newstzpz,https://api.github.com/repos/pytorch/pytorch/git/commits/a5a5d00b87bcaa4349749454bf85f4fcf93b6956
9004652c7ba1fd8440951cb9f9e1c8bc0302b5aa,updated the documentation to remove the unnecessary copy grads when using multiprocessing,jontremblay,https://api.github.com/repos/pytorch/pytorch/git/commits/9004652c7ba1fd8440951cb9f9e1c8bc0302b5aa
436193bf371cc55e0e3d02d7ab24477eb305d701,"fix minor typo in math_{cpu.cc,gpu.cu}

Summary: Closes https://github.com/caffe2/caffe2/pull/196

Reviewed By: pietern

Differential Revision: D4703131

Pulled By: Yangqing

fbshipit-source-id: cb6e61a41a858e9cb164697a585ef257a8d0530e",thinxer,https://api.github.com/repos/pytorch/pytorch/git/commits/436193bf371cc55e0e3d02d7ab24477eb305d701
a74c2bcda8d73a7b57c5e76e247b85f94998b213,"Fix build_ios.sh bug (#194) due to name collision.

Summary: Closes https://github.com/caffe2/caffe2/pull/200

Differential Revision: D4704479

Pulled By: Yangqing

fbshipit-source-id: 7a618fa2cd57fd2ead5cede5d5aed033284ea67e",nicodjimenez,https://api.github.com/repos/pytorch/pytorch/git/commits/a74c2bcda8d73a7b57c5e76e247b85f94998b213
8241cd7b6ed1425eeb88fd380090575978e358f4,"Fix compilation error when compiling with 'clang -x cuda'.

Functions vFetch and vStore are not found by ADL with clang,
so they need to be declared before usage in ReduceCopy.",ilya-biryukov,https://api.github.com/repos/pytorch/pytorch/git/commits/8241cd7b6ed1425eeb88fd380090575978e358f4
37ebbc2809ca6ccac69f2d67e9adc892de56bcfc,the length of any item in padded_sequence should be greater than 0 (#1013),Yugnaynehc,https://api.github.com/repos/pytorch/pytorch/git/commits/37ebbc2809ca6ccac69f2d67e9adc892de56bcfc
7654b3f49ec9d394cf3a15d965457d2840d70c9b,Add function to compute cross_entropy for 2D image (#802),wkentaro,https://api.github.com/repos/pytorch/pytorch/git/commits/7654b3f49ec9d394cf3a15d965457d2840d70c9b
1513b1de6b76561da840b7735431570629eea906,"Add ResizeNearest operator

Summary: This adds a nearest neighbor interpolation resizing operator to caffe2. CPU only, NCHW only, no gradients. Also adds torch2caffe support. This is probably not optimal in terms of performance, but it works.

Reviewed By: ajtulloch

Differential Revision: D4724244

fbshipit-source-id: b8295061141fb513da84acf91fdfd67264119059",jonmorton,https://api.github.com/repos/pytorch/pytorch/git/commits/1513b1de6b76561da840b7735431570629eea906
b3c0aa3b7d9c2e0f3ef4ca5b3b4d895dfb777ec3,fix a typo in ffi doc (#1055),fyu,https://api.github.com/repos/pytorch/pytorch/git/commits/b3c0aa3b7d9c2e0f3ef4ca5b3b4d895dfb777ec3
d9678c2e34908bb3d3a36ac673ae14697590de45,Correct typo in batchnorm documentation,jihunchoi,https://api.github.com/repos/pytorch/pytorch/git/commits/d9678c2e34908bb3d3a36ac673ae14697590de45
42036871e98442f5618e946994b21668d52bd803,"Fix windows build

Summary: Closes https://github.com/caffe2/caffe2/pull/214

Differential Revision: D4755224

Pulled By: asaadaldien

fbshipit-source-id: 8a3c6d13319aecc0bf700bad2b3e9ed2a53571e9",asaadaldien,https://api.github.com/repos/pytorch/pytorch/git/commits/42036871e98442f5618e946994b21668d52bd803
d3334db6274d7a3cd07f20d583056e453dc8134d,"adding batch triangular factorization and solves, add IntegerTensor to cwrap",zkolter,https://api.github.com/repos/pytorch/pytorch/git/commits/d3334db6274d7a3cd07f20d583056e453dc8134d
99bfd36a04c67b5d841070b20e2c4490c5a7bbdb,"CRF layer in caffe2

Summary:
This is implementation of a CRF layer in caffe2 according to this paper: https://arxiv.org/abs/1603.01360
Currently this implementation works only for batch_size = 1

Reference implementations:

- Tensorflow:
 https://github.com/tensorflow/tensorflow/blob/63a21e054007d86269ed1ad0145ebce04ee57a81/tensorflow/contrib/crf/python/ops/crf.py

- Theano:
https://github.com/glample/tagger/blob/master/model.py#L286

Differential Revision: D4644004

fbshipit-source-id: bf0801fd8562d11dca3fefe371c3d85e1dd69ccc",ahhegazy,https://api.github.com/repos/pytorch/pytorch/git/commits/99bfd36a04c67b5d841070b20e2c4490c5a7bbdb
1d656b6769469a4e44373dbd8e2d5b41ade78770,"Ensure displayed progress in ProgressMonitor is between 0 and 100%.
Fixes #1086",recastrodiaz,https://api.github.com/repos/pytorch/pytorch/git/commits/1d656b6769469a4e44373dbd8e2d5b41ade78770
476d85dd3f8ee48f6affc836d5c7fbd8ccfab200,DataLoader: Fix batch data type for numpy array (#1074),zuoxingdong,https://api.github.com/repos/pytorch/pytorch/git/commits/476d85dd3f8ee48f6affc836d5c7fbd8ccfab200
d25433a099d156efe57c06b4e7206e9cfb7400ad,Fix docker build commands (#1103),mgalkov,https://api.github.com/repos/pytorch/pytorch/git/commits/d25433a099d156efe57c06b4e7206e9cfb7400ad
f2c1071c335040558b4d64322082e00d2e4c2026,Adaptive max and average pooling (1D & 2D) (#1084),xternalz,https://api.github.com/repos/pytorch/pytorch/git/commits/f2c1071c335040558b4d64322082e00d2e4c2026
73b18e7ccf1456c1e55e3b20c7c5425c4d1f3dbc,"Enables checkpointing for dper2.

Reviewed By: azzolini

Differential Revision: D4716571

fbshipit-source-id: c4d71ed676d9465290c2e3fcb26efbbecc72cf72",boryiingsu,https://api.github.com/repos/pytorch/pytorch/git/commits/73b18e7ccf1456c1e55e3b20c7c5425c4d1f3dbc
825165358558e5dd967c6946bf2540c28911316e,"caffe2: relax PartitionOp constraints

Summary: We actually copy items inside, so no need to limit this to POD types.

Reviewed By: dzhulgakov

Differential Revision: D4768652

fbshipit-source-id: 98f71b78a7c1dd4a2a2e1bff096d6bf63a0c8f50",JohnRambo,https://api.github.com/repos/pytorch/pytorch/git/commits/825165358558e5dd967c6946bf2540c28911316e
9c58341809c2f9891ade71e7b0dd8e9c232a2d6c,"codemod: use `<>` includes for gtest headers

Summary: These are system headers and so should be included via `<>`.

Reviewed By: yfeldblum

Differential Revision: D4783480

fbshipit-source-id: 979670b594859b45560cead34f615442dfcc9f8b",andrewjcg,https://api.github.com/repos/pytorch/pytorch/git/commits/9c58341809c2f9891ade71e7b0dd8e9c232a2d6c
0d908d813b3a70322942df66c7a277c1db558623,Implements Cumsum function for autograd (#1122),bunelr,https://api.github.com/repos/pytorch/pytorch/git/commits/0d908d813b3a70322942df66c7a277c1db558623
761eef1f19825909525219e4abd8a2c52594397f,Minor typo fix in `backward` function in `torch/autograd/variable.py` (#1143),ironmaniiith,https://api.github.com/repos/pytorch/pytorch/git/commits/761eef1f19825909525219e4abd8a2c52594397f
d93328711457c38a77506790764e9e5ed2c8dc47,"Add a barrier after verification iteration in benchmarks to prevent a race with regular iterations

Summary: Verification was sometimes failing for allreduce halving-doubling. Pieter noticed that it is due to verification step racing with the regular iterations.

Reviewed By: pietern

Differential Revision: D4804558

fbshipit-source-id: f645cb2e332e449a993a634c5bdb42c2dcb8613b",wesolwsk,https://api.github.com/repos/pytorch/pytorch/git/commits/d93328711457c38a77506790764e9e5ed2c8dc47
5c79046d39a0d585744b8c195bc3500a657707f7,Use persistent tensor to store exp_inf (part of optimizer's state) (#1152),tudor-berariu,https://api.github.com/repos/pytorch/pytorch/git/commits/5c79046d39a0d585744b8c195bc3500a657707f7
d8c65cc52abce680a886a8ac690941081950a8b7,"A more deterministic way to find old C1 model

Summary: minor fix about C1 model translator

Reviewed By: Yangqing

Differential Revision: D4807165

fbshipit-source-id: 0149e2655d2901b23a37e92f61d9dd678cf6ee69",harouwu,https://api.github.com/repos/pytorch/pytorch/git/commits/d8c65cc52abce680a886a8ac690941081950a8b7
274b5c900395715f359824609f67956dbca90343,Allow unhashable inputs to parallel_apply,nhynes,https://api.github.com/repos/pytorch/pytorch/git/commits/274b5c900395715f359824609f67956dbca90343
4e4cfd8b2b96883a70becd75b647ff398ef583fc,"Fix main()s to call folly::init/initFacebook/registrationComplete (part 14)

Summary:
Required for D4821763
Based on targets from https://fb.facebook.com/groups/fbcode/permalink/1304073246296178/ (I also excluded those targets which do not depend on folly:singleton).

Reviewed By: meyering

Differential Revision: D4832492

fbshipit-source-id: fcb4ce42e9e5359d4752769f77d7271e550201fe",andriigrynenko,https://api.github.com/repos/pytorch/pytorch/git/commits/4e4cfd8b2b96883a70becd75b647ff398ef583fc
75a635630d30d81baf14b59acebe6586668fcd43,"Update to ignore zero targets

If the target is zero, loss and gradient of input are set to zero. It
is useful for variable-length natural language generation models.",jnhwkim,https://api.github.com/repos/pytorch/pytorch/git/commits/75a635630d30d81baf14b59acebe6586668fcd43
66a20e5c328836c1eb720cf4e2eb916366aae487,"Support TORCH_NVCC_FLAGS environment variable

This is already supported in cutorch since august 2016, and is used in
pytorch integration (to reduce the binary size).",thomas-riccardi,https://api.github.com/repos/pytorch/pytorch/git/commits/66a20e5c328836c1eb720cf4e2eb916366aae487
441d75ce569f89bad3e2f1f2a2075e68ae3bc76b,Adapts basic operations to new THXVector interface,pedropgusmao,https://api.github.com/repos/pytorch/pytorch/git/commits/441d75ce569f89bad3e2f1f2a2075e68ae3bc76b
64ee4056d7a73ee2343c536e61fcf4b1787fd630,updated docker image inside the docs (#1216),wilk,https://api.github.com/repos/pytorch/pytorch/git/commits/64ee4056d7a73ee2343c536e61fcf4b1787fd630
f6fef3718e6f5acae33ae89a62fb12c68d93adb7,fix typo in autograd.rst (#1219),quanvuong,https://api.github.com/repos/pytorch/pytorch/git/commits/f6fef3718e6f5acae33ae89a62fb12c68d93adb7
ade105fb7c198546a0a099ddf7ed2c87a9add5cd,update README to install pyyaml from conda (#1231),eklitzke,https://api.github.com/repos/pytorch/pytorch/git/commits/ade105fb7c198546a0a099ddf7ed2c87a9add5cd
cb66e9cf78e9b4dbe8323a263ecf00f8eda27199,torch.diag bug fix (#1251),ncullen93,https://api.github.com/repos/pytorch/pytorch/git/commits/cb66e9cf78e9b4dbe8323a263ecf00f8eda27199
dd923cf052e7f45aa116326867b7d57c2fc57fc1,"Unmask operator in Caffe2

Summary:
A CPU implementation for unmask operator in caffe2.
There's also a small bug in mask operator, fix it as well.

Reviewed By: ender-wieczorek

Differential Revision: D4896351

fbshipit-source-id: 887d1beb66fe93ea2da1c4e165fce2e026907726",shenxiul,https://api.github.com/repos/pytorch/pytorch/git/commits/dd923cf052e7f45aa116326867b7d57c2fc57fc1
a220f2c3aacbebc46c58f33c182ba18e26dde328,"Fix group-convolution w/o biases on CPU. (#1273)

* Fix group-convolution w/o biases on CPU.

Not having this guard will cause a crash further down in the `cat`
function when it uses the first element in the passed list to create a
new tensor. (And even after that, cat doesn't handle nulls well.)

* Added test for groupconv w/o bias on CPU.",lucasb-eyer,https://api.github.com/repos/pytorch/pytorch/git/commits/a220f2c3aacbebc46c58f33c182ba18e26dde328
ab77742f6e522fd7492b4d50e4ddd68e2bb05cb3,"Add some missing documentation for arguments.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>",ezyang,https://api.github.com/repos/pytorch/pytorch/git/commits/ab77742f6e522fd7492b4d50e4ddd68e2bb05cb3
fc19473501cf6aad4c839483d72ce38eb8fec566,Corrections in legacy modules. (#1286),DigiDigi,https://api.github.com/repos/pytorch/pytorch/git/commits/fc19473501cf6aad4c839483d72ce38eb8fec566
6aa22beb86e90ad57aef5371e766c8f14cbf839b,Fix loss.py docs (#1296),Varal7,https://api.github.com/repos/pytorch/pytorch/git/commits/6aa22beb86e90ad57aef5371e766c8f14cbf839b
a35f507532de66e563034c7fa41a40e36acd39a2,Update functional.py (#1298),shubhamjain0594,https://api.github.com/repos/pytorch/pytorch/git/commits/a35f507532de66e563034c7fa41a40e36acd39a2
a8e6610e3d4504c1ed10227d1385f8e89a603771,Fix argument typo in pad_packed_sequence docstring (#1300),BenFielding,https://api.github.com/repos/pytorch/pytorch/git/commits/a8e6610e3d4504c1ed10227d1385f8e89a603771
b89688658c8c37a3f18071c8a0adb67da2ab1540,"Missing CUDA_NVCC_FLAGS & CUDA_HOST_COMPILER flags at GPU arch detection.

Summary:
This tiny patch fix missing ```CUDA_NVCC_FLAGS``` & ```CUDA_HOST_ARCH``` from ```caffe_detect_installed_gpus()```.

-----------------

People may want define their custom flags or compilers that are more CUDA compatible. Automatic gpu arch detection ignores these flags and fail. Example of such custom flags:

```
cmake . \
-DCUDA_ARCH_NAME=""Auto"" \
-DCUDA_HOST_COMPILER=""/usr/bin/gcc5""
```

* Autodetection part fails regardless proper compiler flags are passed, due to system gcc 7.0 that doesnt work with CUDA thus all arch will be enabled:
```
-- The C compiler identification is GNU 7.0.1
-- The CXX compiler identification is GNU 7.0.1
...//\\...
-- CUDA detected: 8.0
...//\\...
-- Automatic GPU detection failed. Building for all known architectures.
-- Added CUDA NVCC flags for: sm_20 sm_21 sm_30 sm_35 sm_50 sm_60 sm_61
```
* Patch fix the autodetection time as expected:
```
$ cmake ../ -DCUDA_NVCC_FLAGS=""-Xcompiler=-std=c++03 -I/usr/include/cuda/""
-- The C compiler identification is
Closes https://github.com/caffe2/caffe2/pull/288

Differential Revision: D4914215

Pulled By: Yangqing

fbshipit-source-id: c407a750e03cb163f9d57f9f6403042704046014",cbalint13,https://api.github.com/repos/pytorch/pytorch/git/commits/b89688658c8c37a3f18071c8a0adb67da2ab1540
9ef30b337e7cc9b2dc7c5debadf9c08bc120b700,"Add six to Tegra X1 install script

Summary:
When compiling Caffe2 on a Jetson TX2 using JetPack 3.0, the compilation with the Tegra X1 build script runs through perfectly fine. However, when running

    from caffe2.python import workspace

the following error shows up:

> ImportError: No module named six

After installing `six` manually using

    sudo pip install six

this works fine. I thus added the `six` module to the install script.

I assume this will also be required for the `build_raspbian.sh` script, however as I could test this, I didn't add it (yet).
Closes https://github.com/caffe2/caffe2/pull/293

Differential Revision: D4914121

Pulled By: Yangqing

fbshipit-source-id: 75947e8c295e1f5ad3f480a025fe8518dd91a957",HBadertscher,https://api.github.com/repos/pytorch/pytorch/git/commits/9ef30b337e7cc9b2dc7c5debadf9c08bc120b700
6cae3fa896da46bf3d3748ac134312a4052ef110,Typo in Build version of ubuntu (#294),ajinkyakolhe112,https://api.github.com/repos/pytorch/pytorch/git/commits/6cae3fa896da46bf3d3748ac134312a4052ef110
8b5782ed5c02a37520ae484276fcd04ff40b08fa,"Weighted sampling dequeue operator

Summary:
Similar to SafeDequeueBlobsOp, but add weight-based sampling for reading from multiple input BlobsQueue.

WeightedSampleDequeueBlobsOp will take a vector of weights (each weight is mapped to one input blob queue).
Based on probability, we will choose which BlobQueue to fetch.
WeightedSampleDequeueBlobsOp shall stop when any of input BlobQueue is empty.

Reviewed By: dzhulgakov

Differential Revision: D4905160

fbshipit-source-id: 5b1551e2250569f933a6c01ed04442843c5e0cb6",rayleichen,https://api.github.com/repos/pytorch/pytorch/git/commits/8b5782ed5c02a37520ae484276fcd04ff40b08fa
f750a2d2df8407c4b53855f67d7303cce3aba577,"fix a few typos

Summary:
fix typo: Dimention, probablity
Closes https://github.com/caffe2/caffe2/pull/310

Differential Revision: D4915798

Pulled By: Yangqing

fbshipit-source-id: 3a16d3adc469c9930ce0dad8584c4678b3c3b5c0",inspire99,https://api.github.com/repos/pytorch/pytorch/git/commits/f750a2d2df8407c4b53855f67d7303cce3aba577
5b6fb047aa8be0ab615d7b1840ce687970b8b044,"Fix parallel build support in makefile

Summary:
Top-level makefile had `make` hardcoded, resulting in slow build and the following message when following installation instructions:

    warning: jobserver unavailable: using -j1. Add `+' to parent make rule.

Replacing this recursive make command with the variable MAKE fixes the issue.
Closes https://github.com/caffe2/caffe2/pull/324

Differential Revision: D4920978

Pulled By: Yangqing

fbshipit-source-id: 1e75ab41786e52d1b7abcc2c46ad1088880d8c1d",thinkski,https://api.github.com/repos/pytorch/pytorch/git/commits/5b6fb047aa8be0ab615d7b1840ce687970b8b044
95f123a83eff03bfeb93e111c08ecd4bf31b2256,"fix download progress bar's percentage exceed 100%

Summary:
downloaded_size need to be added with the length of returned data_chunk.
When the last block's size less than chunk, the percentage should exceed 100%
Closes https://github.com/caffe2/caffe2/pull/329

Differential Revision: D4922227

Pulled By: Yangqing

fbshipit-source-id: 7d05d9bbf2dad0a9d330be96b60e658908185a46",junluan,https://api.github.com/repos/pytorch/pytorch/git/commits/95f123a83eff03bfeb93e111c08ecd4bf31b2256
6089900011df12f082b3b28c0238874faf3a5d9d,"grammar/typo: ""There's 3"" -> ""There are three""

Summary: Closes https://github.com/facebookincubator/gloo/pull/27

Differential Revision: D4919746

Pulled By: pietern

fbshipit-source-id: 35733b75fc169d2ccff8b10df013eed8c279dfd5",ajschumacher,https://api.github.com/repos/pytorch/pytorch/git/commits/6089900011df12f082b3b28c0238874faf3a5d9d
884690adb35b9411383ac41a4059543b30d97289,"build_ios.sh comments fixes

Summary:
Changed _Android_ to _iOS_ in the comments in scripts/build_ios.sh.
Closes https://github.com/caffe2/caffe2/pull/364

Differential Revision: D4930101

Pulled By: Yangqing

fbshipit-source-id: 8f0a6aa1b43fd57c2f71f1c667c61d1f69b1e061",nayoo,https://api.github.com/repos/pytorch/pytorch/git/commits/884690adb35b9411383ac41a4059543b30d97289
199a09c7dd693106361f245aa9438245861f639b,"XCode -> Xcode

Summary: Insufferable Apple fanboys have burned this into my brain.

Reviewed By: Yangqing

Differential Revision: D4913772

fbshipit-source-id: 486c20e9c921",mboyd1017,https://api.github.com/repos/pytorch/pytorch/git/commits/199a09c7dd693106361f245aa9438245861f639b
8387bc4680643090ad4708017384fe6652d276bc,"Added Python_ADDITIONAL_VERSIONS to cmake so python 2 is default.

Summary:
When installing on systems such as Arch Linux where the default python version is 3 the build will fail. To fix this instead of changing the python link in the shell it is more efficient to set the default python version allowed by cmake.
Closes https://github.com/caffe2/caffe2/pull/361

Differential Revision: D4932214

Pulled By: Yangqing

fbshipit-source-id: 06997d2df68b8e4037d72fd49813f6f74ca7591b",kochie,https://api.github.com/repos/pytorch/pytorch/git/commits/8387bc4680643090ad4708017384fe6652d276bc
3b0069a014c05667b6802784c7f1f21936cfcb6c,"Expose operators execution statistics to python frontend.

Summary: To expose operators execution statistics in python, profiling measurements collected in ProfDAGNet class is leveraged. In current implementation, a new operator is defined that outputs the statistic data in a protobuf message. In the frontend, OperatorStatsContainer works as a wrapper to print ProfDAGNet statistics.

Differential Revision: D4923009

fbshipit-source-id: 18a6d76a405ef277a3fca7a312609051cf943207",zem7,https://api.github.com/repos/pytorch/pytorch/git/commits/3b0069a014c05667b6802784c7f1f21936cfcb6c
2994dd637791f96d86ef4ba06ce24c922bb669db,"Fix python support problems caused by building script errors.

Summary:
When trying to build caffe2 with python provided by homebrew, I find out there are some errors in the building scripts. The ""get_python_cmake_flags.py"" script is supposed to find out the correct python library and header file locations. However, due to these errors, this script does not function correctly. After building, caffe2 is linked against the default python library provided by Apple which causes a crash when trying to validate whether or not the installation is successful:
```shell
python -c 'from caffe2.python import core' 2>/dev/null && echo ""Success"" || echo ""Failure""
```
The fix is as simple as follows:

- Add ""shell"" so that command substitution could work under Makefile.

- Add blank spaces between -D options so that they are treated as options not makefile targets.

- Print the ""flags"" variable without the newline character so that they could be utilized by command substitution correctly.
Closes https://github.com/caffe2/caffe2/pull/391

Differential Revision: D4943212

Pulled By: Yangqing

fbshipit-source-id: 04d3595fa2d89fe57aed5b6a7a91a95114a82a1b",yangyanzhan,https://api.github.com/repos/pytorch/pytorch/git/commits/2994dd637791f96d86ef4ba06ce24c922bb669db
2a098fc20ef6d2f3f6cf6f09ca3dd0c41abf0c83,"string -> std::string in common_rtc.h

Summary:
In its current form, common_rtc.h can only be included in a file where
```
using namespace std;
```
comes before the include
Closes https://github.com/caffe2/caffe2/pull/398

Differential Revision: D4943125

Pulled By: Yangqing

fbshipit-source-id: 3ef15c9353e6dd7326fc5f60322049c9f594ee6c",nicolasvasilache,https://api.github.com/repos/pytorch/pytorch/git/commits/2a098fc20ef6d2f3f6cf6f09ca3dd0c41abf0c83
b3b66e3d00023c78415724e2b4d33b515a82fd46,"MKL related files with review comments incorporated

Summary:
This PR is based on commit ""977c6b3"" as this version allows MKL to use all the cores available.
All MKL related files are added here after incorporating review comments, major changes include

1. usage of Clang-format(Linter) with --style = Google
2. usage of macros for checking input and filter dimension in the mkl operators
3. merged Max and Average pooling functions
4. created a new folder for mkl related python scripts in Python folder and moved them there
5. there is no mkl_alexnet_test.py as that was redundant while convnet_benchmark.py does the same thing
Closes https://github.com/caffe2/caffe2/pull/270

Differential Revision: D4905219

Pulled By: Yangqing

fbshipit-source-id: e5f5b189714a835b93b9ebda24c52e09572dfca7",rgomathi,https://api.github.com/repos/pytorch/pytorch/git/commits/b3b66e3d00023c78415724e2b4d33b515a82fd46
41705ce7d5e797c1924224357bd329cebb6744fb,Add zero padding module (#1326),andrewgiessel,https://api.github.com/repos/pytorch/pytorch/git/commits/41705ce7d5e797c1924224357bd329cebb6744fb
afd01164f859d31b48d5bf4d27db21358f98a284,"Install missing headers.

Summary:
This PR installs missing include headers.
Closes https://github.com/facebookincubator/gloo/pull/30

Differential Revision: D4946478

Pulled By: pietern

fbshipit-source-id: da2d532afc43cf9e5e7fc764dc7821e2dfca6b37",hgaiser,https://api.github.com/repos/pytorch/pytorch/git/commits/afd01164f859d31b48d5bf4d27db21358f98a284
9f9a2da1a1aa6be5eeef06663a7ad592531dcdad,"Revert D4920719: [dper2][operator] ScaleGradientOp

Summary: This reverts commit 0e1e0888f79594be874fdbdda5ccef7389064c50

Differential Revision: D4920719

fbshipit-source-id: 1ca9dc329eaffeb2932267d631506bb124d4e7ae",jeffdunn,https://api.github.com/repos/pytorch/pytorch/git/commits/9f9a2da1a1aa6be5eeef06663a7ad592531dcdad
b93b525a1c6be4245407e6194262c5f1ef44aa99,"Enable specifying of margin in HingeEmbeddingLoss (#1378)

Previously it was not possible to set a value for the margin in the HingeEmbeddingLoss in the constructor. This patch fixes the issue and makes the loss behave as it is described in the docs. 

A discussion of this issue can be viewed here:
https://discuss.pytorch.org/t/issue-with-setting-margin-for-hingeembeddingloss/2088",macaodha,https://api.github.com/repos/pytorch/pytorch/git/commits/b93b525a1c6be4245407e6194262c5f1ef44aa99
a44317fea88adddded91e068088415de1e66fd4b,Change magma_sgesvd to magma_sgesdd which is significantly faster,aam-at,https://api.github.com/repos/pytorch/pytorch/git/commits/a44317fea88adddded91e068088415de1e66fd4b
75f1989bec555ea21836ad1906ac373f6434199c,Add nn.Bilinear and tests,uridah,https://api.github.com/repos/pytorch/pytorch/git/commits/75f1989bec555ea21836ad1906ac373f6434199c
d223d7170351974fef72aade6ad35b432b157c50,"Add shape inference function for RoiPool.

Summary: As the title.

Reviewed By: akyrola

Differential Revision: D4960241

fbshipit-source-id: d5f7d7c2eea72a75f810aa2f532965fff48f8388",ipiszy,https://api.github.com/repos/pytorch/pytorch/git/commits/d223d7170351974fef72aade6ad35b432b157c50
db1eb66456d1c7e31618fe427b72572f3ce2cc7a,corrected docstring for Dropout (#1404),tejaskhot,https://api.github.com/repos/pytorch/pytorch/git/commits/db1eb66456d1c7e31618fe427b72572f3ce2cc7a
79d4ac670cda0541ea338117ec981f84b84a9f25,Add map_location to load_url (#1418),bartolsthoorn,https://api.github.com/repos/pytorch/pytorch/git/commits/79d4ac670cda0541ea338117ec981f84b84a9f25
1040b5f91cdd61d1d1b7fbf0f02f39e65820988b,"Enable bitcode for iOS builds

Summary:
build_ios.sh now have `-fembed-bitcode` flags for cmake and passes these flags to build_host_protoc.sh (which now accepts optional argument `--other-flags`). That allows to use output libs (libCaffe2_CPU.a, libCAFFE2_NNPACK.a, libCAFFE2_PTHREADPOOL.a and libprotobuf-lite.a, libprotobuf.a respectively) in Xcode projects with bitcode enabled.

Bitcode is enabled by default in all projects since Xcode7, is crucial for slicing and is mandatory for watchOS targets. Enabling bitcode for target requires bitcode to be enabled for all dependencies also, so Caffe2 built without bitcode forces developers to switch off bitcode for the whole app.
Closes https://github.com/caffe2/caffe2/pull/457

Reviewed By: bwasti

Differential Revision: D4978644

Pulled By: Yangqing

fbshipit-source-id: 5165abb507fb91bc8c38f7348d6836bccf8fcc22",abjurato,https://api.github.com/repos/pytorch/pytorch/git/commits/1040b5f91cdd61d1d1b7fbf0f02f39e65820988b
93094294ba2b41df2758b676f59e206c37221a16,"function backward attempted to multiply tuple by variables (#1459)

One line fix--changed it to multiple the grad_variables by the
len(variables) when grad_variables is None.",NaimKabir,https://api.github.com/repos/pytorch/pytorch/git/commits/93094294ba2b41df2758b676f59e206c37221a16
4e18d8979130f7d837da1acc1eaebb6bc57d1f13,added twice differentiation for a bunch of ops (#1426),ankitkv,https://api.github.com/repos/pytorch/pytorch/git/commits/4e18d8979130f7d837da1acc1eaebb6bc57d1f13
e50c7daaf964bae83b9c37108c45ed3bd73017b6,Use Qr factorization to get orthogonal matrix in orthogonal init (#1453),GBLin5566,https://api.github.com/repos/pytorch/pytorch/git/commits/e50c7daaf964bae83b9c37108c45ed3bd73017b6
ff0ff33a1128b29ce909e71116e1d5ab1c79b752,Fix docs for InstanceNorm (#1477),abhiskk,https://api.github.com/repos/pytorch/pytorch/git/commits/ff0ff33a1128b29ce909e71116e1d5ab1c79b752
8a2433eacb34af09da98a14f4557bc3039f91c10,"Add model saving and loading to resnet50_trainer.py

Summary:
Script caffe2/caffe2/python/examples/resnet50_trainer.py can be used to train a ResNet-50 model with Imagenet data (or similar).

However, currently the script does not actually save the model, so it is kind of useless.

Task 1:  After each Epoch, save the model in a file ""<filename>_X.mdl' where X is the epoch number and <filename> is given as a command line parameter. By default, use ""resnet50_model"" as filename.

Task 2: Add a functionality to restore the model from a previous file:
 - add a command line parameter ""load_model"", which user can use to specify a filename.
 - if this parameter is set, load the model parameters from the previous file

Reviewed By: prigoyal

Differential Revision: D4984340

fbshipit-source-id: 333e92679ba52a7effe9917fdfc2d55d652b868f",HengCV,https://api.github.com/repos/pytorch/pytorch/git/commits/8a2433eacb34af09da98a14f4557bc3039f91c10
033ab9da1be749849ad3f9d2effa4ac8b1843301,"Adding video data layer for caffe2

Summary: Adding a simple video data layer which allows to read video data from frames, videos and output 5D tensor. It also allows multiple labels. The current implementation is based on ffmpeg

Differential Revision: D4801798

fbshipit-source-id: 46448e9c65fb055c2d71855447383a33ade0e444",dutran,https://api.github.com/repos/pytorch/pytorch/git/commits/033ab9da1be749849ad3f9d2effa4ac8b1843301
e3f41a4962d2d012b6ed1c72fb3124f42e3ffff7,Add high order gradient support for Sigmoid (#1496),caogang,https://api.github.com/repos/pytorch/pytorch/git/commits/e3f41a4962d2d012b6ed1c72fb3124f42e3ffff7
4ad2e155bc5b3a279039ed5c902228ce6248400d,"Make nn.Sequential more pythonic (#1510)

A minor fix which uses `enumerate` during iteration.",icyblade,https://api.github.com/repos/pytorch/pytorch/git/commits/4ad2e155bc5b3a279039ed5c902228ce6248400d
c5ae79fe4e7f9ab5a7a22b835d26c9dfdc53e810,Make clamp twice differentiable (#1514),t-vi,https://api.github.com/repos/pytorch/pytorch/git/commits/c5ae79fe4e7f9ab5a7a22b835d26c9dfdc53e810
105df5844dca21f964d180a918c808489862941f,Implement lgamma function.,ethanluoyc,https://api.github.com/repos/pytorch/pytorch/git/commits/105df5844dca21f964d180a918c808489862941f
65cf2f011793a31f686d739bb46a45466468b93a,"compile error when build on mac enviroment

Summary:
build on mac, when build error occurred

[ 71%] Building CXX object caffe2/CMakeFiles/Caffe2_CPU.dir/operators/transpose_op.cc.o
In file included from /Users/pg/DeepLearning/caffe2/caffe2/operators/tile_op.cc:1:
/Users/pg/DeepLearning/caffe2/caffe2/operators/tile_op.h:25:28: error: implicit instantiation of undefined
      template 'std::__1::array<int, 2>'
    std::array<int32_t, 2> temp_params = {{tiles_, axis_}};
                           ^
/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/__tuple:114:65: note: template is declared here
template <class _Tp, size_t _Size> struct _LIBCPP_TYPE_VIS_ONLY array;
                                                                ^
In file included from /Users/pg/DeepLearning/caffe2/caffe2/operators/tile_op.cc:1:
/Users/pg/DeepLearning/caffe2/caffe2/operators/tile_op.h:119:28: error: implicit instantiation of undefined
      template 'std::__1::array<int, 2>'
    std::array<int32_t, 2> temp_params = {{tiles_, axis_}};
                           ^
/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/__tuple:114:65: note: template is declared here
template <class _Tp, size_t _Size> struct _LIBCPP_TYPE_VIS_ONLY array;
Closes https://github.com/caffe2/caffe2/pull/519

Reviewed By: asaadaldien

Differential Revision: D5020422

Pulled By: bwasti

fbshipit-source-id: 2a5896d0a7aa643dfe3ff688db958434b6e87780",iamqk,https://api.github.com/repos/pytorch/pytorch/git/commits/65cf2f011793a31f686d739bb46a45466468b93a
3a7e068439e5c75b656faac83f65c89c10cc2bfc,Remove spurious memo argument in Module.parameters() (#1527),wjaskowski,https://api.github.com/repos/pytorch/pytorch/git/commits/3a7e068439e5c75b656faac83f65c89c10cc2bfc
d1a44676828ef65067414c938b15412f85d1a39e,"fix a bug when calling modules

a module that returns a non-standard data structure currently breaks
due to checks for backwards hooks. This refactors the code slightly so
this will only break in the event of backwards hooks.",mdering,https://api.github.com/repos/pytorch/pytorch/git/commits/d1a44676828ef65067414c938b15412f85d1a39e
93f1d0ca7c9474bf3e7bc6bb1ae012352afa56cc,"L1 Operator

Summary: Adds the L1 Distance operator to distance_op.

Reviewed By: bwasti

Differential Revision: D5007719

fbshipit-source-id: fd547c6645cf5f87305e9ebfd95ed918779c1d2a",benzyx,https://api.github.com/repos/pytorch/pytorch/git/commits/93f1d0ca7c9474bf3e7bc6bb1ae012352afa56cc
9db77873167654026fdb0baa86fb285e89a035cb,Updating __getitem__ and __len__ for containers (#1544),dasguptar,https://api.github.com/repos/pytorch/pytorch/git/commits/9db77873167654026fdb0baa86fb285e89a035cb
029290c5b11a215258f8e728e877c1560efd021e,SpatialDepthWiseConvolution,stooloveu,https://api.github.com/repos/pytorch/pytorch/git/commits/029290c5b11a215258f8e728e877c1560efd021e
28f4f6db2c0631db826e8cd74b24fe5f7677d734,"typo error for torch.addr (#1547)

fix the typo error in the example for torch.addr",xwgeng,https://api.github.com/repos/pytorch/pytorch/git/commits/28f4f6db2c0631db826e8cd74b24fe5f7677d734
ba885a1a51776823609159fda24276563dc1fdde,"expose bitwise operators from C/CUDA (#1556)

* fix issue #1549, expose bitwise and

* expose C bitwise or of Tensor

* expose C bitwise xor of Tensor

* use built-in method for inplace and, or, xor

* expose C bitwise lshift(ilshift) and rshift(irshift) of Tensor",stegben,https://api.github.com/repos/pytorch/pytorch/git/commits/ba885a1a51776823609159fda24276563dc1fdde
cb79c24d0b98ba53225235c4a30d517e642903a0,Added powerpc64le support (#1572),SeanNaren,https://api.github.com/repos/pytorch/pytorch/git/commits/cb79c24d0b98ba53225235c4a30d517e642903a0
b8b7f879c2104044fc4ed40e98c0620be4ecbbee,.gitignore updated with editor temporaries (#1574),chetkhatri,https://api.github.com/repos/pytorch/pytorch/git/commits/b8b7f879c2104044fc4ed40e98c0620be4ecbbee
156fe2866665c55557e41bb46603fef3e39bfb35,dataloader can now handle growing datasets (#1575),isacarnekvist,https://api.github.com/repos/pytorch/pytorch/git/commits/156fe2866665c55557e41bb46603fef3e39bfb35
b61378b4b6711a8a1567b3149301f09905286dae,"vectorized version of lstm_unit

Summary: vectorized lstm_unit using eigen

Reviewed By: ajtulloch

Differential Revision: D5051296

fbshipit-source-id: 1fa39ce474c731772c4169150622943a7eaec8e3",msmelyan,https://api.github.com/repos/pytorch/pytorch/git/commits/b61378b4b6711a8a1567b3149301f09905286dae
f072c74dfd1098cb9fb42e8ec9e498259c5f18c9,make it effective to transfer a tensor from other devices to device 0 (#1610),rosinwang,https://api.github.com/repos/pytorch/pytorch/git/commits/f072c74dfd1098cb9fb42e8ec9e498259c5f18c9
5a7f67bd4192e283f1a735c958357facd03784b2,"Add stack traces on fatal signals

Summary:
When a fatal signal is fired to a task that links against caffe2 this PR adds stacktraces from every thread that's currently running. Only linux is supported currently. The signals that are currently supported are SIGABRT, SIGINT, SIGILL, SIGFPE, SIGBUS and SIGSEGV (more signals can easily be added, but for now this seemed like the major signals that might be fired - see signal_handler.cc:138 for the table of signals).

I've added tests that verify that each of those signals indeed output the expected number of stacktraces.

We need to add linking against libdl since on linux apparently it's not implicitly always linked in (I'm coming from macOS where I believe it is).

Example output can be found [here](https://gist.github.com/danzimm/814faa1229d9c54f359d23ba038344a6) - note that the signal name changes depending on the signal that was sent (as well as the number in parenthesis that corresponds to the specified signal).
Closes https://github.com/caffe2/caffe2/pull/596

Reviewed By: akyrola

Differential Revision: D5087526

Pulled By: pietern

fbshipit-source-id: ba8d058c9ca1cf06b41667205193f8699f8d6964",danzimm,https://api.github.com/repos/pytorch/pytorch/git/commits/5a7f67bd4192e283f1a735c958357facd03784b2
45524ec33ce1bf75ece04d4f6eb939ba0c9469e9,Fix indices bug in MM.py (#1613) (#1617),tcosmo,https://api.github.com/repos/pytorch/pytorch/git/commits/45524ec33ce1bf75ece04d4f6eb939ba0c9469e9
356c19319fee2408906d5e9157e00987fca642b2,"Change repo from bwasti to caffe2.

Summary:
I'm assuming the repo should be caffe2/caffe2.git and not bwasti/caffe2.git. Changed it accordingly.
Closes https://github.com/caffe2/caffe2/pull/572

Differential Revision: D5105328

Pulled By: aaronmarkham

fbshipit-source-id: 4bd3babbd93c79831be79c6d40b81d873fcc3f4c",williford,https://api.github.com/repos/pytorch/pytorch/git/commits/356c19319fee2408906d5e9157e00987fca642b2
43be6456e2ce356b864a1f34e02ba84d0e3f5dfd,"UNUSED_VARIABLE VS compile fail fix

Summary:
fix test projects UNUSED_VARIABLE compile fail on visual studio
Closes https://github.com/caffe2/caffe2/pull/613

Differential Revision: D5121541

Pulled By: Yangqing

fbshipit-source-id: c353e8df4995e732e4d5d64bac15d849464efea2",zzz197,https://api.github.com/repos/pytorch/pytorch/git/commits/43be6456e2ce356b864a1f34e02ba84d0e3f5dfd
b5a215db0a688219ad9dddeb4bdc508318da615b,"Added python-pip and python-numpy into build_raspbian.sh

Summary:
Added python-pip and python-numpy into build_raspbian.sh script
because they are not installed in ubuntu/debian minimal image.
Closes https://github.com/caffe2/caffe2/pull/609

Differential Revision: D5121550

Pulled By: Yangqing

fbshipit-source-id: 14dd1450275fcc2aa9d2a06f0982f460528a1930",herry13,https://api.github.com/repos/pytorch/pytorch/git/commits/b5a215db0a688219ad9dddeb4bdc508318da615b
bf6f6308886a96006f677cfce152081ce1ceb407,"bug fix in CMakeLists.txt (CAFFE2_CPU_FLAGS and CAFFE2_WHITELIST)

Summary:
Fixed a bug in CMakeLists.txt: should not use option cmd for setting the initial value(empty string) of CAFFE2_CPU_FLAGS and CAFFE2_WHITELIST, because option can only be used for boolean(ON/OFF) variables. Use set cmd instead. The bug can cause compilation errors if CAFFE_CPU_FLAGS is set to ON, since an invalid 'ON' flag will be added to CXX_FLAGS. (2) Add build_* in .gitignore to allow multiple build directories in repo
Closes https://github.com/caffe2/caffe2/pull/611

Differential Revision: D5121545

Pulled By: Yangqing

fbshipit-source-id: 1f57042075356b6bf7138f65565b327be2a6d272",caiyang,https://api.github.com/repos/pytorch/pytorch/git/commits/bf6f6308886a96006f677cfce152081ce1ceb407
3fe8abb492359573c9b69df25311409d1d245dae,"fixed gflags 2.2.0 error and image_input_op.h

Summary:
- caffe2 compiles now with gflags 2.2.0 (compiled from source), see issue https://github.com/caffe2/caffe2/issues/491
- fixed an error in image_input_op.h (did not compile in vs2015)
Closes https://github.com/caffe2/caffe2/pull/559

Differential Revision: D5121555

Pulled By: Yangqing

fbshipit-source-id: 9d2bedadd13d1872bb930a95d67ed20263988d13",matzeox,https://api.github.com/repos/pytorch/pytorch/git/commits/3fe8abb492359573c9b69df25311409d1d245dae
a94fc625f50833f126cb70eae51aa90172cc7ab6,"make random generator more flexible in context.h

Summary:
using typedef to replace the type of Pseudo-random number engines, it would make it flexible to use
Closes https://github.com/caffe2/caffe2/pull/615

Differential Revision: D5121539

Pulled By: Yangqing

fbshipit-source-id: 988e57f8d119cb6f3bfe692fdb303aba2ecacbeb",Knight-X,https://api.github.com/repos/pytorch/pytorch/git/commits/a94fc625f50833f126cb70eae51aa90172cc7ab6
630af4d7d8812efa43a01d938f0fc53c47802b0b,add learning rate schedulers (#1370),Jiaming-Liu,https://api.github.com/repos/pytorch/pytorch/git/commits/630af4d7d8812efa43a01d938f0fc53c47802b0b
3ccbf2313259b8dfcd9d77922644475066ddda5a,"String-related fixes for Python 3

Summary: This diff is one step towards enabling python 3 build by making it be more diligent in its handling of strings.

Reviewed By: salexspb

Differential Revision: D4893083

fbshipit-source-id: 28b8adf3280e8d1f0a7dc9b0fee5ad53f2fada57",tomdz,https://api.github.com/repos/pytorch/pytorch/git/commits/3ccbf2313259b8dfcd9d77922644475066ddda5a
4eb448a051a1421de1dda9bd2ddfb34396eb7287,"Fix simple typo

Dimension a bit wrong",Shuailong,https://api.github.com/repos/pytorch/pytorch/git/commits/4eb448a051a1421de1dda9bd2ddfb34396eb7287
7c3add4408636d366c30763d5e3b187b194bc223,"better android ndk path

Summary:
use user defined android ndk path instead of hard code.
Closes https://github.com/caffe2/caffe2/pull/506

Differential Revision: D5162646

Pulled By: Yangqing

fbshipit-source-id: 5093888e15607b3bf6682e05eb91aa94c6206b01",csarron,https://api.github.com/repos/pytorch/pytorch/git/commits/7c3add4408636d366c30763d5e3b187b194bc223
447fe953e5c58044434f01945a7fdb49ea1bfefd,"Modify the sample code of volatile (#1694)

The original two inputs (torch.randn(5,5)) can not be used as input of resnet, which must be (batch, channels, width, height)",Magic-Bubble,https://api.github.com/repos/pytorch/pytorch/git/commits/447fe953e5c58044434f01945a7fdb49ea1bfefd
b3e179ea31f78b8aba4fa550493933428ece6473,"fixing lmdb.cc when compiled on Windows (mkdir -> _mkdir)

Summary:
Should fix #462 .
Closes https://github.com/caffe2/caffe2/pull/539

Reviewed By: asaadaldien, dzhulgakov

Differential Revision: D5162615

Pulled By: Yangqing

fbshipit-source-id: 985d3694e389bcf1fd96990254a53d806baba0cb",gfursin,https://api.github.com/repos/pytorch/pytorch/git/commits/b3e179ea31f78b8aba4fa550493933428ece6473
00843c57c936720b3d17f4c0afaab08dcb52a7cc,substitute cudnnFind* functions with cudnnFind*Ex,aromnvidia,https://api.github.com/repos/pytorch/pytorch/git/commits/00843c57c936720b3d17f4c0afaab08dcb52a7cc
ec2de16776096a7735f0ad81c9bef54d449c98cf,Improve README copyediting,taion,https://api.github.com/repos/pytorch/pytorch/git/commits/ec2de16776096a7735f0ad81c9bef54d449c98cf
3bd619589146e8259e00c65af898dedca250bcab,"removed Sum from simple_operator_layers.py; passed unit tests

Summary: removed softmax, sigmoid, tanh, relu from simple_operator_layers.py; passed all unit tests

Reviewed By: kittipatv

Differential Revision: D5150271

fbshipit-source-id: abe611bf6c5de5caba189181e9e41d705d8c5c54",wutao27,https://api.github.com/repos/pytorch/pytorch/git/commits/3bd619589146e8259e00c65af898dedca250bcab
e9bf702c5e4f4570b0790edb93a199f80a32b56c,"LSTM bias_hh, fix docs

Rename W_hi ... to b_hi ...",makarandtapaswi,https://api.github.com/repos/pytorch/pytorch/git/commits/e9bf702c5e4f4570b0790edb93a199f80a32b56c
a76098ac1532d5e9ee24b4776258ae731627f8e3,"fix optimizer when given single parameters (instead of an iterable)

When I use the named_parametes to modify the lr and weight decay, I will face a bug. Because the value of the named_parameters return is  torch.nn.paramter.Parameter, not a generator of the Parameter.",mileyan,https://api.github.com/repos/pytorch/pytorch/git/commits/a76098ac1532d5e9ee24b4776258ae731627f8e3
d7db75c10f54a0f7e31ffbc090d9ced4fd11b8a2,"added CosineSimilarity to nn.distance and updated docs (#1672)

* added CosineSimilarity to nn.distance and updated docs",aron-bordin,https://api.github.com/repos/pytorch/pytorch/git/commits/d7db75c10f54a0f7e31ffbc090d9ced4fd11b8a2
4316fb48768403549246566917fab2809a45b017,"Implement APMeter op

Summary: Implements an APMeter operator (APMeterOp) to calculate AP for multilclass classification given prediction socres and labels. The Op takes a score tensor [nsamples x nclasses] and a label tensor [nsamples x nclasses], and outputs a float tensor of size nclasses as the AP for each class.

Reviewed By: akyrola

Differential Revision: D5082565

fbshipit-source-id: ae7304bc8fc999c361245b9aec38eb9a5f5eef4b",rxianfb,https://api.github.com/repos/pytorch/pytorch/git/commits/4316fb48768403549246566917fab2809a45b017
2f385d490b8c7967fb5b0fd1214487d3323b3604,"apply clang-tidy modernize-use-override

Summary: Use clang-tidy to mechanically add missing `override` and remove redundant `virtual`.

Reviewed By: igorsugak

Differential Revision: D5211868

fbshipit-source-id: 15cec17d39690ffa8072ffeccdf9fedaae1f6839",vgao1996,https://api.github.com/repos/pytorch/pytorch/git/commits/2f385d490b8c7967fb5b0fd1214487d3323b3604
dcf07a2d7f36e554e71e2031f4564d46d41c0d65,Fix typo in ParameterList documentation,rdipietro,https://api.github.com/repos/pytorch/pytorch/git/commits/dcf07a2d7f36e554e71e2031f4564d46d41c0d65
7c024e93c688efe28a34b2da8d4a1f35a030a828,Implement Cumprod function for autograd (#1439),martinarjovsky,https://api.github.com/repos/pytorch/pytorch/git/commits/7c024e93c688efe28a34b2da8d4a1f35a030a828
c1f974aa9fcb44888aa0e0c83dec5d64538ef049,"Deprecate CNNModelHelper in python/crf.py

Reviewed By: harouwu

Differential Revision: D5241631

fbshipit-source-id: 3dc448355bc2a766ae9eda1dc579e501743b35cf",bddppq,https://api.github.com/repos/pytorch/pytorch/git/commits/c1f974aa9fcb44888aa0e0c83dec5d64538ef049
9d916e561cf9e07d40429997f1d1350f8a85c280,"batch norm docfix (#1804)

fixes the formula for batch normalization (moves the epsilon inside
the square root)",albanie,https://api.github.com/repos/pytorch/pytorch/git/commits/9d916e561cf9e07d40429997f1d1350f8a85c280
2ec294a8bbcab1585eb2f8d197f08c1f78f5d2a0,"Fix a few typos and grammars in comment

Summary:
Fix a few typos and grammars in comment

by using language-check, python library
spell_checker source code is here : https://github.com/17-1-SKKU-OSS/011A/blob/master/spell_checker/spell_checker.py
here is the text file which indicates what things should be fixed :  https://github.com/17-1-SKKU-OSS/011A/tree/master/spell_checker/fix/caffe2
Closes https://github.com/caffe2/caffe2/pull/719

Differential Revision: D5165118

Pulled By: aaronmarkham

fbshipit-source-id: 7fb8ef7a99d03cd5fd2f9ebdb01b9865e90fc37b",haracejacob,https://api.github.com/repos/pytorch/pytorch/git/commits/2ec294a8bbcab1585eb2f8d197f08c1f78f5d2a0
3cecdf84f15420318474feebec988f118b6d6c85,Storage from_file method (#1821),vlasenkov,https://api.github.com/repos/pytorch/pytorch/git/commits/3cecdf84f15420318474feebec988f118b6d6c85
172a356668da88b911f2ce44f75a8897326ec51a,"forgotten import in variables.py

Fixing error on line 661: 
warnings.warn(""masked_copy_ is deprecated and renamed to masked_scatter_, and will be removed in v0.3"")
NameError: name 'warnings' is not defined",aosokin,https://api.github.com/repos/pytorch/pytorch/git/commits/172a356668da88b911f2ce44f75a8897326ec51a
6150d9bef2fa384fbc221b8685069f7858727a14,"Building dropout as layer

Summary: Dropout layer and unittest for DPer2

Reviewed By: chocjy

Differential Revision: D5254866

fbshipit-source-id: 5eaea81808ddf8e0c7a7d76209ea44cda2ee28aa",jackielxu,https://api.github.com/repos/pytorch/pytorch/git/commits/6150d9bef2fa384fbc221b8685069f7858727a14
dd1525d3465437beb4da88649fd551a8fe6b3c3a,"fix #790 so model.init_params = False takes effect

Summary:
Given the parameter init_params=False, Weight Blob(*_w) and Bias Blob (*_b) should be suppressed in model.param_init_net. Without this fix, the init_params=False doesn't take effect in brew.conv as it does in brew.fc or other ops. This issue is the root cause of #790 [https://github.com/caffe2/caffe2/pull/790].
Closes https://github.com/caffe2/caffe2/pull/824

Reviewed By: harouwu

Differential Revision: D5276676

Pulled By: akyrola

fbshipit-source-id: 8f7088a8e1976658f67e027223e555375b3a2392",davinwang,https://api.github.com/repos/pytorch/pytorch/git/commits/dd1525d3465437beb4da88649fd551a8fe6b3c3a
03f41c81200ab55f749996d5b85f4b56991888c3,"fix capitalization of Python, make it consistent",esc,https://api.github.com/repos/pytorch/pytorch/git/commits/03f41c81200ab55f749996d5b85f4b56991888c3
4b93f32234e649b40010c3d2647ce7fc12617cc2,rename TensorLib -> ATen,zdevito,https://api.github.com/repos/pytorch/pytorch/git/commits/4b93f32234e649b40010c3d2647ce7fc12617cc2
cf4ac83a91af409c7818999c8b23be45e7ee6851,"Make List.__getitem__() works with output of List.field_names()

Summary:
As described in T19378176 by kittipatv, in this diff, we fix the issue of __getitem__() of schema.List.

For example, given Map(int32, float) (Map is a special List), field_names() will return ""lengths"", ""values:keys"", & ""values:values"". ""values:keys"" and ""values:values"" are not accessible via __getitem__(). __getitem__() bypasses the values prefix and directly access the fields in the map. Other APIs (e.g., _SchemaNode & dataset_ops) expect ""values:keys"" and ""values:values"" as it simplifies traversal logic. Therefore, we should keep field_names() as is and fix __getitem__().

Reviewed By: kittipatv

Differential Revision: D5251657

fbshipit-source-id: 1acfb8d6e53e286eb866cf5ddab01d2dce97e1d2",sunnieshang,https://api.github.com/repos/pytorch/pytorch/git/commits/cf4ac83a91af409c7818999c8b23be45e7ee6851
3f6cda8696bd80d9b91d4bcb32ced2b5f9e4082e,fix bug of threshold activation,LiZhaoxing,https://api.github.com/repos/pytorch/pytorch/git/commits/3f6cda8696bd80d9b91d4bcb32ced2b5f9e4082e
ee10e7457f492f1d6c61fdde76f1c2b1ef47380d,Corrected erroneous docstring for MultiLabelSoftMarginLoss,varunagrawal,https://api.github.com/repos/pytorch/pytorch/git/commits/ee10e7457f492f1d6c61fdde76f1c2b1ef47380d
ca2b608f837f80caf97d3d4021c665764eb127c1,"Fixed typo

Summary:
peaces -> pieces, peace -> piece
Closes https://github.com/caffe2/caffe2/pull/819

Differential Revision: D5312417

Pulled By: aaronmarkham

fbshipit-source-id: 59d2c3f475197a5f29dc7cf3ecaf675a242d3cdf",hskang9,https://api.github.com/repos/pytorch/pytorch/git/commits/ca2b608f837f80caf97d3d4021c665764eb127c1
8f1e641d5f1d4d485d6b86e6a8182caca64e7c07,"Deprecate CNNModelHelper in python/data_workers_test.py

Summary: Deprecate CNNModelHelper in python/data_workers_test.py

Reviewed By: harouwu

Differential Revision: D5312089

fbshipit-source-id: 37b72ac2031acf14a7e6a6ea0a298b71b00b10dd",yzheng624,https://api.github.com/repos/pytorch/pytorch/git/commits/8f1e641d5f1d4d485d6b86e6a8182caca64e7c07
fd86c51c39e2c48d787ffb9b717db8d732ad587a,"Add ResizeNearest

Summary: Added the CUDA implementation of ResizeNearest (forward pass only)

Reviewed By: wickedfoo

Differential Revision: D5290087

fbshipit-source-id: 4291e65b2b4b6a1a197275d5ed8710f40000b59e",mrharicot,https://api.github.com/repos/pytorch/pytorch/git/commits/fd86c51c39e2c48d787ffb9b717db8d732ad587a
b5e1df046e6c1a025c1339d61bccd2cd8aa127c4,fixed typo in formula of  GRU in doc (#1921),moskomule,https://api.github.com/repos/pytorch/pytorch/git/commits/b5e1df046e6c1a025c1339d61bccd2cd8aa127c4
ea659b8f2ef422d51a4f70729d235928346c8356,"broadcast to global parameters when using warmup

Reviewed By: asaadaldien, jay-mahadeokar

Differential Revision: D5340692

fbshipit-source-id: 80879847ff71c8d620de502ef95a9ffb4bdf595d",yqwangustc,https://api.github.com/repos/pytorch/pytorch/git/commits/ea659b8f2ef422d51a4f70729d235928346c8356
635bb5ec9d99c53aeded558c54dd4c093fa273bb,corrects typo,lanpa,https://api.github.com/repos/pytorch/pytorch/git/commits/635bb5ec9d99c53aeded558c54dd4c093fa273bb
ae6523649062616b1936c15838b7b6c99a6d7e32,Fix typo,qbx2,https://api.github.com/repos/pytorch/pytorch/git/commits/ae6523649062616b1936c15838b7b6c99a6d7e32
e8689dda8f91701fa4553edb82f8fed3e8e19c96,"Python 3 compatible integer division

Summary:
As the title says.
Closes https://github.com/caffe2/caffe2/pull/879

Differential Revision: D5372787

Pulled By: akyrola

fbshipit-source-id: 0ff469c0d227f1b2252c1a0c4f6f8bebaac5580f",willyd,https://api.github.com/repos/pytorch/pytorch/git/commits/e8689dda8f91701fa4553edb82f8fed3e8e19c96
90d0762d143d8ffac1dd58a5ef143041f20e1705,Use torch.arange instead of torch.range in test_torch.py (#1996),lynic,https://api.github.com/repos/pytorch/pytorch/git/commits/90d0762d143d8ffac1dd58a5ef143041f20e1705
0a34f05d5bb23440a087ff1cd9ec365521199a55,"Always include THNN in the build, don't check for CUDA twice

As a result, the project builds on MacOS with gcc-6 (without CUDA).",jgehring,https://api.github.com/repos/pytorch/pytorch/git/commits/0a34f05d5bb23440a087ff1cd9ec365521199a55
86b6a6e2f8207e31008135604197e11dce82210f,"Added PiecewiseLinearTransform CUDA Op

Summary: Added a CUDA implementation of the PiecewiseLinearTransformOp.

Differential Revision: D5378537

fbshipit-source-id: 38857f59f5cc52e16e1ecc97983a0b0b82a46c74",gsethi523,https://api.github.com/repos/pytorch/pytorch/git/commits/86b6a6e2f8207e31008135604197e11dce82210f
49f679d0e94ceb81ed171a37f0d4b400a65b7feb,Acknowledge the existence of cpu HalfTensor (#2018),narendasan,https://api.github.com/repos/pytorch/pytorch/git/commits/49f679d0e94ceb81ed171a37f0d4b400a65b7feb
0025e1c776938cc824974a374ba3156c98941510,"Fix typos in the docstrings of Conv3d, AvgPool3d and MaxPool3d (#2030)

* Fix a typo of the docstring of Conv3d

* Fix typos in docstrings of 3D operations.",Kongsea,https://api.github.com/repos/pytorch/pytorch/git/commits/0025e1c776938cc824974a374ba3156c98941510
73128f7b088ef94b38d6973044ee48fcfb7ed52a,"fix minor typos (#2051)

* Update extending.rst

fix typo

* Update cuda.rst

fix typo",Naruto-Sasuke,https://api.github.com/repos/pytorch/pytorch/git/commits/73128f7b088ef94b38d6973044ee48fcfb7ed52a
c8afdb6f4b6d23786540819913e712bc4406d608,"Caffe2: Add Open method to DBReader which takes DB pointer

Summary: Currently the DBReader always creates the DB instance itself when Open is called.  Add an Open method that takes in a DB pointer and takes ownership of it, so the DB can be initialized outside the DBReader.

Reviewed By: panshen1

Differential Revision: D5392458

fbshipit-source-id: d8660ab41d349f32030e4934b47bd17256a440df",kevinwilfong,https://api.github.com/repos/pytorch/pytorch/git/commits/c8afdb6f4b6d23786540819913e712bc4406d608
f483679425fc766660003569fcc9789ee6a6fb0d,Implementation of Alias Multinomial for faster Multinomial sampling (#1046),amartya18x,https://api.github.com/repos/pytorch/pytorch/git/commits/f483679425fc766660003569fcc9789ee6a6fb0d
ce96b84ccbdfbbee7f744942b1bb9fdc5924e442,"Check for shared_mem size in multinomial single-sample implementation

Handle limited shared memory on function torch.multinomial

Update THCTensorRandom.cu",BestSonny,https://api.github.com/repos/pytorch/pytorch/git/commits/ce96b84ccbdfbbee7f744942b1bb9fdc5924e442
16dd9972390cdf5c24fd1bc0936b669f24ddf8b7,Spelling tweaks for documentation (#2114),brettkoonce,https://api.github.com/repos/pytorch/pytorch/git/commits/16dd9972390cdf5c24fd1bc0936b669f24ddf8b7
cddb73899cba99c97b694e70d72f8ab5995d9753,"fix strip prefix bug in SaveOp

Summary:
if strip_prefix_ not found in blob name, strip_prefix_.size() characters of blob name will be stripped.
Closes https://github.com/caffe2/caffe2/pull/924

Differential Revision: D5440941

Pulled By: akyrola

fbshipit-source-id: 1db772fac4c74f2ce05105eec4bc7742a9067ebc",CSLJXing,https://api.github.com/repos/pytorch/pytorch/git/commits/cddb73899cba99c97b694e70d72f8ab5995d9753
8b42308f711c2fe7b2915c99dbe6ceac6b9d6823,"Bug in line 381 (sparse) (#2130)

The function iterates over columns and sets ""sparsity"" fraction of entires in each column to 0. The number of zeros in a column (num_zeros) is then ceil(rows*sparsity)",suragnair,https://api.github.com/repos/pytorch/pytorch/git/commits/8b42308f711c2fe7b2915c99dbe6ceac6b9d6823
95ccbf8b0b29bc0d285367c7de693fb24628bd26,better error message in load_state_dict when there are inconsistent tensor sizes (#2151),greaber,https://api.github.com/repos/pytorch/pytorch/git/commits/95ccbf8b0b29bc0d285367c7de693fb24628bd26
bf1fc250d1e9e4a1c958f2997b336ef96155f26b,"get conda root dir automatically, trick from Dockerfile",gngdb,https://api.github.com/repos/pytorch/pytorch/git/commits/bf1fc250d1e9e4a1c958f2997b336ef96155f26b
4d45ce7d114d734f5a2f8ab4a233217b08dcf660,Added UpSampling module and associated tests.,mikepound,https://api.github.com/repos/pytorch/pytorch/git/commits/4d45ce7d114d734f5a2f8ab4a233217b08dcf660
4a4d8841e676769310fdaac8d33a23fc1081af10,Delete unused import,Stonesjtu,https://api.github.com/repos/pytorch/pytorch/git/commits/4a4d8841e676769310fdaac8d33a23fc1081af10
eae6400d597373d7c1dd5a242968d3bdd5764e88,"Updated summary for the FC layer in caffe2

Summary: Fixed incorrect description of the input tensor X, and auto-formatted the file.

Reviewed By: jamesr66a

Differential Revision: D5467876

fbshipit-source-id: 1936cf5eb65824c8aeaf2c7924d5b850ab36b593",haychris,https://api.github.com/repos/pytorch/pytorch/git/commits/eae6400d597373d7c1dd5a242968d3bdd5764e88
0458985c1b7a202c7d041ac8260a2e0978e7a62c,"Fix build with external nnpack installation

Summary:
libpthreadpool is needed during the linking stage and is missing when user chooses to use an external nnpack installation (from system libraries).

Fixes GitHub issue #459.

Detailed discussion on [this comment](https://github.com/caffe2/caffe2/issues/459#issuecomment-308831547).
Closes https://github.com/caffe2/caffe2/pull/808

Differential Revision: D5430318

Pulled By: Yangqing

fbshipit-source-id: 5e10332fb01e54d8360bb929c1a82b0eef580bbb",dbermond,https://api.github.com/repos/pytorch/pytorch/git/commits/0458985c1b7a202c7d041ac8260a2e0978e7a62c
f1fd4ac7edeb2477404883b21c3c4fe8420c0757,Added aarch64 support (#2226),yawara,https://api.github.com/repos/pytorch/pytorch/git/commits/f1fd4ac7edeb2477404883b21c3c4fe8420c0757
930b6b83c53e790fd76b705a9cdaf91921c1de70,"Update class comment of Context

Summary:
Fixes https://github.com/caffe2/caffe2/issues/988
Closes https://github.com/caffe2/caffe2/pull/989

Differential Revision: D5518437

Pulled By: Yangqing

fbshipit-source-id: 885e6fed2a32eed57c3b3aeb16fe65925406501c",wangkuiyi,https://api.github.com/repos/pytorch/pytorch/git/commits/930b6b83c53e790fd76b705a9cdaf91921c1de70
e3c45206eca042396b42615279370c47f665a78a,"Add a method to run a train net multiple times in layer_test_util.py

Summary: This method runs a train net multiple times therefore enables testing layers with iteration-dependent behavior.

Differential Revision: D5493750

fbshipit-source-id: a7fb967a66f799aaf82acfadc4ecf66e0744da20",oscarlight8,https://api.github.com/repos/pytorch/pytorch/git/commits/e3c45206eca042396b42615279370c47f665a78a
da66f1004242af45f484f1e6d543b8ff4473ef05,"Improve StringJoin operator

Summary:
StringJoin operator converts input array/matrix elements to string then join them to make vector of strings

Changes:
* Support string tensor input
* Support join on 1-axis
* Add unit tests

Differential Revision: D5513705

fbshipit-source-id: 25f96ed3586065c15f845a968c9f8864ca8f5bdf",hoangmit,https://api.github.com/repos/pytorch/pytorch/git/commits/da66f1004242af45f484f1e6d543b8ff4473ef05
83ba2b10919174619e2bf08a9a972da56705fc53,"Typo correction in CMakelist.txt

Summary: Closes https://github.com/caffe2/caffe2/pull/1010

Differential Revision: D5554930

Pulled By: akyrola

fbshipit-source-id: 7bd93608aeace1baacff00b4c302fc4a5e20a607",uestclx,https://api.github.com/repos/pytorch/pytorch/git/commits/83ba2b10919174619e2bf08a9a972da56705fc53
5d721c1c14107b68b385352bcfd45898010e864b,"Some adjustments for Windows build

Summary:
1. switch the protoc building system from msbuild to cmake
2. set default CMAKE_GENERATE to VS2015
3. set default CMAKE_BUILD_TYPE to Release
4. improve error handling
5. add the generated protobuf include path
6. exclude many optional dependencies from build_windows.bat
Closes https://github.com/caffe2/caffe2/pull/1014

Differential Revision: D5559402

Pulled By: Yangqing

fbshipit-source-id: 019e3a6c3c909154027fa932ce1d6549476b23bb",xw0,https://api.github.com/repos/pytorch/pytorch/git/commits/5d721c1c14107b68b385352bcfd45898010e864b
4d8a8c2e1e68bd8b4b56f679b68ed3620d6e34c9,"Implement dot attention

Summary:
Implement dot attention as described in https://arxiv.org/abs/1508.04025
This saves the computation of weighted encoder outputs in `rnn_cell.py`
When the encoder and decoder dimensions are different, we apply an FC, which corresponds to the general case below Figure 2.
Refactored unit tests.

Reviewed By: jhcross

Differential Revision: D5486976

fbshipit-source-id: f9e9aea675b3b072fbe631bc004199b90a9d95cb",jmp84,https://api.github.com/repos/pytorch/pytorch/git/commits/4d8a8c2e1e68bd8b4b56f679b68ed3620d6e34c9
02e5367bdd82df0aaff0224eea91c7653b92d0aa,"Support a build script for Tizen target

Summary:
There does not exist appropriate build script for Tizen software platform.
This commit is to fix #847.

Signed-off-by: Geunsik Lim <geunsik.lim@samsung.com>
Closes https://github.com/caffe2/caffe2/pull/877

Differential Revision: D5571335

Pulled By: Yangqing

fbshipit-source-id: 12759a3c0cb274ef93d7127b8185341e087f2bfa",leemgs,https://api.github.com/repos/pytorch/pytorch/git/commits/02e5367bdd82df0aaff0224eea91c7653b92d0aa
e51fec3be04b51deda6ffcc2362ed270ebbd3311,Update sparse.py (#2336),adampolyak,https://api.github.com/repos/pytorch/pytorch/git/commits/e51fec3be04b51deda6ffcc2362ed270ebbd3311
b96c4e714b697d1028229b3aeb845497a40c4a08,"Fix build failure on MacOS X with clang-800.0.42.1

Summary:
Signed-off-by: Jammy Zhou <jammy.zhou@gmail.com>
Closes https://github.com/caffe2/caffe2/pull/1047

Differential Revision: D5583196

Pulled By: Yangqing

fbshipit-source-id: 7fe782b6caa14074573fbdacd68f50e16fb85e3f",JammyZhou,https://api.github.com/repos/pytorch/pytorch/git/commits/b96c4e714b697d1028229b3aeb845497a40c4a08
9199c954f147105d5e5fc559c67bbdd785e44dad,Fix typo in DistributedDataParallel (#2320),ArEsKay3,https://api.github.com/repos/pytorch/pytorch/git/commits/9199c954f147105d5e5fc559c67bbdd785e44dad
1c0d20d58c788d9b2caa75fee653d40c2f568637,"add in make uninstall for cmake

Summary:
After sudo make install, it is quite cumbersome to remove the installed files manually.This change allows the user to simply type sudo make uninstall to remove all installed files.
Closes https://github.com/caffe2/caffe2/pull/748

Differential Revision: D5590971

Pulled By: Yangqing

fbshipit-source-id: b354640056c88b9975dd0cf195a6a4d8cad8d0ab",jasjuang,https://api.github.com/repos/pytorch/pytorch/git/commits/1c0d20d58c788d9b2caa75fee653d40c2f568637
e908cf28f45edce9de6fd736a2e16c67e3070674,"Docker move

Summary:
Bringing over selected dockerfiles from documentation branch and updated the GPU Dockerfiles to use some of lukeyeager provided docker configurations. Latest docker with CUDA 8.0 and cuDNN 6 can be pulled via `docker pull caffe2ai/caffe2` or built with `ubuntu-16.04-cuda8-cudnn6-all-options/Dockerfile`.
**You must use nvidia-docker instead of docker to run the GPU-enabled dockers.** Tutorial files can be overlaid by building `ubuntu-16.04-gpu-tutorial/Dockerfile`. Supersedes #911. Closes #876. Closes #923.
Closes https://github.com/caffe2/caffe2/pull/949

Reviewed By: Yangqing

Differential Revision: D5510872

Pulled By: aaronmarkham

fbshipit-source-id: 390f5eea1d9ec1a3edda828470b12386ab8a1775",aaronmarkham,https://api.github.com/repos/pytorch/pytorch/git/commits/e908cf28f45edce9de6fd736a2e16c67e3070674
ad847474335e9eecc3ba5e8e7ef0cc34e3e9b33e,"Optimized Tiling Code

Summary: Turned a number of uniform shader variables into constants

Differential Revision: D5596760

fbshipit-source-id: 68004c081c6b9ba2e55f7f74e48a673489c927b1",fricc33,https://api.github.com/repos/pytorch/pytorch/git/commits/ad847474335e9eecc3ba5e8e7ef0cc34e3e9b33e
641e582f3100fb726d7b795eedcb880b0feb0f2b,Fix typo (#2378),rostyboost,https://api.github.com/repos/pytorch/pytorch/git/commits/641e582f3100fb726d7b795eedcb880b0feb0f2b
3a8feb7fb7415c95ef5c69783e8a9205ce0f7234,Address integer division to make it compatible with py2,ailzhang,https://api.github.com/repos/pytorch/pytorch/git/commits/3a8feb7fb7415c95ef5c69783e8a9205ce0f7234
b3029df1d0b9558564c3c728b4004629d229b83d,"Added window mode for caffe2 sequence operator

Summary: This can be used for local attention to mask elements outside of a window

Reviewed By: jamesr66a

Differential Revision: D5643677

fbshipit-source-id: 92b33866258ccc7307d5bcf08234610aa3fb152d",jingfeidu,https://api.github.com/repos/pytorch/pytorch/git/commits/b3029df1d0b9558564c3c728b4004629d229b83d
4ca573575320ba1c7f88b288272b9aae6fdb2c6d,"Allow inplace for spatial_bn_op

Summary: att

Reviewed By: Yangqing

Differential Revision: D5644717

fbshipit-source-id: 1a020fe4ca7028056ce7bebddb7bfd1437998530",jerryzh168,https://api.github.com/repos/pytorch/pytorch/git/commits/4ca573575320ba1c7f88b288272b9aae6fdb2c6d
5c43fcda8d93f799c2082f2f67bb7c3a6299ed99,Support params that donâ€™t require grad in DistributedDataParallel (#2464),LuoweiZhou,https://api.github.com/repos/pytorch/pytorch/git/commits/5c43fcda8d93f799c2082f2f67bb7c3a6299ed99
a32e98b700abb620ba773ec668112ea080bdd788,Add documentation for std/var unbiased argument (#2509),allenye0119,https://api.github.com/repos/pytorch/pytorch/git/commits/a32e98b700abb620ba773ec668112ea080bdd788
502b43641fec6038df5349f6c4071d6b4d309717,"More flexible tiling for Conv and ConvTranspose

Summary: With these changes, Conv, ConvTranspose, PRelu, and Relu work with tiling now. The default is still batching.

Differential Revision: D5623321

fbshipit-source-id: 07aa378d24165ec19e751cd79c70dea995003be9",hlu1,https://api.github.com/repos/pytorch/pytorch/git/commits/502b43641fec6038df5349f6c4071d6b4d309717
67a55b81e335d1b9b7a862bbca2913589104f6af,"Forward blobs into workspace

Summary:
Better isolation for workspaces to allow forwarding selected blobs
from parent to child workspace, possibly under new names. Used for proper
isolation of subnets (loops, then/else branhes, etc) from outer workspace.

Reviewed By: azzolini

Differential Revision: D5681667

fbshipit-source-id: e61a2c7c98ee2abf1f0761905f4bfae47c201c32",ilia-cher,https://api.github.com/repos/pytorch/pytorch/git/commits/67a55b81e335d1b9b7a862bbca2913589104f6af
008a62b18ab28368684ce79a8a1a0a0efed40ae4,DOC fixed Tensor.expand docstring (#2495),kmike,https://api.github.com/repos/pytorch/pytorch/git/commits/008a62b18ab28368684ce79a8a1a0a0efed40ae4
27bd3df71bfced095682784560fb50916675e44a,"Patching EmeddingBag to accept 2D input (#2429)

* Patching EmeddingBag to accept 2D input

* fix for CUDA inputs

* fix lint",ajfisch,https://api.github.com/repos/pytorch/pytorch/git/commits/27bd3df71bfced095682784560fb50916675e44a
2de1bc894b494055e5c6f30d415cd680d993a6df,"move ShapeOp out from utility_ops

Summary: move ShapeOp out from utility_ops

Reviewed By: ajtulloch

Differential Revision: D5686081

fbshipit-source-id: ac1ae50bfa2e36eddd1834839169ba3cdf0722dc",wat3rBro,https://api.github.com/repos/pytorch/pytorch/git/commits/2de1bc894b494055e5c6f30d415cd680d993a6df
14d8c034245708c0c9fb792a955223241ddaee78,adding backward capability for potrf (Cholesky) (#2386),jmxpearson,https://api.github.com/repos/pytorch/pytorch/git/commits/14d8c034245708c0c9fb792a955223241ddaee78
61e4723132ba30d3841c648b5d1ba0c6d6632bbe,Fix typos (#2472),taehoonlee,https://api.github.com/repos/pytorch/pytorch/git/commits/61e4723132ba30d3841c648b5d1ba0c6d6632bbe
150dc7a8e3fc45176fc0b3a3ef1e6ecc8252bba5,Improve Windows Compatibility(for libshm) (#2455),peterjc123,https://api.github.com/repos/pytorch/pytorch/git/commits/150dc7a8e3fc45176fc0b3a3ef1e6ecc8252bba5
b5949d8e9d9e43737979bcc089de2a2d2f783e1d,Adding implicit padding for 3d average pooling,houseroad,https://api.github.com/repos/pytorch/pytorch/git/commits/b5949d8e9d9e43737979bcc089de2a2d2f783e1d
94b59902010c3cf37f46da5d83fa7e19bc2fd03a,Add torch.cuda.get_device_name function (#2540),jcjohnson,https://api.github.com/repos/pytorch/pytorch/git/commits/94b59902010c3cf37f46da5d83fa7e19bc2fd03a
cdae579c2295eae5cc9fad73390a8d790bdb0d7d,"Fix typos in ""Extending PyTorch"" (#2558)",GabrielBianconi,https://api.github.com/repos/pytorch/pytorch/git/commits/cdae579c2295eae5cc9fad73390a8d790bdb0d7d
c1b09cd5ab6fa19e947e5818399c9b89bad9c28c,Fix typo in docstring example (#2562),iamaziz,https://api.github.com/repos/pytorch/pytorch/git/commits/c1b09cd5ab6fa19e947e5818399c9b89bad9c28c
19f6941e7cf0c8e88f8e51f1ed4a3e97a4ba736d,"fix arxiv link to batch-norm paper

Summary: arxiv link to batch-norm paper was broken because dot(.) was included at the end

Reviewed By: zem7

Differential Revision: D5734405

fbshipit-source-id: e037c14091e7f9e415c2f7a3008cbf2bf066e699",jspark1105,https://api.github.com/repos/pytorch/pytorch/git/commits/19f6941e7cf0c8e88f8e51f1ed4a3e97a4ba736d
e69063405e1c3da76a838f3167bc0e334ecdd2a0,Allow param groups to be added to Optimizer dynamically (#2374),mjdietzx,https://api.github.com/repos/pytorch/pytorch/git/commits/e69063405e1c3da76a838f3167bc0e334ecdd2a0
0f3a5d31802b2b5568ba0b25aacd6096b037a305,"Tuning number of parameter servers based on performance estimation job

Summary:
1) Adds monitoring of CPU utilization in trainers and PS's, and report the utilization to global statistics
2) Adds the plan execution time to global stats
3) Uses CPU utilization and network utilization observed from performance estimation job to calculate the optimal number of parameter servers needed for the actual job. The optimal number of parameter server is the minimum number of servers needed while parameter servers are not the bottleneck in execution.

//Note: The calculation assumes that parameter shards are assigned to PS's in a uniform way and accesses to the shards follow a uniform access pattern. In reality, shards' access pattern may be skewed. As a next step, we should monitor shard access pattern in performance estimation job and distribute the shards in the optimal way.//

Reviewed By: sf-wind

Differential Revision: D5674398

fbshipit-source-id: 67a07cb9ed4e4d61ff5e81a0ecfe519b8feb2352",heslami,https://api.github.com/repos/pytorch/pytorch/git/commits/0f3a5d31802b2b5568ba0b25aacd6096b037a305
bf013f4c992f274594715b9fa1ce72299b650c17,fix Python 2 gloo install (#2597),hongyi-zhang,https://api.github.com/repos/pytorch/pytorch/git/commits/bf013f4c992f274594715b9fa1ce72299b650c17
7fa7a101af3230f8854f5df780c94c3d0ba2b112,Fix emmbedding doc formatting (#2605),r9y9,https://api.github.com/repos/pytorch/pytorch/git/commits/7fa7a101af3230f8854f5df780c94c3d0ba2b112
1b013c0b520ad7925e918f5ebe1ca8560672cb02,fixed issue #2613 in torch/legacy/nn (#2624),iacolippo,https://api.github.com/repos/pytorch/pytorch/git/commits/1b013c0b520ad7925e918f5ebe1ca8560672cb02
42448cf07f921aa1ca2f45d1c056ac025aeb9c1d,"Fix to make the sample code executable as-is in ""Extending PyTorch"" (#2621)",L0SG,https://api.github.com/repos/pytorch/pytorch/git/commits/42448cf07f921aa1ca2f45d1c056ac025aeb9c1d
1c414426df0b5957b91abe396a91e4f51c77dc55,"Caffe2: Cuda implementation for BatchOneHot operator

Summary: Cuda implementation for BatchOneHot operator.

Reviewed By: lvdmaaten

Differential Revision: D5639080

fbshipit-source-id: 8ee280c4bab64c1fdfb7429ee2c9ac8c02933931",mayankrana,https://api.github.com/repos/pytorch/pytorch/git/commits/1c414426df0b5957b91abe396a91e4f51c77dc55
bc4f233b569f2742621d801cfa3711dd0e108442,"Make use of zeus kv store.

Summary:
Implement atomic add operation for zeus kv store.
All nodes now use zeus as KVStore instead of replying on master hosting a KVServer
Code cleanup.

Reviewed By: andrewwdye

Differential Revision: D5581697

fbshipit-source-id: ba7d99215fb478a30942ff593f13dad65aa48d36",rishirajsinghjhelumi,https://api.github.com/repos/pytorch/pytorch/git/commits/bc4f233b569f2742621d801cfa3711dd0e108442
d01adcbe0e4c8007be0cfc318871eb745858b271,modify orthogonal init,nadavbh12,https://api.github.com/repos/pytorch/pytorch/git/commits/d01adcbe0e4c8007be0cfc318871eb745858b271
0a9f93e43c3939040b2b3ce8f30482e094f9f56f,add env var for python executable,gfrogat,https://api.github.com/repos/pytorch/pytorch/git/commits/0a9f93e43c3939040b2b3ce8f30482e094f9f56f
efda016108f16d4ac6087a3bcb0909729c1a7e11,"fix dynamic-type-mismatch (ubsan) in caffe2/caffe2/core/tensor.h

Summary:
UBSan report:

```
UndefinedBehaviorSanitizer: dynamic-type-mismatch caffe2/caffe2/core/tensor.h:786:22 in
caffe2/caffe2/core/tensor.h:787:19: runtime error: member call on address 0x60c01f610440 which does not point to an object of type 'caffe2::Tensor<caffe2::Tensor<caffe2::CPUContext> >'
*** Aborted at 1505298367 (Unix time, try 'date -d 1505298367') ***
*** Signal 6 (SIGABRT) (0xf2) received by PID 242 (pthread TID 0x7fb376f06700) (linux TID 33215) (maybe from PID 242, UID 0), stack trace: ***
0x60c01f610440: note: object is of type 'N6caffe26TensorINS_10CPUContextEEE'
 07 5e 81 60  c8 47 13 35 00 00 00 00  90 f3 73 80 20 60 00 00  98 f3 73 80 20 60 00 00  a0 f3 73 80
              ^~~~~~~~~~~~~~~~~~~~~~~
              vptr for 'N6caffe26TensorINS_10CPUContextEEE'
    #0 0x1f0d1c22 in std::vector<long, std::allocator<long> > caffe2::GetTensorInfo<caffe2::Tensor<caffe2::CPUContext> >(void const*, bool*, unsigned long*, caffe2::DeviceOption*) caffe2/caffe2/core/tensor.h:787:19
    #1 0x9a5e0a1 in caffe2::FacebookOperatorObserver::log() caffe2/caffe2/fb/init/net_observer.cpp:300:15
    #2 0x9a5b49d in caffe2::FacebookOperatorObserver::Stop() caffe2/caffe2/fb/init/net_observer.cpp:229:11
    #3 0x447d046 in caffe2::Operator<caffe2::CPUContext>::Run(int) caffe2/caffe2/core/operator.h:308:20
    #4 0x1ecedb2f in caffe2::SimpleNet::Run() caffe2/caffe2/core/net_simple.cc:51:14
    #5 0x1f1ba169 in caffe2::Workspace::RunNet(std::basic_fbstring<char, std::char_traits<char>, std::allocator<char>, std::fbstring_core<char> > const&) caffe2/caffe2/core/workspace.cc:211:26
...
```

The bug is that `GetTensorType` and `GetTensorType` take context as template argument, not tensor itself.

Reviewed By: bddppq

Differential Revision: D5826781

fbshipit-source-id: 9cfd2ca1aaef6f8ee8a556ce7b553c0a4f43a100",philippv,https://api.github.com/repos/pytorch/pytorch/git/commits/efda016108f16d4ac6087a3bcb0909729c1a7e11
3821fca0c65e7197c1366218e45b2ba29c05d136,"DOC: i{send, recv} message order with MPI backend",stsievert,https://api.github.com/repos/pytorch/pytorch/git/commits/3821fca0c65e7197c1366218e45b2ba29c05d136
c3fd31b1a2c87dc3ce398d1c3e63eac5a9417bc5,"weights for labels in image_input_op

Summary: Introduced weight for labels in multi-lable setting. An extra weight blob is introduced and read in the operator in case lable setting is weighted sparse.

Reviewed By: kevinwilfong

Differential Revision: D5812467

fbshipit-source-id: efb209092e1e9effc915b0a753fa0c67b47a4fb6",dkm2110,https://api.github.com/repos/pytorch/pytorch/git/commits/c3fd31b1a2c87dc3ce398d1c3e63eac5a9417bc5
ea8b09365c0f0ce88fcc147098eb0adcb09d4951,"Specifying the value used for padding (#2751)

* Specifying the value used for padding

The ""pad_packed_sequence"" function fills padded elements with zeros, but sometimes it is not useful. For example, some previous papers on NLP, including my recent paper [1], use a max-pooling technique for RNN-based sentence representations. More specifically, the max-pooling technique selects the maximum value from all time steps (i.e., hidden states) for each dimension. In such a case, we do not want the padded zeros to be selected. To overcome this situation, we can simply use a very small value instead of zero.

An LSTM example is shown below:

input = embedding(Variable(batchInput))
packedInput = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first = True)
h, (hn, cn) = self.encoder(packedInput, (h0, c0))
h, _ = nn.utils.rnn.pad_packed_sequence(h, -1024.0 batch_first = True)
sentenceRep, _ = torch.max(h, 1, keepdim = True)

[1] A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks. Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. The 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017).
https://arxiv.org/abs/1611.01587 (Equation (4))

* Modified the order of the arguments

Following the suggestion, I modified the order of the arguments.",hassyGo,https://api.github.com/repos/pytorch/pytorch/git/commits/ea8b09365c0f0ce88fcc147098eb0adcb09d4951
c6ea6ed8ffa76c398db076a6a8efc198b9379787,"Add Nd Padding, Pad1d functions and ConstantPad3d (#2657)",dhpollack,https://api.github.com/repos/pytorch/pytorch/git/commits/c6ea6ed8ffa76c398db076a6a8efc198b9379787
5229a79bf591d3b37aacefbf0ffcd1987bfb968b,Implement THCUNN code for GridSampler (#2737),zou3519,https://api.github.com/repos/pytorch/pytorch/git/commits/5229a79bf591d3b37aacefbf0ffcd1987bfb968b
39434ee2e44fb2770db22dc83a3c9ca1a664a8da,Added LPPool1d. (#2783),ejoebstl,https://api.github.com/repos/pytorch/pytorch/git/commits/39434ee2e44fb2770db22dc83a3c9ca1a664a8da
1a83c372ecb0c81bdd5110c516cc6e9f9554a469,address issue #1488 by using defaultdict in load_state_dict,randxie,https://api.github.com/repos/pytorch/pytorch/git/commits/1a83c372ecb0c81bdd5110c516cc6e9f9554a469
2b9765ad02b666d623566935b385fc3d058ad33d,Erf and erfinv (#2799),IraKorshunova,https://api.github.com/repos/pytorch/pytorch/git/commits/2b9765ad02b666d623566935b385fc3d058ad33d
a340d141de62ead79a1b9b2a6aa9d1725513b4f1,"Check num_elements > num_samples in UniformSampling

Summary: When num_elements is less than num_samples, a workflow should fail during net construction time. Currently, it fails at run time.

Reviewed By: kittipatv

Differential Revision: D5858085

fbshipit-source-id: e2ab3e59848bca58806eff00adefe7c30e9ad891",anshulverma,https://api.github.com/repos/pytorch/pytorch/git/commits/a340d141de62ead79a1b9b2a6aa9d1725513b4f1
34a1d414a56c82a716601baa5092ccb262974801,[Distributed/Gloo] 3X performance improvement of Gloo AllReduce By Enabling CUDA Direct (#2827),teng-li,https://api.github.com/repos/pytorch/pytorch/git/commits/34a1d414a56c82a716601baa5092ccb262974801
8103e185d4f58bd15be999dd65a144fea6a2655a,"Fix OSX build w/CUDA=ON

Summary:
connect to https://github.com/caffe2/caffe2/issues/1249

With this change, build, install, and smoke test pass on OSX with CUDA=ON.
```
$ cmake -DUSE_CUDA=ON ..
$ sudo make install
$ python -c 'from caffe2.python import core' 2>/dev/null && echo ""Success"" || echo ""Failure""
Success
```
Closes https://github.com/caffe2/caffe2/pull/1251

Differential Revision: D5898758

Pulled By: Yangqing

fbshipit-source-id: 4b2362af800dbcf2d5c441ab97f68a1c23f19f24",Setogit,https://api.github.com/repos/pytorch/pytorch/git/commits/8103e185d4f58bd15be999dd65a144fea6a2655a
605beb25658f915e67fff5856ad55e9b7ea54dea,Parallelize CUDA LookupTable_renorm (#2803),squidgetx,https://api.github.com/repos/pytorch/pytorch/git/commits/605beb25658f915e67fff5856ad55e9b7ea54dea
8ffe8eca6c18d27a6e14261c829f57cd783abcd0,rename spatial version,SsnL,https://api.github.com/repos/pytorch/pytorch/git/commits/8ffe8eca6c18d27a6e14261c829f57cd783abcd0
0a5ee1e8061dbef76f0008728708a2ff1de7e131,"Implemented RowWiseSparseAdagrad operator that only keeps one moment term per embedding

Summary: Implemented version of SparseAdagrad that only keeps track of an average sum of squared gradients term for each row of the parameter tensor, rather than a sum of squared gradients term for each individual parameter.

Differential Revision: D5881918

fbshipit-source-id: bd96ccf25554b457baaaca9309fc8048adbb37f7",ffjiang,https://api.github.com/repos/pytorch/pytorch/git/commits/0a5ee1e8061dbef76f0008728708a2ff1de7e131
711d7137c753fba9423f103cfa25274a85ec7773,"Implement the gradient operator for element-wise Logit

Summary: Implemented logit gradient with eps as arg.  Add the unit test for it and explored the optimal parameter to run the test.

Reviewed By: asaadaldien

Differential Revision: D5910655

fbshipit-source-id: 44898b784a57c7ad45519b202b1eaf95c1c4d460",ygdx,https://api.github.com/repos/pytorch/pytorch/git/commits/711d7137c753fba9423f103cfa25274a85ec7773
f2037970cb665c894d52adfa68e05a48b05a6a73,Cleanup for 'prob_dist' in multinomial function (fixes #1584),MicaelCarvalho,https://api.github.com/repos/pytorch/pytorch/git/commits/f2037970cb665c894d52adfa68e05a48b05a6a73
e67c2bc5674dcc42226b9ff603e6c365c868a7b5,"Fix detection of NCCL_INCLUDE_DIR (#2877)

* Fix detection of nccl.h when libnccl.so is in /usr/lib/x86_64-linux-gnu and similar paths

* full support for independent NCCL_LIB_DIR and NCCL_INCLUDE_DIR

* lint fix

* add back CUDA_HOME",ducksoup,https://api.github.com/repos/pytorch/pytorch/git/commits/e67c2bc5674dcc42226b9ff603e6c365c868a7b5
c775b904262fd62385aaa4a75ccd0e6395d48e84,"Fix aten submodule

Effectively D5935765",facebook-github-bot,https://api.github.com/repos/pytorch/pytorch/git/commits/c775b904262fd62385aaa4a75ccd0e6395d48e84
a64daf2c5975bfae53c43ede88c3a84aa4eadad7,support dictionary return types in nn.Module's __call__ (#2037),DeNeutoy,https://api.github.com/repos/pytorch/pytorch/git/commits/a64daf2c5975bfae53c43ede88c3a84aa4eadad7
00b62db7230a820cbb9021eb273801ff89f00743,"Fix scope error

error: â€˜getInitConfigâ€™ was not declared in this scope",jcatana,https://api.github.com/repos/pytorch/pytorch/git/commits/00b62db7230a820cbb9021eb273801ff89f00743
c488a9e9bf9eddca6d55957304612b88f4638ca7,Add Numpy array interface to tensors,kohr-h,https://api.github.com/repos/pytorch/pytorch/git/commits/c488a9e9bf9eddca6d55957304612b88f4638ca7
9088a940d7485da1dc78009dbbd882722409ca48,Completed Stride() documentation (#2948),Ujjwal-9,https://api.github.com/repos/pytorch/pytorch/git/commits/9088a940d7485da1dc78009dbbd882722409ca48
b3bcba60c748c244d1b5b1c10aba3cb88186c4ce,"Correct padding docs of 3D modules (#2970)

3D modules apply padding on all three sides. ""Both"" doesn't make sense here.
I used the wording of the AvgPool3d docstring, where it was already correct.",mdraw,https://api.github.com/repos/pytorch/pytorch/git/commits/b3bcba60c748c244d1b5b1c10aba3cb88186c4ce
f535700cccd1343113e7e552ce111759043a4839,"Add weighted_sampling operator to Caffe2

Summary: Add weighted_sampling operator to Caffe2

Reviewed By: akyrola

Differential Revision: D5962199

fbshipit-source-id: ab3f56a1dc7b8eaf4ed4d74af6c6c08dccca5a1e",qinghe-ss,https://api.github.com/repos/pytorch/pytorch/git/commits/f535700cccd1343113e7e552ce111759043a4839
5f8bab47c8f87c73f08ad13705aea1fda747e997,bugfix for 2428 ussue (#3000),Scitator,https://api.github.com/repos/pytorch/pytorch/git/commits/5f8bab47c8f87c73f08ad13705aea1fda747e997
44a0f6805e64c7222017ea7cbd6be9f033543ae7,"fix get_cpu_blob_name()

Summary: add def get_cpu_blob_name(self, base_str) back before D6001124

Reviewed By: akyrola

Differential Revision: D6004994

fbshipit-source-id: 318581d2b2c22878929993160da8edcb7d7a58e6",ellie-wen,https://api.github.com/repos/pytorch/pytorch/git/commits/44a0f6805e64c7222017ea7cbd6be9f033543ae7
77ae903650f89e3c07d69cd6e510b700fb827be0,"Skip negative indices

Summary: A single negative index can crash the job today.  We want to skip a few of them but not a lot.  If we skip too many then we will force the job to crash.

Reviewed By: kennyhorror

Differential Revision: D6003461

fbshipit-source-id: 7881ed6c2cfa78c7bda90c7aa01e81ca00fd08a6",russoue,https://api.github.com/repos/pytorch/pytorch/git/commits/77ae903650f89e3c07d69cd6e510b700fb827be0
3e9f0092ebd28164802f65f07b681161ac23fb3a,"Remove Redundant CMAKE_BUILD_TYPE

Summary: Closes https://github.com/caffe2/caffe2/pull/1323

Differential Revision: D6031534

Pulled By: Yangqing

fbshipit-source-id: de75523b17f67d092d45edb91fbb4e83c67b04be",malekdoghman,https://api.github.com/repos/pytorch/pytorch/git/commits/3e9f0092ebd28164802f65f07b681161ac23fb3a
17d68f824dccf7ea9982e26f1c398406b1bba108,Fix typo. (#3140),acburigo,https://api.github.com/repos/pytorch/pytorch/git/commits/17d68f824dccf7ea9982e26f1c398406b1bba108
f176c864f0d04a8b5d24a1774b3b766cb89b8a29,minor autograd reference change in readme (#3144),rasbt,https://api.github.com/repos/pytorch/pytorch/git/commits/f176c864f0d04a8b5d24a1774b3b766cb89b8a29
23a3f78988aeeaf3ca193ec0b03fd755ddbcdd49,"Reverse the order of checks in torch.gather (#3130)

* Reverse the order of checks in torch.gather

* Remove unnecessary comment

* Add missing check for indexing dimension",elanmart,https://api.github.com/repos/pytorch/pytorch/git/commits/23a3f78988aeeaf3ca193ec0b03fd755ddbcdd49
50de9160aa2d79af8ea271f97fe9370fe003e88e,"Update THDTensor.cpp

Add `__STDC_FORMAT_MACROS` to fix gcc issues",davidmascharka,https://api.github.com/repos/pytorch/pytorch/git/commits/50de9160aa2d79af8ea271f97fe9370fe003e88e
6a4182eeadd14514201bb61ab4e295bd46e92bdb,"weighted sample op cuda

Summary: CUDA version of weighted sampling operator; minor changes for CPU version

Reviewed By: asaadaldien

Differential Revision: D6106668

fbshipit-source-id: 42d7607bd845a4a39cf5b89d7476904cb5928431",enosair,https://api.github.com/repos/pytorch/pytorch/git/commits/6a4182eeadd14514201bb61ab4e295bd46e92bdb
0748ea56ebaddc44efcf5ff734bede6c14b7fb89,"Change size by kernel_size in __repr__

Probably, __repr__ should return `MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1)))` -> `MaxPool2d (kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1)))`",vfdev-5,https://api.github.com/repos/pytorch/pytorch/git/commits/0748ea56ebaddc44efcf5ff734bede6c14b7fb89
5fc122bf3973504e619cd677ad4a7fc1011642cd,"Fix to #2236 - tensor.numpy() checks that no positional arguments are passed. (#3224)

* tensor.numpy() checks that no arguments are passed

* tensor.numpy() checks that no arguments are passed

* Improve .numpy() argument checking performance",ghostFaceKillah,https://api.github.com/repos/pytorch/pytorch/git/commits/5fc122bf3973504e619cd677ad4a7fc1011642cd
b5170c8bf11158cff722b1632bcd6954d87b7bbe,"improves pack padded sequence operation runtime #1788 (#3278)

* improves pack padded sequence operation runtime #1788

* error message",dirkweissenborn,https://api.github.com/repos/pytorch/pytorch/git/commits/b5170c8bf11158cff722b1632bcd6954d87b7bbe
e43a63a968083aeff957c04c7d8c9ffa794a446d,"tensor: Ensure that the tensor is contiguous before pinning (#3266) (#3273)

* tensor: Ensure that the tensor is contiguous before pinning (#3266)

pin_memory() was producing out-of-order tensor when the given
tensor was transposed, i.e. in column-major order.
This commit fixes this by calling contiguous() before pinning.

* test: add contiguous test for pin_memory (#3266)",ozancaglayan,https://api.github.com/repos/pytorch/pytorch/git/commits/e43a63a968083aeff957c04c7d8c9ffa794a446d
b46ced4aabab244de88981ed50aae5de9a106bf4,"clarification in docstring of Module.register_forward_hook() (#3279)

* made it explicit in the docstring of Module.register_forward_hook() that the hook(s) will be called AFTER calling forward().

* added ""every time"" in docstring of Module.register_forward_pre_hook()",andreh7,https://api.github.com/repos/pytorch/pytorch/git/commits/b46ced4aabab244de88981ed50aae5de9a106bf4
837f933cac6c55366752fe14a793d0010eef0d89,"remove 'path' from key_averages header

path appears to be unused",alexholdenmiller,https://api.github.com/repos/pytorch/pytorch/git/commits/837f933cac6c55366752fe14a793d0010eef0d89
a99506f2fcd7ccad307662f7b4ebd33fcd13b159,"fixed  error: namespace ""std"" has no member ""min""",zhaopengme,https://api.github.com/repos/pytorch/pytorch/git/commits/a99506f2fcd7ccad307662f7b4ebd33fcd13b159
9735ddd89904be67e4924f6e69ac6df4cf40c083,check_env_flag now ignores case (#3317),Erotemic,https://api.github.com/repos/pytorch/pytorch/git/commits/9735ddd89904be67e4924f6e69ac6df4cf40c083
dc6c9e8df8ef1963147774edf158f01293c55c0c,"Fix compilation without numpy.

Fix this and related errors:

    Tensor.cpp:309:47: error: â€˜PyArray_Checkâ€™ was not declared in this scope",darrengarvey,https://api.github.com/repos/pytorch/pytorch/git/commits/dc6c9e8df8ef1963147774edf158f01293c55c0c
7b00adf5d31c4fbf5556151514eadea4c1e2aa4b,"Add CUDNN_LIB_DIR in rpath (#3255)

* Add CUDNN_LIB_DIR in link -rpath

* insert CUDNN_LIB_PATH in front of rpath",bermanmaxim,https://api.github.com/repos/pytorch/pytorch/git/commits/7b00adf5d31c4fbf5556151514eadea4c1e2aa4b
a0ce84e476b8b3f2614085ad151ecb4c764b1aac,fix triplet margin loss documentation (#3339),johny-c,https://api.github.com/repos/pytorch/pytorch/git/commits/a0ce84e476b8b3f2614085ad151ecb4c764b1aac
2e42272cc1a3d4441053c0a859c40550c17acad4,Make DataParallel a no-op when CUDA not available (#3318),Nintorac,https://api.github.com/repos/pytorch/pytorch/git/commits/2e42272cc1a3d4441053c0a859c40550c17acad4
d38fccc586ba6e6c7f3811d7255c17d0ea34d54c,Debian/Ubuntu comes with GCC 4.9.2 and it does require -D_FORCE_INLINES (#3380),eLvErDe,https://api.github.com/repos/pytorch/pytorch/git/commits/d38fccc586ba6e6c7f3811d7255c17d0ea34d54c
dce525ab6bab5ea308863620f41f36540807298f,"adds sample_n function (#3249)

* adds sample_n function

* fixes style issues

* uses more efficient api calls

* fix bug where transpose applied to 1 dimension",geoffroeder,https://api.github.com/repos/pytorch/pytorch/git/commits/dce525ab6bab5ea308863620f41f36540807298f
3f6fccd1a8e46036ae41a971b35cbd4dfd48ba93,fixes for torch.nn.Hardtanh (examples and CPU implementation) (#3391),AceCoooool,https://api.github.com/repos/pytorch/pytorch/git/commits/3f6fccd1a8e46036ae41a971b35cbd4dfd48ba93
7b5ac333adf7efaf16a3ca2f68fb74afd49a44d7,"Update README.md (#3392)

Getting started HyperLinks match up now.",frankcash,https://api.github.com/repos/pytorch/pytorch/git/commits/7b5ac333adf7efaf16a3ca2f68fb74afd49a44d7
d4a0ec62dc42d8d437a1b1e493f54b6a3161b0ef,Typo fix in torch.median (#3399),gokceneraslan,https://api.github.com/repos/pytorch/pytorch/git/commits/d4a0ec62dc42d8d437a1b1e493f54b6a3161b0ef
3cb34744db55fd4c6381704e5c8219bdec540dd5,"adaptive pooling supports only specifying size in certain dimension (#3127)

* adaptive pooling supports only specifying size in certain dimension",blackyang,https://api.github.com/repos/pytorch/pytorch/git/commits/3cb34744db55fd4c6381704e5c8219bdec540dd5
b57f82a2cb497b2b3be42dcaf2c44ed8d841d4d1,made the repository available for embedding into other projects,warmspringwinds,https://api.github.com/repos/pytorch/pytorch/git/commits/b57f82a2cb497b2b3be42dcaf2c44ed8d841d4d1
74d1bb54e67e32f423a4c1f9521b08f7edb67e14,Add single argument version of torch.arange (#3494),Dhanton,https://api.github.com/repos/pytorch/pytorch/git/commits/74d1bb54e67e32f423a4c1f9521b08f7edb67e14
68116d7f8447ae1650c762c11a40aa81ef507f86,Fix test_torch.py test for Power see issue #3277 (#3517),avmgithub,https://api.github.com/repos/pytorch/pytorch/git/commits/68116d7f8447ae1650c762c11a40aa81ef507f86
d2ddbaaf8dec36436b8ca892b41b956273680667,Fix command highlight in README (#3521),todpole3,https://api.github.com/repos/pytorch/pytorch/git/commits/d2ddbaaf8dec36436b8ca892b41b956273680667
73431f087b48560ab4b65f8814809fc019061223,"Allow torch.load and torch.save to take pathlib.Path (#3589)

* Allow torch.load to take pathlib.Path

pathlib has been python standard library for filesystem path since python 3.4
But `torch.load` currently cannot take `pathlib.Path` as its filename of state dictionary.
I changed `torch.load` and `_with_file_like` to check so that they can accept `pathlib.Path` typed filepath.

* Fix flake8: too long line & indentation",j-min,https://api.github.com/repos/pytorch/pytorch/git/commits/73431f087b48560ab4b65f8814809fc019061223
30d06218cbb3fd787b15a2194c2cd3fb0e1edf46,"Solved boolean ambiguity for variables and tensors which contain one value. (#3656)

* Solved boolean ambiguity for variables and tensors which contain one value.

* Update variable.py

* Update tensor.py",VladislavZavadskyy,https://api.github.com/repos/pytorch/pytorch/git/commits/30d06218cbb3fd787b15a2194c2cd3fb0e1edf46
e33df2b88a31c08567c81f49c45f1eb530cd7ef4,"Add border-padding for grid_sampler (#3599)

* adds border padding to spatial grid sampler

* fixes flake8 * adds docs",josecabjim,https://api.github.com/repos/pytorch/pytorch/git/commits/e33df2b88a31c08567c81f49c45f1eb530cd7ef4
e8abfd359a8052bb24d116841d49888d8e67b343,"Limit this fix to apple clang only

Summary:
Use ""__apple_build_version__"" macro to distinguish Apple's Clang while brew installed LLVM will compile caffe2 without trouble.
Closes https://github.com/caffe2/caffe2/pull/1461

Differential Revision: D6316861

Pulled By: Yangqing

fbshipit-source-id: f7a08cdd8822b197a93aa11dc8f28ef5cd738eee",CaoZhongZ,https://api.github.com/repos/pytorch/pytorch/git/commits/e8abfd359a8052bb24d116841d49888d8e67b343
7b047c161de1576fd644d6f89aefe0d1d0462d8f,"NegateGradientOp and test

Summary: add NegateGradientOp: in forward pass, this op simply copies the input to output. In backward pass, it flips the sign of gradients.

Reviewed By: dragonxlwang

Differential Revision: D6314456

fbshipit-source-id: 56afd8b131eff9f7e120ab7e4e87461df49649d4",Wakeupbuddy,https://api.github.com/repos/pytorch/pytorch/git/commits/7b047c161de1576fd644d6f89aefe0d1d0462d8f
b97dfc8a92109fc4c311e0767a95ae75ee8ee4a5,"Pretty names: support names set via export or Variable constructor (#3371)

Add (fully opt-in) functionality to support setting pretty names for
nodes in the graph. In particular

- Variable now has a `name` parameter in the constructor
- export now has `input_names` and `export_names` parameters

Nodes that are not named via this mechanism continue to be named
internally with unique integers.

Names have a few rules.

- They must all be unique in the graph.
- They may not be integers (because of potential conflicts with
  internally generated names).",anderspapitto,https://api.github.com/repos/pytorch/pytorch/git/commits/b97dfc8a92109fc4c311e0767a95ae75ee8ee4a5
c4b0db5079ccf8d0c92c2950933d78926f501431,"Remove hard file offset reset in load() (#3695)

* improved file offset logic

* load offset test

* whitespace

* needless exception handling

* test integer in binary",plang85,https://api.github.com/repos/pytorch/pytorch/git/commits/c4b0db5079ccf8d0c92c2950933d78926f501431
1f64c2ef91a6251e548222488ee93daecae5e914,"Rename pyro.distributions.Multinomial -> .Categorical (#3766)

* Rename distributions.Multinomial -> distributions.Categorical

* Rename Multinomial -> Categorical

* Update docs

* Update variable.py

* Update distributions.py

* Update variable.py",fritzo,https://api.github.com/repos/pytorch/pytorch/git/commits/1f64c2ef91a6251e548222488ee93daecae5e914
2c39f3de9908903365d9d956b1464896d553e40e,flake8 fix,stefan-it,https://api.github.com/repos/pytorch/pytorch/git/commits/2c39f3de9908903365d9d956b1464896d553e40e
38cd6b3bd0d5b4e33693b96731edce1cfa825d62,"Fix run_test.sh mpiexec failures under virtual python envs (#3792)

If virtual python environment is in use (e.g. conda) and
mpiexec was compiled with --enable-mpirun-prefix-by-default option,
it will fail by default as the path is updated to the prefix and
different python (most cases /usr/bin/python) will be used.",sdmonov,https://api.github.com/repos/pytorch/pytorch/git/commits/38cd6b3bd0d5b4e33693b96731edce1cfa825d62
a9ef76b9c68b7caa7254087721854be64d8ba995,Reflect renaming of OS X to macOS (#3795),srstevenson,https://api.github.com/repos/pytorch/pytorch/git/commits/a9ef76b9c68b7caa7254087721854be64d8ba995
3ac2a20c5ffbab75243529632a4778797e51fdba,"Fix DataParallel scattering for empty lists / dicts / tuples (#3769)

* Fix DataParallel scattering for empty lists and dicts

* Fix DataParallel scattering for empty tuples",pemazare,https://api.github.com/repos/pytorch/pytorch/git/commits/3ac2a20c5ffbab75243529632a4778797e51fdba
fca77c9e256b9f67ae7d593ba242cab033bcd89f,Correct gradient of rosenbrock (#3881),theFool32,https://api.github.com/repos/pytorch/pytorch/git/commits/fca77c9e256b9f67ae7d593ba242cab033bcd89f
3a5bbc214004c938156f4dda359265d170d3402c,improve PackedSequence docs to explain batch_sizes (#3878),tristanz,https://api.github.com/repos/pytorch/pytorch/git/commits/3a5bbc214004c938156f4dda359265d170d3402c
558516fcdb04c45831989f9a588ff82e8edb5561,"More docs for Conv1d Conv2d (#3870)

* Add a bit of notation explanation

For a first time user of Conv1d, it is not clear from documentation what N, C and L exactly mean. This should clarify this. Same for Conv2d.",iaroslav-ai,https://api.github.com/repos/pytorch/pytorch/git/commits/558516fcdb04c45831989f9a588ff82e8edb5561
0ba9e5a636a279d7ab62a9de2250b4d0c53d8b96,"Remove unused lambda capture

Summary: Remove unused lambda capture parameter which produces a warning in clang 5.

Reviewed By: Maratyszcza

Differential Revision: D6399643

fbshipit-source-id: cb49dc89749bd1d0143148ed559aa397f4d8f592",avaneyev,https://api.github.com/repos/pytorch/pytorch/git/commits/0ba9e5a636a279d7ab62a9de2250b4d0c53d8b96
094df38e2f358d2a87d6083e850b083c90a34d3a,Fix dependency build when pwd contains spaces (#3950),alcinos,https://api.github.com/repos/pytorch/pytorch/git/commits/094df38e2f358d2a87d6083e850b083c90a34d3a
8cb32ba630238f8de15695c9b0492ca02d1a9518,"rnn.py: Note zero defaults for hidden state/cell

* Add a note on zero defaults for hidden states/cells of
  RNNs/LSTMs/GRUs.

* Should fix the note in #434

Signed-off-by: mr.Shu <mr@shu.io>",mrshu,https://api.github.com/repos/pytorch/pytorch/git/commits/8cb32ba630238f8de15695c9b0492ca02d1a9518
3af2b8f4285d3ed57fb3f615c75a86dae81f30c4,"Adding length verification check to pack_segments

Summary:
Adding a check to pack_segments to make sure the lengths passed in add up as expected.

Additionally started to address https://fb.facebook.com/groups/1405155842844877/permalink/1977332432293879/ , but it might not fix that issue, but is still useful if it does not help that issue.

Reviewed By: salexspb

Differential Revision: D6443490

fbshipit-source-id: 680dc763a788a550d321d97a556c5b46e3402dd1",pjh5,https://api.github.com/repos/pytorch/pytorch/git/commits/3af2b8f4285d3ed57fb3f615c75a86dae81f30c4
b43c1b2bed3b4c8390170441b1ec7cfffb54ce4e,"Fix and upgrade brew.layer_norm

Summary:
While working on layer normalization for LSTMs I encountered an issue where the layer norm parameters (which are the scale/gain and bias/shift from the paper) were not registered in the model for `brew.layer_norm`. salexspb explained that this is because it was using the `init_net_param` API instead of `create_param`. This diff fixes this.

While fixing I noticed that I noticed that `brew.layer_norm` actually had a bug where it was multiplying with the bias instead of adding it. Another issue was that the function giving the scale and bias a shape of `[1]`, however the paper (https://arxiv.org/pdf/1607.06450.pdf) specifies that, like for batch norm, there is one scale and bias parameter per neuron, i.e. the shape should be `[1, axis_dimension]`. The API now takes an explicit `dim_in` parameter (also more consistent with other normalization functions in that module) so that this can be specified. See tests for how this now looks.

Reviewed By: jhcross

Differential Revision: D6454290

fbshipit-source-id: fc00ca614de3190c40ab743e8984bec9e85fb58c",goldsborough,https://api.github.com/repos/pytorch/pytorch/git/commits/b43c1b2bed3b4c8390170441b1ec7cfffb54ce4e
f01052ade417cb234d22214800287e9f0aae8b17,"Use enabled in torch.autograd.profiler.emit_nvtx (#4032)

Or else it's always enabled.",lopuhin,https://api.github.com/repos/pytorch/pytorch/git/commits/f01052ade417cb234d22214800287e9f0aae8b17
ba93c031f28f5346f628364dc9712984ba9c6fe0,Moving distribution classes into a separate package,neerajprad,https://api.github.com/repos/pytorch/pytorch/git/commits/ba93c031f28f5346f628364dc9712984ba9c6fe0
dbbfdee4c0fd613ac890e73742ea0a79289f8453,"Implement FCTransposed gradient

Summary: Add FCTranposed gradient implementation

Reviewed By: salexspb

Differential Revision: D6551998

fbshipit-source-id: 0ee8ac7df8c33e55d715bfe65d58bb9bbe1afa50",BIT-silence,https://api.github.com/repos/pytorch/pytorch/git/commits/dbbfdee4c0fd613ac890e73742ea0a79289f8453
1eae0ac8b18dd2aa9a92280ac93f61eddb4328c5,Update instancenorm.py (#4171),innerlee,https://api.github.com/repos/pytorch/pytorch/git/commits/1eae0ac8b18dd2aa9a92280ac93f61eddb4328c5
7f25fff2fee7dafdb3bcfa7c72f141d7698376cf,"add reparameterization, combine sample and sample_n (#4142)",alicanb,https://api.github.com/repos/pytorch/pytorch/git/commits/7f25fff2fee7dafdb3bcfa7c72f141d7698376cf
def4b78b6fd80b8081b9b31cb89b6a187e624039,adding index_select to symbolic.py (#4061),sunaaron,https://api.github.com/repos/pytorch/pytorch/git/commits/def4b78b6fd80b8081b9b31cb89b6a187e624039
7874f611a5c6c3ebcfeb61f40877ea2a5f5469ab,Allowing usage of GPU Direct within PyTorch for the Broadcast operation (#4183),Jorghi12,https://api.github.com/repos/pytorch/pytorch/git/commits/7874f611a5c6c3ebcfeb61f40877ea2a5f5469ab
e393a4f03cdf13da6164990c2f1afe174dbce239,fix typo (#4206),stakahashy,https://api.github.com/repos/pytorch/pytorch/git/commits/e393a4f03cdf13da6164990c2f1afe174dbce239
d4d8698581a213aa0babccf6c06faf2210db1ff4,Fix repeat non owning (#4084),maciejkula,https://api.github.com/repos/pytorch/pytorch/git/commits/d4d8698581a213aa0babccf6c06faf2210db1ff4
5c46427f08bd7e21ce06e8812dd98cb8d76c3b8e,"Rearrange dimensions for pointwise operations for better performance. (#4174)

* Rearrange dimensions for pointwise operations for better performance.

In existing code, pointwise operations on transposed tensors process data
""column by column"", resulting in poor performance.  The worse case happens when
all operands are transposed tensors.

This change tries to ""un-transpose"" tensors in such a case, so that memory
access patterns are as sequential as possible.

* More explanation on what rearrangeDims() does.

* Fixed a very important (and stupid) typo.",yongjik,https://api.github.com/repos/pytorch/pytorch/git/commits/5c46427f08bd7e21ce06e8812dd98cb8d76c3b8e
b86dc0c8ba3d4a6feb16863c04b931f43efd1ad8,"add reduce arg to PoissonNLLLoss (#3770)

* add reduce arg to PoissonNLLLoss

* fixed comments except reference function

* fixed unit test

* small indentation fix

* fixing last comments by richard

* lint check

* another linting issue",kevinzakka,https://api.github.com/repos/pytorch/pytorch/git/commits/b86dc0c8ba3d4a6feb16863c04b931f43efd1ad8
492e26fbcdb9a942f4685e1f09363335c5d65ea7,Pad sequences and Pack sequences (#3875),hhsecond,https://api.github.com/repos/pytorch/pytorch/git/commits/492e26fbcdb9a942f4685e1f09363335c5d65ea7
e519ef53378c15c8d6ba582aba454b2e1ff6169e,Adding torch.expm1() and its inplace function (#4350),vishwakftw,https://api.github.com/repos/pytorch/pytorch/git/commits/e519ef53378c15c8d6ba582aba454b2e1ff6169e
ab80c27b4768a12e4d7fb0c18146817e7f08d4f8,Fix undefined FileNotFoundError (#4384),atabakd,https://api.github.com/repos/pytorch/pytorch/git/commits/ab80c27b4768a12e4d7fb0c18146817e7f08d4f8
ff9d1aeab5d943cfb549cd3257ab7129a624b1e9,removes duplicate variable reference crash from pad_sequences (#4383),williamFalcon,https://api.github.com/repos/pytorch/pytorch/git/commits/ff9d1aeab5d943cfb549cd3257ab7129a624b1e9
fec3d4a079b082ec6d56ab93f31cdb0a228d09fc,"RNN support has been implemented (#4409)

* RNN support has been implemented

https://github.com/pytorch/pytorch/pull/4163/commits/4447b80b5ee66bf347af6ce8dcfb9d3b0683cf02 was merged in and now support RNN",cheeseblubber,https://api.github.com/repos/pytorch/pytorch/git/commits/fec3d4a079b082ec6d56ab93f31cdb0a228d09fc
bb04034bf7c359e3861ec3638d9c6d96b0416622,"Adding a time limit reader

Summary: ReaderWithTimeLimit() class to stop after a certain amount of time

Reviewed By: boryiingsu

Differential Revision: D6477623

fbshipit-source-id: 165874c9344b0c9c7e0b33e12e72e24c46669cb2",kuttas,https://api.github.com/repos/pytorch/pytorch/git/commits/bb04034bf7c359e3861ec3638d9c6d96b0416622
8fd3888c4ce144bf29308b271888cdde0336cd2b,"Provide CMake support for contrib/prof

Summary:
`contrib/prof` provides functionality for profiling (eg. `prof_dag`) but no CMake.
Hence, provide CMake support for building it.

Reviewed By: Yangqing

Differential Revision: D6640488

fbshipit-source-id: 9ed8095b10d7c0337db061206daf2a66f41f4713",ir413,https://api.github.com/repos/pytorch/pytorch/git/commits/8fd3888c4ce144bf29308b271888cdde0336cd2b
c43b120d4329dbcbed114eae8b4cfb23f11b3779,"Improve float precision stability of `linspace` op, fix 4419. (#4470)

Signed-off-by: HE, Tao <sighingnow@gmail.com>",sighingnow,https://api.github.com/repos/pytorch/pytorch/git/commits/c43b120d4329dbcbed114eae8b4cfb23f11b3779
7c729e6321f795e61bf2393aedde3345d6a90f69,- added size_splits to functional (#3837),ptrblck,https://api.github.com/repos/pytorch/pytorch/git/commits/7c729e6321f795e61bf2393aedde3345d6a90f69
6d32e36682c1fe3207de5bffba82f113a694f31f,"Caffe2 Operator: GPU implementation of Swish Activation

Summary: GPU (CUDA) implementation of the Swish activation function in Caffe2.

Reviewed By: Yangqing, xianjiec

Differential Revision: D6656907

fbshipit-source-id: f5f2c667055abf679728d2b5d43998895ddec708",manojkris,https://api.github.com/repos/pytorch/pytorch/git/commits/6d32e36682c1fe3207de5bffba82f113a694f31f
ad45d1bfb54979de5936c4977213cbf86e27ce8d,"Added Caffe2 operator CPU binding for Gloo Allgather

Summary:
Added Caffe2 operator binding for Gloo Allgather algorithm.
Added new test to verify the binding. Binding is supported only for
CPU device with these changes.

Reviewed By: pietern

Differential Revision: D6610074

fbshipit-source-id: b21df9b5e71befbdb6841d6b146727bb4c83d753",rathir,https://api.github.com/repos/pytorch/pytorch/git/commits/ad45d1bfb54979de5936c4977213cbf86e27ce8d
a20ac05c8b18ff0f07a46bc8f9002baa3d7a17c1,Added method cuda to PackedSequence. (#4430),jusjusjus,https://api.github.com/repos/pytorch/pytorch/git/commits/a20ac05c8b18ff0f07a46bc8f9002baa3d7a17c1
81898e5d470caac8674ee457c5265cbf190c54b1,"Fix for wrong newline in caffe_translator.py (Crop layer translation)

Summary:
- fixed the false newline at the initialization of the crop layer translation which caused the exceptions described in issue #1215
Closes https://github.com/caffe2/caffe2/pull/1746

Differential Revision: D6716228

Pulled By: Yangqing

fbshipit-source-id: dd93b06b3b903f96505d6e6f8e67caeb6981fe66",lebroomer,https://api.github.com/repos/pytorch/pytorch/git/commits/81898e5d470caac8674ee457c5265cbf190c54b1
188ee3ff0baf439453c5dd63e1a053ba03b472ad,Fix wrong learning rate evaluation in CosineAnnealingLR in Python 2 (#4656),nguyen-binh-minh,https://api.github.com/repos/pytorch/pytorch/git/commits/188ee3ff0baf439453c5dd63e1a053ba03b472ad
27d7182d6c7e223e04166f33d5ec46ef8b510944,"replace full stop by comma

From (batch. hidden_size) to (batch, hidden_size)",smittal6,https://api.github.com/repos/pytorch/pytorch/git/commits/27d7182d6c7e223e04166f33d5ec46ef8b510944
d452291a72751e94c0871c5c741316bad50b98a5,updated documentation for Embedding layer. Fixes #4682 (#4684),shagunsodhani,https://api.github.com/repos/pytorch/pytorch/git/commits/d452291a72751e94c0871c5c741316bad50b98a5
4ea6e6a556ec4217e25b0eb69f6d5423c9d73d1b,"testSparseLookup

Summary: add basic test for SparseLookup

Reviewed By: kennyhorror

Differential Revision: D6749915

fbshipit-source-id: f97af785e4f89f36788a992843066fd1ec2b75a9",idning,https://api.github.com/repos/pytorch/pytorch/git/commits/4ea6e6a556ec4217e25b0eb69f6d5423c9d73d1b
e9dceec2c8271cda156a7d2452395423bb2a7c92,"Fix the Macro definiton for E in cpuid.h; #undef E

Summary: Changed #undef C to #undef E after the definition of Macro E in cpuid.h

Reviewed By: ot, luciang

Differential Revision: D6763664

fbshipit-source-id: beb221f0c690b5450c39577dd0a843613d802e9c",csummersea,https://api.github.com/repos/pytorch/pytorch/git/commits/e9dceec2c8271cda156a7d2452395423bb2a7c92
1d4e996b8712196cc68c0b52c1e7faaa45ddd64b,"Separate parameter downloading tasks from training tasks and run them in a different group

Summary:
At the end of distributed training, trainer needs to download the parameters back from parameter servers for saving the model. Currently, this parameter downloading happens at the end of job's epoch task group, which creates several problems when checkpointing is enabled for distributed training:

1. When checkpointing is enabled, we run multiple training epochs. At the end of each epoch, the model download tasks will run to collect parameters, but we won't save the model until the true end of training, so there is a big waste of resource.
2. After trainer0 downloads the parameters, these parameters take a lot of memory, so trainer0 can easily run out of memory in the next epoch of training.

Our solution is to insert a parameter download task group between the job's training epoch_group and the job's exit_group.

Reviewed By: azzolini

Differential Revision: D6765393

fbshipit-source-id: 5a4f556fc3c1cd7834a7c406a3c0de3fccd50c49",mraway,https://api.github.com/repos/pytorch/pytorch/git/commits/1d4e996b8712196cc68c0b52c1e7faaa45ddd64b
409b1c8319ecde4bd62fcf98d0a6658ae7a4ab23,Improve wording of Sequential docs (#4790),sotte,https://api.github.com/repos/pytorch/pytorch/git/commits/409b1c8319ecde4bd62fcf98d0a6658ae7a4ab23
c2afd590ae716021bb1747a47756dc6aa356f50d,"parallelize elementwise operation with openmp (#2764)

* parallelize discontiguous tensors' basic operations

* add comments

* remove unnecessary header file

* remove trailing whitespace

* resolve omp parallel for error(need for statement directly) in windows",MlWoo,https://api.github.com/repos/pytorch/pytorch/git/commits/c2afd590ae716021bb1747a47756dc6aa356f50d
1e7d15953e718417bc8b84766d41d41f4090ac25,Added Chi2 test for distributions (#4815),rachtsingh,https://api.github.com/repos/pytorch/pytorch/git/commits/1e7d15953e718417bc8b84766d41d41f4090ac25
3c952426fbad58722a7e93b0d64bc15889e400de,"Add operator attaching net observer

Summary:
Commonly, net observers attach operator observers at construction. This diff separates the logic into a base class to inherit from.
Closes https://github.com/caffe2/caffe2/pull/1806

Reviewed By: salexspb

Differential Revision: D6808623

Pulled By: mdschatz

fbshipit-source-id: 75ef0eea913ef30943541c829c0a976965f42736",mdschatz,https://api.github.com/repos/pytorch/pytorch/git/commits/3c952426fbad58722a7e93b0d64bc15889e400de
ea0283325cdba3ad3b96ef239af70e5bbc70b0a8,fix copy/paste error in debug message,temporaer,https://api.github.com/repos/pytorch/pytorch/git/commits/ea0283325cdba3ad3b96ef239af70e5bbc70b0a8
28f056fed2b82701b155a1e783e57cb2fbd5517d,"add reduce=True argument to MultiLabelMarginLoss (#4924)

* add reduce=True argument to MultiLabelMarginLoss

* Fix lint

* Addressed comments

* Remove unneeded syncthreads calls",li-roy,https://api.github.com/repos/pytorch/pytorch/git/commits/28f056fed2b82701b155a1e783e57cb2fbd5517d
7c7e09fe2dbfdedfc0e57cc416f568e8d43f483d,"Adding the Percentile op & UT

Reviewed By: MisterTea

Differential Revision: D6879507

fbshipit-source-id: 7ca4165a42c073e384d3a6138ef033ca384afd49",eugene-kharitonov,https://api.github.com/repos/pytorch/pytorch/git/commits/7c7e09fe2dbfdedfc0e57cc416f568e8d43f483d
e027277a5720da90b6a69f9b7b4cdd73a290d997,"Set of RL improvements: Fix error in quantile computation. Handle missing values in sparse_to_dense. Replace page_size with minibatch size.

Summary: Set of RL improvements: Fix error in quantile computation.  Handle missing values in sparse_to_dense.  Replace page_size with minibatch size.

Differential Revision: D6888977

fbshipit-source-id: bb84477866c64da5ff57d6c25df1c8d3b799e437",MisterTea,https://api.github.com/repos/pytorch/pytorch/git/commits/e027277a5720da90b6a69f9b7b4cdd73a290d997
6aaa701c9cee1dca8c16c4568bc1a6c36c9bc3a7,"Adding ThresholdedRelu Op support.

Summary: Core operator and python operator changes for adding ThresholdedRelu Op support.

Reviewed By: houseroad

Differential Revision: D6900660

fbshipit-source-id: 9b17ede13ccb3264286389c7fc633ab9c1a7bbbf",girifb,https://api.github.com/repos/pytorch/pytorch/git/commits/6aaa701c9cee1dca8c16c4568bc1a6c36c9bc3a7
0df54f4d7477323a8783f1c722f3be6801f5a414,"Fix typo

Summary: Closes https://github.com/caffe2/caffe2/pull/1919

Reviewed By: ppwwyyxx

Differential Revision: D6945318

Pulled By: orionr

fbshipit-source-id: 700585e56d627d17f8280fe40d81ae8d984a7f40",ppwwyyxx,https://api.github.com/repos/pytorch/pytorch/git/commits/0df54f4d7477323a8783f1c722f3be6801f5a414
a061000250cf7745801ca9118428f97ffaa9ac3f,"Added check and test for betas parameter in Adam optimizer (#5147)

* Added check and test for betas parameter in Adam optimizer

* Simplified test",lazypanda1,https://api.github.com/repos/pytorch/pytorch/git/commits/a061000250cf7745801ca9118428f97ffaa9ac3f
07be53b57f00c58c73bf8f3563213ad77f0c6e15,"Move EmbeddingBag into ATen (#4856)

This diff creates code related to EmbeddingBag in ATen. It also allows sparse gradients.",cpuhrsch,https://api.github.com/repos/pytorch/pytorch/git/commits/07be53b57f00c58c73bf8f3563213ad77f0c6e15
19c2ad8834c009d1ed5a46590208c76501aea0a1,CUDA 9.0 and cuDNN 7 (#5186),Amir-Arsalan,https://api.github.com/repos/pytorch/pytorch/git/commits/19c2ad8834c009d1ed5a46590208c76501aea0a1
e39e86f11980297db2b48f41d625561bae1852f5,Remove deprecated references to volatile (#5193),Iwontbecreative,https://api.github.com/repos/pytorch/pytorch/git/commits/e39e86f11980297db2b48f41d625561bae1852f5
d116e471438982ab57154d1af989aeb68b35d892,Fix compiler error. (#5179),klshrinidhi,https://api.github.com/repos/pytorch/pytorch/git/commits/d116e471438982ab57154d1af989aeb68b35d892
86803004e321c1e3a616b0250579884b5129c8aa,"Fix cmake function to resolve libraries correctly

Summary:
Previous behavior may fail to resolve the correct library name. A rework of https://github.com/caffe2/caffe2/pull/1935 as it was messed up in the rebase...
Closes https://github.com/caffe2/caffe2/pull/1950

Reviewed By: bddppq

Differential Revision: D6974530

Pulled By: yinghai

fbshipit-source-id: 924b653e8ac0b68c46341edfd3eb05d9cc0155f2",yinghai,https://api.github.com/repos/pytorch/pytorch/git/commits/86803004e321c1e3a616b0250579884b5129c8aa
da79697d454aefe5d2b551edb6738a99f47ac1d4,"make explicit about keyword-onlyness of `out` (#5165)

* make explicit about keyword-onlyness of `out`

fix issue 2 of https://github.com/pytorch/pytorch/issues/5156#issuecomment-364521510",zym1010,https://api.github.com/repos/pytorch/pytorch/git/commits/da79697d454aefe5d2b551edb6738a99f47ac1d4
f7cc8e8822ed641d7a349308bc20b65acc6f7c56,"Implementing Pow operator (this merges existing pow with a scalar and new pow with a tensor exponent).

Summary: The old pow operator has been deleted in math_ops.cc, math_ops.cu and math_ops.h, while the new operator supporting scalar and tensor exponent has been added in pow_op.cc, pow_op.h an elementwise_op.cu.

Reviewed By: houseroad

Differential Revision: D6893040

fbshipit-source-id: 30f614beea6f859fee25ce4f85573142885dde45",mnaumovfb,https://api.github.com/repos/pytorch/pytorch/git/commits/f7cc8e8822ed641d7a349308bc20b65acc6f7c56
3975abe5491ffc1cfb2a0df60d050ebbad903787,"Make caffe2 handle out of bounds values correctly

Summary:
according to the new onnx standard in https://github.com/onnx/onnx/pull/513
Closes https://github.com/caffe2/caffe2/pull/1903

Reviewed By: dzhulgakov

Differential Revision: D6920004

Pulled By: smessmer

fbshipit-source-id: 95771f467499ae625ff0156418a4cdf5e5631a02",smessmer,https://api.github.com/repos/pytorch/pytorch/git/commits/3975abe5491ffc1cfb2a0df60d050ebbad903787
5c93ca258bab7bd74a8ec94d64647e48c8ad8797,check attribute existence in SpatialFullConvolution (#5255),katotetsuro,https://api.github.com/repos/pytorch/pytorch/git/commits/5c93ca258bab7bd74a8ec94d64647e48c8ad8797
f51e28440869700787748f7c1542214224bcc5c9,"Fix ASAN detected global buffer overflows in autograd (#5289)

* Fix asan buffer overflow in autograd saved_variable.cpp

* Fix asan global buffer overflow in any_variable_requires_grad

* Revert change in any_variable_requires_grad",vedanuj,https://api.github.com/repos/pytorch/pytorch/git/commits/f51e28440869700787748f7c1542214224bcc5c9
c71c84ee044de7fc7a9b337995f6864ec92cbcd8,Tweak 'detach' docstring. (#5292),malmaud,https://api.github.com/repos/pytorch/pytorch/git/commits/c71c84ee044de7fc7a9b337995f6864ec92cbcd8
fae6c6712180d325d2a5c454bfb51eb42d8d3206,"Configurable flushing denormal numbers on CPU (#5294)

* Configurable flushing denormal numbers on CPU

* Formatting

* Update docs

* Minor doc changes",tunz,https://api.github.com/repos/pytorch/pytorch/git/commits/fae6c6712180d325d2a5c454bfb51eb42d8d3206
642e4d0762c90d9b0b3f8084a0eaae9d71001b93,Fix typos (#5340),juniorrojas,https://api.github.com/repos/pytorch/pytorch/git/commits/642e4d0762c90d9b0b3f8084a0eaae9d71001b93
ba8bbeced316a6406ada879199b11059af818be9,Fix input size checks in ATen for SpatialFractionalMaxPooling (#5337),btgraham,https://api.github.com/repos/pytorch/pytorch/git/commits/ba8bbeced316a6406ada879199b11059af818be9
e2519e7dd15de727e2da0d1e9220dd662be39df7,Fix undefined refence to convolve_5x5_sse on SSE4.1 CPUs (#5371),geordi,https://api.github.com/repos/pytorch/pytorch/git/commits/e2519e7dd15de727e2da0d1e9220dd662be39df7
232837a75e5aebe81c51783ab679bc388df33a78,"[auto] Update onnx to 3ca6622 - Fix pow op's test case (#546) (#548)
https://github.com/onnx/onnx/commit/3ca6622ad0cd33334697aee6eea608d671f21357",onnxbot,https://api.github.com/repos/pytorch/pytorch/git/commits/232837a75e5aebe81c51783ab679bc388df33a78
07646e405e2318327b0c2d2aaa744ab5e80f9f28,no_bias in resnet32x32 (#1817),pengbo-learn,https://api.github.com/repos/pytorch/pytorch/git/commits/07646e405e2318327b0c2d2aaa744ab5e80f9f28
fc9837899ddb7020463cb9db7ace4bd6b2de54bd,Embedding.load_pretrained method (#5350),asoltysik,https://api.github.com/repos/pytorch/pytorch/git/commits/fc9837899ddb7020463cb9db7ace4bd6b2de54bd
bc4c919a9e2e9fa146fc54b00a49838f3e3adf86,"update dependencies (#5423)

On OS X from source I get    `Missing build dependency: Unable to import the typing module. `",rdp,https://api.github.com/repos/pytorch/pytorch/git/commits/bc4c919a9e2e9fa146fc54b00a49838f3e3adf86
8327982904d7b2ffb2096314ba9de02ce495b806,"Set python random seed in workers (#5415)

* Set python random seed in workers

* Import random",achalddave,https://api.github.com/repos/pytorch/pytorch/git/commits/8327982904d7b2ffb2096314ba9de02ce495b806
6b95ca4edacb5ab739daa89fdef3f69f22f4f24f,DataParallel: GPU imbalance warning (#5376),lemairecarl,https://api.github.com/repos/pytorch/pytorch/git/commits/6b95ca4edacb5ab739daa89fdef3f69f22f4f24f
2ad242bee9db420f451c9ac5fb7ee533d9b753ff,"Update Dependencies.cmake (#1920)

force find_package first to find OpenCV 3 when we have default package OpenCV 2 installed.",wjcskqygj2015,https://api.github.com/repos/pytorch/pytorch/git/commits/2ad242bee9db420f451c9ac5fb7ee533d9b753ff
7b33ef4cffed0dcd5c2506c4db1b2624736a22a3,Documentation cleanup for activation functions (#5457),pmitros,https://api.github.com/repos/pytorch/pytorch/git/commits/7b33ef4cffed0dcd5c2506c4db1b2624736a22a3
ca90d4c356ead9a11f37c073224022f4a5acdc86,"Add -s for Android back

Android libraries are statically linked, we'd better strip binaries",freedomtan,https://api.github.com/repos/pytorch/pytorch/git/commits/ca90d4c356ead9a11f37c073224022f4a5acdc86
11a736b6822cbcc68dc84b6bba76508e32b46bd0,"Sqrt op (#2101)

* First attempt on sqrt op

* Adding the Sqrt op along with the test cases

* Made changes per @Yangqing's questions re: tensor format and used hypothesis to generate input tensor",jspisak,https://api.github.com/repos/pytorch/pytorch/git/commits/11a736b6822cbcc68dc84b6bba76508e32b46bd0
15eae9543e8d916656f3394df65d46d3e23f992b,Fixed dimensions in docs of conv and conv_transpose (#5543),cjsg,https://api.github.com/repos/pytorch/pytorch/git/commits/15eae9543e8d916656f3394df65d46d3e23f992b
c93076495dd50dc434e819661022fe3fd12e34c1,add: padding_value to `torch.nn.utils.rnn.pad_sequence` (#5540),Evpok,https://api.github.com/repos/pytorch/pytorch/git/commits/c93076495dd50dc434e819661022fe3fd12e34c1
8376e63738367e72c2823f7619495f087f9368a0,fixed softmax support documentation (#5557),dillonalaird,https://api.github.com/repos/pytorch/pytorch/git/commits/8376e63738367e72c2823f7619495f087f9368a0
6aef608f10bd5a9fc748e141e50d7d27ff42bb64,"Fix Out of Memory failure in test TensorTest.Tensor64BitDimension (#2114)

* WIP: Fix Out of Memory failure in test TensorTest.Tensor64BitDimension

* WIP: update warning message and wrap resize inside TensorTest.Tensor64BitDimension

* WIP: only catch exception which is related to out of memory

* WIP: add return in the out of memory exception",mingzhe09088,https://api.github.com/repos/pytorch/pytorch/git/commits/6aef608f10bd5a9fc748e141e50d7d27ff42bb64
eededd3f976adaf19d56b8aa5fe9a36b712b5751,"Move main reshape logic for easier reuse (#2122)

We'll want to reuse this logic for Int8 Reshape, but currently the code assumes
Input(0) and Output(0) are TensorCPUs, which may not be the case for a
subclass.",kevinbchen,https://api.github.com/repos/pytorch/pytorch/git/commits/eededd3f976adaf19d56b8aa5fe9a36b712b5751
c2721ab5039466ced3af077fae14d8ee578ff484,"Add per-element unique op for CPU (#5503)

Questions/possible future works:

How to template-ize to extend support beyond LongTensor?
How to check if autograd works (and if not, how to add explicit gradient)?
CUDA support?
Testing command:
DEBUG=1 NO_CUDA=1 MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build && DEBUG=1 NO_CUDA=1 MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py develop && python3 test/test_torch.py

Partially fixes #2031

* Initial commit for unique op

* Working unique with test

* Make inverse indices shape conform to input

* flake8 whitespace removal

* address review comment nits

* Expose fn and add docs. Explicitly declare no gradients

* Trial generic dispatch implementation

* Add tests for generics

* flake8 whitespace

* Add basic CUDA error throwing and templateize set

* Explicit contiguous and AT_DISPATCH_ALL_TYPES return

* Remove extraneous numpy conversion

* Refactor out .data calls

* Refactored to variable return length API with wrapper fn as opposed to returning a 0-length tensor, per off-line reviewer comments

* Remove A

* Don't use hidden torch._unique() in test

* Fix documentations",theweiho,https://api.github.com/repos/pytorch/pytorch/git/commits/c2721ab5039466ced3af077fae14d8ee578ff484
8720d72d7c3821f11703acdc2da4819445f50acb,Fixing inconsistent docs (missing parameters docs). (#5620),perone,https://api.github.com/repos/pytorch/pytorch/git/commits/8720d72d7c3821f11703acdc2da4819445f50acb
42bf2f9289daf59072e5b83ed4e2dd9a03eb9739,"Explain floating point issue in torch.arange doc (#5708)

* Explain floating point issue in torch.arange doc

https://github.com/pytorch/pytorch/issues/5556
https://github.com/pytorch/pytorch/issues/5704
https://github.com/pytorch/pytorch/pull/5600

* Add line break to stay below max comment length

* Copyedit

* Typofix",bheinzerling,https://api.github.com/repos/pytorch/pytorch/git/commits/42bf2f9289daf59072e5b83ed4e2dd9a03eb9739
f69fb3829a35e4b46181c33f2cbfae5eb7bbe11a,Add documentation for LPPool1D (#5730),calvinleenyc,https://api.github.com/repos/pytorch/pytorch/git/commits/f69fb3829a35e4b46181c33f2cbfae5eb7bbe11a
f377159cc812927a63074ec41c3ad2e7749385a8,"make dimension checker of `scatter_add_` consistent with `scatter_` (#5659)

* make dimension checker of scatter_add_ consistent with scatter_

* move TH_TENSOR_DIM_APPLY3_SIZE_SCATTER out of scatter and scatterAdd",Officium,https://api.github.com/repos/pytorch/pytorch/git/commits/f377159cc812927a63074ec41c3ad2e7749385a8
f5f6258288da2d52a1414367abe81e369a0845eb,Enable additional tensor types in Gloo backend (#5483),myleott,https://api.github.com/repos/pytorch/pytorch/git/commits/f5f6258288da2d52a1414367abe81e369a0845eb
a24d4b74541c394b9af54ad420d664fb0205f7fb,"Fix compilation with CUDA < 8.0 (#5621)

* Compile with CUDA 7.5 and GCC > 4.9

* Removed static keyword from device constants.",jpuigcerver,https://api.github.com/repos/pytorch/pytorch/git/commits/a24d4b74541c394b9af54ad420d664fb0205f7fb
b499332aafafd3cb11f55d32988f898a3d3d00bb,fixed a message typo in ATen CMakeLists.txt (#5802),mnicnc404,https://api.github.com/repos/pytorch/pytorch/git/commits/b499332aafafd3cb11f55d32988f898a3d3d00bb
0b5b28f6a77ccfa5cdb1fff691cdeb488da43c0b,"add some onnx exported supports (#5734)

* add some onnx export supports

* fix the number of spaces

* fix blank line and white spaces

* rm initialize

* split upsample, gt off, add lt",acnokego,https://api.github.com/repos/pytorch/pytorch/git/commits/0b5b28f6a77ccfa5cdb1fff691cdeb488da43c0b
99b1f6cfad85a4856550cc1e787afd7ff9e6c6aa,"Enable resetting of batchnorm running moments and cumulative (""simple"") moving average (#5766)",jma127,https://api.github.com/repos/pytorch/pytorch/git/commits/99b1f6cfad85a4856550cc1e787afd7ff9e6c6aa
19367537084a0e9352646947c0f4974b89e2aae5,Added an implementation of a multivariate normal distribution (#4950),tbrx,https://api.github.com/repos/pytorch/pytorch/git/commits/19367537084a0e9352646947c0f4974b89e2aae5
6af3429f4fd9cf297d8009014561bd891c673f3b,"Add 2D Row-wise Arg Max Operator

Add operator to return row-wise arg max of 2D matrix.",econti,https://api.github.com/repos/pytorch/pytorch/git/commits/6af3429f4fd9cf297d8009014561bd891c673f3b
7aeda25cfbdfd775106782b48d7d9c2253495a71,"Add type / shape inference for IndexHash op

just as title says",itomatik,https://api.github.com/repos/pytorch/pytorch/git/commits/7aeda25cfbdfd775106782b48d7d9c2253495a71
d4996e50deb0f593d270180e75c971fc43d4ca4c,"Minor (but important) documentation update for SplitOp

This was just a typo, but an important one. Confused me for a while.",ashwinb,https://api.github.com/repos/pytorch/pytorch/git/commits/d4996e50deb0f593d270180e75c971fc43d4ca4c
7bef225e720dd66859fdab9781179c567b96cfb0,"[Caffe2] Fix double map lookup in operator_schema.h

[Caffe2] Fix double map lookup in `operator_schema.h`.",wqfish,https://api.github.com/repos/pytorch/pytorch/git/commits/7bef225e720dd66859fdab9781179c567b96cfb0
834608809413029e6a66c7d14a171c4fd250a86b,"Export number of iterations of AtomicIterOp (#2338)

* Exported AtomicIterOp count

* Exported AtomicIterOp count",mlappelbaum,https://api.github.com/repos/pytorch/pytorch/git/commits/834608809413029e6a66c7d14a171c4fd250a86b
7cbbc0bc74aa13e21c452b6550d246294663b6e2,Implementation of the logistic-normal distribution (#5547),alshedivat,https://api.github.com/repos/pytorch/pytorch/git/commits/7cbbc0bc74aa13e21c452b6550d246294663b6e2
2030ac7545b29a684a88b6f1b5fc70c2eb3bb036,Recommend citation (implements #4126) (#5955),katrinleinweber,https://api.github.com/repos/pytorch/pytorch/git/commits/2030ac7545b29a684a88b6f1b5fc70c2eb3bb036
831780390ce8f72cfef53766eb3def9b1c326e38,"Fixed non-determinate preprocessing on DataLoader (#4640)

dded ind_worker_queue parameter to data.DataLoader. It makes preprocessing determinate.

DataLoader in multiprocessing mode may cause non-deterministic issue. Even if radom_seed has frozen, each subprocess may get tasks in unstable order. This is caused by different I/O time while data loads. If you use augmentation while data loading, it makes results unreproduceble. Look at the https://discuss.pytorch.org/t/deterministic-non-deterministic-results-with-pytorch/9087

To fix this issue I have added the individual queue for each worker. In this case each worker get tasks in the stable order. In summary, subprocess produces the stable results.

To reproduce issue you may change ind_worker_queue to False and run the script several times.
Code to reproduce issue is in the corresponding PR.

* TestIndividualWorkerQueue added to DataLoader tests

* Review fixes

* ""Simplify"" code by removing itertools

* Rebase conflicts fix

* Review fixes

* Fixed shutdown behavior

* Removed ind_worker_queue flag.

* Rebase on master

* Disable tests that use DataLoader with multiple workers (#5322)",AlexanderRadionov,https://api.github.com/repos/pytorch/pytorch/git/commits/831780390ce8f72cfef53766eb3def9b1c326e38
2df578a71af3fedf84717a112ef9c18335a01d4f,add mkl dependencies to setup (#5991),jpchen,https://api.github.com/repos/pytorch/pytorch/git/commits/2df578a71af3fedf84717a112ef9c18335a01d4f
8054dbd6552c3600916ff8bb9d1e811e95a6f13d,Trivial typo (#6053),topiaruss,https://api.github.com/repos/pytorch/pytorch/git/commits/8054dbd6552c3600916ff8bb9d1e811e95a6f13d
0a4f146228883566a506acf46a0ebc1989d47c68,"Codemod imports from libfb to use full path /caffe2

Codemoding imports from libfb.py of the format ""from libfb import X"". This is part of a larger codemod to remove the mapping from libfb/py to libfb, in the interest of enabling static typechecking in fbcode.",shannonzhu,https://api.github.com/repos/pytorch/pytorch/git/commits/0a4f146228883566a506acf46a0ebc1989d47c68
d2453afb1ea249c9b87930f785115d14a8a335f1,"Add SumElementsInt operator

Added a caffe2 math sum operator so that it takes integers (only int32)
Changed the SumFloatIter to SumGenericIter so that it takes >1 types.
Added a sumElementInt operator",roxiehehehe,https://api.github.com/repos/pytorch/pytorch/git/commits/d2453afb1ea249c9b87930f785115d14a8a335f1
8964aab260035f52776e31c8694385a62ec7ef16,"fix docs error in torch.nn.functional.nll_loss (#6060)

According to the code in _torch/nn/functional.py:1399_
(```if target.size()[1:] != input.size()[2:]:```),
if the size of input is (N, C, d_1, d_2, ..., d_K), the size of target should be (N, d_1, d_2, ..., d_K).",sundw2014,https://api.github.com/repos/pytorch/pytorch/git/commits/8964aab260035f52776e31c8694385a62ec7ef16
64e2c03bea4bc6f9f765052eeca0a65fc92ef91b,"Enable TensorDataset to get any number of tensors (#6038)

Keeping compatibility, enable TensorDataset to get any number of tensors.

* Enable TensorDataset to get any number of tensors

* Update dataset.py

Fix syntax error on python 2.7

* Add several test for tensordataset

* Fix whitespaces

* Simplify args

* Update dataset.py",Jaesuny,https://api.github.com/repos/pytorch/pytorch/git/commits/64e2c03bea4bc6f9f765052eeca0a65fc92ef91b
7ffcb2029501a6759e4a2c60aa174230fba54700,small math cleanups in the docs (#6057),samuela,https://api.github.com/repos/pytorch/pytorch/git/commits/7ffcb2029501a6759e4a2c60aa174230fba54700
f8270c0225e19403038aec2d8af2697a2b5326ec,"Enable MKLDNN convolution forward and backward (#6062)

* Enable MKLDNN convolution forward and backward

* minor change

* fix mkldnn build error when building ATen standalone",mingfeima,https://api.github.com/repos/pytorch/pytorch/git/commits/f8270c0225e19403038aec2d8af2697a2b5326ec
3aca8f3b40b4ccc78ac6dcd29b04c9713380dd5f,adding const fxn modifier to Operator::type() (#2484),Swetko,https://api.github.com/repos/pytorch/pytorch/git/commits/3aca8f3b40b4ccc78ac6dcd29b04c9713380dd5f
92a0f7835e7e2f6342fe2fd32a25299ea86aed31,Support returning dictionaries in DataParallel (#6113),mseitzer,https://api.github.com/repos/pytorch/pytorch/git/commits/92a0f7835e7e2f6342fe2fd32a25299ea86aed31
ad34d88959c8e17a4b58cdda87afc4c00c73dde6,added word object to function doc string for clarity (#6204),robert-wagner,https://api.github.com/repos/pytorch/pytorch/git/commits/ad34d88959c8e17a4b58cdda87afc4c00c73dde6
5dcf7078c689f7055ca6837e67ca834cc70d6497,"default build with MKL for desktop (#6266)

* default build with MKL for desktop

default build with MKL for desktop

* remove SET(INTEL_COMPILER_DIR ""/opt/intel"")",BelBES,https://api.github.com/repos/pytorch/pytorch/git/commits/5dcf7078c689f7055ca6837e67ca834cc70d6497
a093ec997f618228ffb73d175ca00192d2bee71d,fix typo (#6329),crcrpar,https://api.github.com/repos/pytorch/pytorch/git/commits/a093ec997f618228ffb73d175ca00192d2bee71d
4cde7c0f0913f551db1a67db2e5c2952c9947dfa,Modify cmake dedent function to make it compatible with Windows. (#6296),harrysummer,https://api.github.com/repos/pytorch/pytorch/git/commits/4cde7c0f0913f551db1a67db2e5c2952c9947dfa
c00ee6da8fe489cf76f47339b0f2c37a810f4e41,"Fix typos (#6348)

* Fix typo

* Fix typo

* Update faq.rst",nzw0301,https://api.github.com/repos/pytorch/pytorch/git/commits/c00ee6da8fe489cf76f47339b0f2c37a810f4e41
119ea3902184907e321bd27f949119d1eac9163d,add cuda headers (#6401),bstriner,https://api.github.com/repos/pytorch/pytorch/git/commits/119ea3902184907e321bd27f949119d1eac9163d
a91c88a34835ad79b80faf0a60f01d5cc0692c41,"Check mappings ONNX -> Caffe2 bear the same argument names (#6317)

* Check mappings ONNX -> Caffe2 bear the same argument names

When adding an extra arg to an input ONNX op, if it's not supported in Caffe2, the exporter would just silently pass it to NetDef and ignore it in the implementation. It's pretty error-prone. Caffe2 also has an OpSchema description and we can enforce that all arguments explicitly appear in schema or listed explicitly in Caffe2.

See also https://github.com/caffe2/caffe2/pull/2478

Add test for C2 argument checking

* Some operators do not log arguments, which prevents argument checks.
Invite users to file an issue to fix the schema.",huitseeker,https://api.github.com/repos/pytorch/pytorch/git/commits/a91c88a34835ad79b80faf0a60f01d5cc0692c41
79c3ebc040c4bac896477030d8af4ac94bc6f440,adds correct precision to test_noncontig_conv_grad (#6440),subcomputes,https://api.github.com/repos/pytorch/pytorch/git/commits/79c3ebc040c4bac896477030d8af4ac94bc6f440
acb7df11a29c9c4de39ac863b0f25fd762cdfad1,"Add torch.randint and torch.randint_like functions (#6136)

Adds randint and randint_like to TensorFactories.cpp",Naman-ntc,https://api.github.com/repos/pytorch/pytorch/git/commits/acb7df11a29c9c4de39ac863b0f25fd762cdfad1
f3a9be0ed5c782a2fbea21844032ab9d7000e3e2,Fix RNN parameters description (#6575),cdiep,https://api.github.com/repos/pytorch/pytorch/git/commits/f3a9be0ed5c782a2fbea21844032ab9d7000e3e2
084e3a755b0ecb8c725041586235b3abc24596ae,fix incorrect path (#6605),narumiruna,https://api.github.com/repos/pytorch/pytorch/git/commits/084e3a755b0ecb8c725041586235b3abc24596ae
fd6d11ae6647363fc27ef1815f40c6ce8db79d40,Fixed text of error message in case of unexpected target size (#6617),soomy,https://api.github.com/repos/pytorch/pytorch/git/commits/fd6d11ae6647363fc27ef1815f40c6ce8db79d40
53d2612b55b9168b55e31804a70d8c874ef0d08a,Fix a typo in the setup.py script (#6632),srib,https://api.github.com/repos/pytorch/pytorch/git/commits/53d2612b55b9168b55e31804a70d8c874ef0d08a
bc6243cb4a977323bbc6770af1a79bc5bb92ce4a,"Explicitly define all caffe2 reducer ops by name (#6513)

* Explicitly define all caffe2 reducer ops by name instead of string concatenating them

Explicitly define all caffe2 reducer ops by name instead of string concatenating them.

* Use recursion to make the equal() function compatible with C++11.

* Trivial change.

* Trivial change.

* Trivial change to force the flaky build system to rebuild.

* Trivial change to force the flaky build system to rebuild.

* Trivial change to force the flaky build system to rebuild.

* Trivial change to force the flaky build system to rebuild.

* Trivial change to force the flaky build system to rebuild.

* Addressed @dzhulgakov's comments.

* Addressed @dzhulgakov's comments.

* Trivial change to force the flaky build system to rebuild.

* Trivial change to force the flaky build system to rebuild.",costin-eseanu,https://api.github.com/repos/pytorch/pytorch/git/commits/bc6243cb4a977323bbc6770af1a79bc5bb92ce4a
f2c99753786714859f7c73c87efcba4ea429da09,Add DistributedDataParallelCPU (#5919),xhzhao,https://api.github.com/repos/pytorch/pytorch/git/commits/f2c99753786714859f7c73c87efcba4ea429da09
639dd0e3244ef27dddb3f82c0fef913fd1d42407,"Fix an error in the tensor docs. (#6658)

The docs incorrectly stated that there was seven CPU tensor types and
eight GPU tensor types, before listing eight types for both CPU and GPU.",ssidorenko,https://api.github.com/repos/pytorch/pytorch/git/commits/639dd0e3244ef27dddb3f82c0fef913fd1d42407
d5f041aa8b482ad5cf0772bde0eea2a0ff07fc3d,Updated documentation for cross entropy loss to include multi-dimensional input shapes (#6638),vellamike,https://api.github.com/repos/pytorch/pytorch/git/commits/d5f041aa8b482ad5cf0772bde0eea2a0ff07fc3d
7fcaf3b49e0538713bb8f8b55baaf9c687dcfd4d,"Update torch.nn.init and torch.nn.utils.clip_grad (#6173)

Introducing two updates.

1. Add param to He initialization scheme in torch.nn.init
Problem solved:
The function calculate_gain can take an argument to specify the type of non-linearity used. However, it wasn't possible to pass this argument directly to the He / Kaiming weight initialization function.

2. Add util to clip gradient value in torch.nn.utils.clip_grad
Problem solved:
DL libraries typically provide users with easy access to functions for clipping the gradients both using the norm and a fixed value. However, the utils clip_grad.py only had a function to clip the gradient norm.

* add param to He initialization scheme in torch.nn.init

* add util to clip gradient value in torch/nn/utils/clip_grad.py

* update doc in torch.nn.utils.clip_grad

* update and add test for torch.nn.utils.clip_grad

* update function signature in torch.nn.utils.clip_grad to match suffix_ convention

* ensure backward compatibility in torch.nn.utils.clip_grad

* remove DeprecationWarning in torch.nn.utils.clip_grad

* extend test and implementation of torch.nn.utils.clip_grad

* update test and implementation torch.nn.utils.clip_grad",tonybeltramelli,https://api.github.com/repos/pytorch/pytorch/git/commits/7fcaf3b49e0538713bb8f8b55baaf9c687dcfd4d
63d42408d01c4509ff57c623b152e4a1c90673a8,"[Caffe2] Detectron fpn support (#6645)

* [Caffe2] Update collect_and_distribe op to fit arbitrary size

* [Caffe2] batch_permutation CPU implementation

* Make requested changes",daquexian,https://api.github.com/repos/pytorch/pytorch/git/commits/63d42408d01c4509ff57c623b152e4a1c90673a8
9c47eb554858edad4534479878ec1f82cef4c95c,"Fixes test_torch.py so that all tests pass on Volta hardware. (#6736)

Issue: ""python3 test_cuda.py"" currently results in a failure when using Volta hardware.

The failure is in test_advancedindex, and is caused by two ""sub-tests."" At line 4651 a series of indices are used to compare PyTorch's and Numpy's indexing behavior. At least two of these indices index the same element of the reference tensor multiple times. These are:

[slice(None), [[2]], [[0, 3], [4, 4]]]
[slice(None), [[0, 1], [1, 0]], [[2, 3], [3, 0]]]

The first index selects the 5th element of the third row twice, and the
second index selects the 4th element of the second row twice.

This causes the test to attempt to update the same index with two distinct values simultaneously. On my machine the Numpy created tensor will always take the ""latter"" of these two values, while the Volta tensor will always take the ""former."" (Not to say this behavior is guaranteed by either framework.)

The fix is to remove these two indices from test_torch.py. This causes all tests to pass.

While updating test_torch.py I also noticed that assert_get_eq(tensor, indexer) had a bug where it was referring to ""reference"" instead of ""tensor."" This bug had no impact on behavior. The fix is to have this function refer to its input tensor, ""tensor,"" instead. All tests still pass after this fix.",mruberry,https://api.github.com/repos/pytorch/pytorch/git/commits/9c47eb554858edad4534479878ec1f82cef4c95c
e44f901b55873ebb6b1b0d3bab30fd89d487b71c,"added functionality for state_dict/load_state_dict for lr_scheduler ( Fixes: #3026 ) (#6342)

* added functionality for state_dict/load_state_dict for lr_scheduler

* fixed linting issues/removed unused import

* refactor lr_scheduler state_dicts/state_dict holds everything __dict__ but optimizer

* changed documentation in lr_scheduler

* Update lr_scheduler.py",ArmenAg,https://api.github.com/repos/pytorch/pytorch/git/commits/e44f901b55873ebb6b1b0d3bab30fd89d487b71c
f6da2fd944280b50c452e4909990e07d0dfd9c64,"Make the variable closer to usage (#6752)

`chain` is used for the loop below.",103yiran,https://api.github.com/repos/pytorch/pytorch/git/commits/f6da2fd944280b50c452e4909990e07d0dfd9c64
7e1c5ca6d5b1aa97ffe1f35acf5d4a2ece6c0d47,Add missing #include for CAFFE2_MODULE macro. (#6790),xkszltl,https://api.github.com/repos/pytorch/pytorch/git/commits/7e1c5ca6d5b1aa97ffe1f35acf5d4a2ece6c0d47
26ddefbda14d9b11d1c4f6be181d878a3deae418,"[feature request] [Caffe2] Enable MKLDNN support for inference (#6699)

* Add operators based-on IDEEP interfaces

Signed-off-by: Gu, Jinghui <jinghui.gu@intel.com>

* Enable IDEEP as a caffe2 device

Signed-off-by: Gu, Jinghui <jinghui.gu@intel.com>

* Add test cases for IDEEP ops

Signed-off-by: Gu, Jinghui <jinghui.gu@intel.com>

* Add IDEEP as a caffe2 submodule

Signed-off-by: Gu, Jinghui <jinghui.gu@intel.com>

* Skip test cases if no IDEEP support

Signed-off-by: Gu, Jinghui <jinghui.gu@intel.com>

* Correct cmake options for IDEEP

Signed-off-by: Gu, Jinghui <jinghui.gu@intel.com>

* Add dependences on ideep libraries

Signed-off-by: Gu, Jinghui <jinghui.gu@intel.com>

* Fix issues in IDEEP conv ops and etc.

Signed-off-by: Gu, Jinghui <jinghui.gu@intel.com>

* Move ideep from caffe2/ideep to caffe2/contrib/ideep

Signed-off-by: Gu Jinghui <jinghui.gu@intel.com>

* Update IDEEP to fix cmake issue

Signed-off-by: Gu, Jinghui <jinghui.gu@intel.com>

* Fix cmake issue caused by USE_MKL option

Signed-off-by: Gu, Jinghui <jinghui.gu@intel.com>

* Correct comments in MKL cmake file

Signed-off-by: Gu, Jinghui <jinghui.gu@intel.com>",gujinghui,https://api.github.com/repos/pytorch/pytorch/git/commits/26ddefbda14d9b11d1c4f6be181d878a3deae418
d9bde84b84522ae24610ed43a679697fe86b8786,"Add threshold for ops using openmp macro (#5584)

* add threshold for ops using omp macro

* modify interface for ops using omp macro

* modify some thresholds

* implement C macros with optional parameters to avoid duplicating definitions for all pointwise operations

* add a parameter of LAB_IMPLEMENT_BASIC_FUNCTION for vectorizing

* modify the comment

* Revert ""add a parameter of LAB_IMPLEMENT_BASIC_FUNCTION for vectorizing""
Modify macro LAB_IMPLEMENT_VECTORIZED_FUNCTION to enable optional parameters

This reverts commit 8ef783a0cc67b653c435e64a3beb6866a6b4216d.

Conflicts:
	aten/src/TH/generic/THTensorMath.c

* fix build error on windows

* retrigger the test",zy97140,https://api.github.com/repos/pytorch/pytorch/git/commits/d9bde84b84522ae24610ed43a679697fe86b8786
2f311be90b80c07d06de4eb9fa33e6bd14377bb8,add default value to ConstantFill doc (#6923),fumihwh,https://api.github.com/repos/pytorch/pytorch/git/commits/2f311be90b80c07d06de4eb9fa33e6bd14377bb8
e7babb1890b1658886835ff4627d8b00aa84bae1,"[aten] only lookup CuDNN if compiling with CUDA (#6905)

ATen can be configured to compile without CUDA support by passing
-DNO_CUDA=0 to cmake.  However, cmake will look for CuDNN independently
of that flag and may eventually find it.  In cases were compilation
without CUDA support was requested on system with CUDA installed, this
will result in linking errors while building some tests that rely only
on CuDNN being found.

Do not look for CuDNN if -DNO_CUDA=1 was provided in the cmake call
since it does not make sense to compile with CuDNN if CUDA support was
disabled.",ftynse,https://api.github.com/repos/pytorch/pytorch/git/commits/e7babb1890b1658886835ff4627d8b00aa84bae1
6ebcb4606f079b9152cb242b36e03b8eddcb6173,fix typo in the LSTMCell math definition (#6951),rolczynski,https://api.github.com/repos/pytorch/pytorch/git/commits/6ebcb4606f079b9152cb242b36e03b8eddcb6173
984516bdc4bcc433768474ba4bddf1271a87502f,typo corrected: is -> if (#6980),bombs-kim,https://api.github.com/repos/pytorch/pytorch/git/commits/984516bdc4bcc433768474ba4bddf1271a87502f
ee00a8049a21a27f126ac820859e0643c70fcc8b,"Add max pooling support to EmbeddingBag (#5725)

* Add max mode support to EmbeddingBag

* Lint fix

* Fix compilation issue on other platforms

* Rebase + don't waste memory when not in max mode

* Oops, missed a spot

* Fix whitespace from merge

* less precision

* Lower precision to avoid spurious failures

* Minor typo

* Switch to size()",Lalaland,https://api.github.com/repos/pytorch/pytorch/git/commits/ee00a8049a21a27f126ac820859e0643c70fcc8b
0b0279981dcc0c600a85b94c73b50c04ba05ec71,"Fix example for new_zeros in documentation (#7128)

Fix for Issue #7088",wongjoel,https://api.github.com/repos/pytorch/pytorch/git/commits/0b0279981dcc0c600a85b94c73b50c04ba05ec71
197412fa8f2d3c280afa06a07e2025e77a8efc54,Fix typo in comment (#7183),takp,https://api.github.com/repos/pytorch/pytorch/git/commits/197412fa8f2d3c280afa06a07e2025e77a8efc54
4ab6ea5b1fdb1943bca8cdcb5f4d758a746e9565,Add unbuffered flag to distributed node launcher (#7226),andfoy,https://api.github.com/repos/pytorch/pytorch/git/commits/4ab6ea5b1fdb1943bca8cdcb5f4d758a746e9565
c96f2624a280ae3e2d9195c35150f6aec85f6a02,"Speedup sparse init (#6899)

* Sparse initialization speedup

* +empty line

* simplify indexing

* Can't reproduce locally...

* Can't reproduce locally...+

* Can't reproduce locally...+

* Fix test, cleanup",mttk,https://api.github.com/repos/pytorch/pytorch/git/commits/c96f2624a280ae3e2d9195c35150f6aec85f6a02
a0c1e5faea68950790498ef8e0c26b469a993ae2,Change the error message in pad_sequence to be more user-friendly (#7283),superbobry,https://api.github.com/repos/pytorch/pytorch/git/commits/a0c1e5faea68950790498ef8e0c26b469a993ae2
f06fcc6efad81bb2619820ada21744d3e6e2f2c4,"Fix bug that introduced in pull #3280 (#7292)

Apparently get() is a function of requests, not a module (not sure if in
the past get() used to be a module). Therefore, the syntax in #3280 will
alway fail with ImportError, and requests lib will never be used (kind
of defeat the purpose of that pull request).
Also, if requests lib is used, should add stream=True parameter,
otherwise requests.get() will load the whole response into memory.",jfan-uber,https://api.github.com/repos/pytorch/pytorch/git/commits/f06fcc6efad81bb2619820ada21744d3e6e2f2c4
56daed0a85928e9794c48804f9c4cf34a11a3267,copy paste documentation error fixed in Softmin (#7324),acheshkov,https://api.github.com/repos/pytorch/pytorch/git/commits/56daed0a85928e9794c48804f9c4cf34a11a3267
bebccc0c6d472d82072cefc2747ca65b63acf7ea,Improve math formula rendering in Poisson Distribution docs. (#7340),fracting,https://api.github.com/repos/pytorch/pytorch/git/commits/bebccc0c6d472d82072cefc2747ca65b63acf7ea
f1e38725bfe73a29335d921786250bc71b15f7b6,"add `to` method for PackedSequence (#7319)

* ENH: add to method for PackedSequence

* ENH: return self if possible

* TST: remove extra data

* DOC: add more explanation

* TST: remove extra data

* DOC: minor fix",facaiy,https://api.github.com/repos/pytorch/pytorch/git/commits/f1e38725bfe73a29335d921786250bc71b15f7b6
537cb10525d9ccc03c3d0157b4d3071f584914cb,improve DataParallel/DistributedDataParallel docs (#7407),acgtyrant,https://api.github.com/repos/pytorch/pytorch/git/commits/537cb10525d9ccc03c3d0157b4d3071f584914cb
f43e06712829ce24552d73e2442ef31385559729,Make optimizer not complain about parameters with requires_grad=False (#7419),domaala,https://api.github.com/repos/pytorch/pytorch/git/commits/f43e06712829ce24552d73e2442ef31385559729
97c5c0b034b55cf050c1653ff31a78d1417a4914,add python library linking on Windows (#7157),azias,https://api.github.com/repos/pytorch/pytorch/git/commits/97c5c0b034b55cf050c1653ff31a78d1417a4914
a257bd19a2f932c91f2d8bdeeb497495d5027018,added state_dict/load_state_dict for ReduceLROnPlateau (#7201),kahne,https://api.github.com/repos/pytorch/pytorch/git/commits/a257bd19a2f932c91f2d8bdeeb497495d5027018
ea98256e96d8e0baa807bd228be5dd1a3fde14e8,Buf check_unique fix for jit (#7468),vakker,https://api.github.com/repos/pytorch/pytorch/git/commits/ea98256e96d8e0baa807bd228be5dd1a3fde14e8
20041e27044a23d92842ee7ead5faa26583b2f1e,"better cache for nccl resourse (#6970)

allow more than 1 device list to be stored",FDecaYed,https://api.github.com/repos/pytorch/pytorch/git/commits/20041e27044a23d92842ee7ead5faa26583b2f1e
9789602814388c19e1afb6eede3ebb5a312078ed,Fix excess ']' in nn.utils.rnn.pack_sequence (#7475),phizaz,https://api.github.com/repos/pytorch/pytorch/git/commits/9789602814388c19e1afb6eede3ebb5a312078ed
b6adf6871c5bdca29bf9ae1ac3dbedfb869caabd,EmbeddingBag to handle empty bags in all modes (#7389),danielsimig,https://api.github.com/repos/pytorch/pytorch/git/commits/b6adf6871c5bdca29bf9ae1ac3dbedfb869caabd
857e3f4a5efcf108f95aa6c5bd735c52e016a202,Throw error in tensor constructor when numpy strides mismatch (#7440),jrwalsh1,https://api.github.com/repos/pytorch/pytorch/git/commits/857e3f4a5efcf108f95aa6c5bd735c52e016a202
5f96a2d26a7b8ec65360f8cf9159484276ac0aa7,"Add sparse gradient option to pretrained embedding (#7492)

* Add sparse gradient option to pretrained embedding

* Add sparse gradient option to pretrained embedding

* Trailing white space",borguz,https://api.github.com/repos/pytorch/pytorch/git/commits/5f96a2d26a7b8ec65360f8cf9159484276ac0aa7
dc0faab18dd5296b66dfb00776253062def87fdf,"Add zeros_ and ones_ init + tests (#7488)

* Add zeros_ and ones_ init + tests

* Dedup tests

* Remove all occurences of as_variable",karandwivedi42,https://api.github.com/repos/pytorch/pytorch/git/commits/dc0faab18dd5296b66dfb00776253062def87fdf
cf9751207e8b97f5e844aacdaea494a2ba997caf,"Allow building Caffe2 with ATen support (Addresses #7249) (#7297)

* Addresses Issue #7249, where Caffe2 cannot be built with ATen support

* Fixed indentation",MahdiNazemi,https://api.github.com/repos/pytorch/pytorch/git/commits/cf9751207e8b97f5e844aacdaea494a2ba997caf
1f0800056242e2e0f099056a69ebabc4184b16bf,return value of LSTM example fixed. (#7534),domschl,https://api.github.com/repos/pytorch/pytorch/git/commits/1f0800056242e2e0f099056a69ebabc4184b16bf
9213336c73098bf017a016154c67b1c50443df4e,fix cmake USE_ASAN (#7608),ffk0716,https://api.github.com/repos/pytorch/pytorch/git/commits/9213336c73098bf017a016154c67b1c50443df4e
c5b9a36f1e999a28872875b6b060c933cacbcd18,"Make return uniform in lbfgs step (#7586)

* Make return uniform in lbfgs step

This ensures that we are returning results of the same type
in LBFGS step.

* Adding test case to exercise different exit points

Sets the tolerance_grad to negative infinity and positive
infinity to deterministically excercise the early exit branch

* Fixing lint error",lematt1991,https://api.github.com/repos/pytorch/pytorch/git/commits/c5b9a36f1e999a28872875b6b060c933cacbcd18
b4d5e67e5ffcfcd6de99e0762f963554b9efa524,"Add asin, acos, tan, atan operators (#7600)",highker,https://api.github.com/repos/pytorch/pytorch/git/commits/b4d5e67e5ffcfcd6de99e0762f963554b9efa524
84730aa6595c462c8151dec4153e8eed01828286,support <= and >= (#7633),zasdfgbnm,https://api.github.com/repos/pytorch/pytorch/git/commits/84730aa6595c462c8151dec4153e8eed01828286
32b23a4bfc64a4b824a89a6491590576622d15ed,"Throw error on tensor creation when sequence shape cannot be determined (#7583)

* first commit

* unit test

* minor style edits",sethah,https://api.github.com/repos/pytorch/pytorch/git/commits/32b23a4bfc64a4b824a89a6491590576622d15ed
ec71c689fce7c3f26a5ec5bce954eb528ee0b37f,"[JIT][script] Add matmul(@), pow(**) operator (#7648)

* add matmul(@), pow(**) operator

* fix bug(matmul not in py2) in @ operator

* fix bugs

* add get_fn help func to remove duplication in test_jit",ChunliF,https://api.github.com/repos/pytorch/pytorch/git/commits/ec71c689fce7c3f26a5ec5bce954eb528ee0b37f
5ee5537b9800bb312575247873c0e5139b30bfe4,Fix typo in document (#7725),qixiuai,https://api.github.com/repos/pytorch/pytorch/git/commits/5ee5537b9800bb312575247873c0e5139b30bfe4
42134ee799c055791a25cfa6f33f5b2d091d1e54,"Allow empty storage for the 'Edge' class. (#7595)

This commit:
- Converts edge storage to an optional type.
- Adds a new test in tarjans_test.
- Refactors related bits in other files.",yyetim,https://api.github.com/repos/pytorch/pytorch/git/commits/42134ee799c055791a25cfa6f33f5b2d091d1e54
4352eab3678274c6811c7622178ed4963538282e,"Call grad_mode.py context managers as decorators (#7737)

* call grad_mode.py context managers as decorators

* flake fixes

* switch to using context manager in wrapper

* fix set_grad_enabled test

* removed dumb github UI whitespace

* revert set_grad_enabled to normal, update tests",jvmncs,https://api.github.com/repos/pytorch/pytorch/git/commits/4352eab3678274c6811c7622178ed4963538282e
2ebcf4bb37739733e76b754284cf8b2ffcba1c30,"[Caffe2] Enabling AMD GPU Backend for Caffe2 (#7566)

* Add hip support for caffe2 core

* Add MIOPEN header/wrapper to caffe2 core

* Add HIP device into caffe2 PB

* top level makefile change for rocm/hip

* makefile scaffolding for AMD/RocM/HIP

* Makefile scafodding for AMD/RocM/HIP; add makefile/utility for HIP files

* caffe2 PB update for AMD/ROCM HIP device

* Add AMD/RocM/Thrust dependency

* HIP threadpool update

* Fix makefile macro

* makefile fix: duplicate test/binary name

* makefile clean-up

* makefile clean-up

* add HIP operator registry

* add utilities for hip device

* Add USE_HIP to config summary

* makefile fix for BUILD_TEST

* merge latest

* Fix indentation

* code clean-up

* Guard builds without HIP and use the same cmake script as PyTorch to find HIP

* Setup rocm environment variables in build.sh (ideally should be done in the docker images)

* setup locale

* set HIP_PLATFORM

* Revert ""set HIP_PLATFORM""

This reverts commit 8ec58db2b390c9259220c49fa34cd403568300ad.

* continue the build script environment variables mess

* HCC_AMDGPU_TARGET

* Cleanup the mess, has been fixed in the lastest docker images

* Assign protobuf field hip_gpu_id a new field number for backward compatibility

* change name to avoid conflict

* Fix duplicated thread pool flag

* Refactor cmake files to not add hip includes and libs globally

* Fix the wrong usage of environment variables detection in cmake

* Add MIOPEN CNN operators

* Revert ""Add MIOPEN CNN operators""

This reverts commit 6e89ad4385b5b8967a7854c4adda52c012cee42a.",petrex,https://api.github.com/repos/pytorch/pytorch/git/commits/2ebcf4bb37739733e76b754284cf8b2ffcba1c30
215fe057ea0c178d5cbfe2e815a251351c7d3263,"No Default argument to max_unpool functions (Fixes #7327) (#7388)

* Fix for Issue #7327

* Added testcase for max_unpool",vedaanta,https://api.github.com/repos/pytorch/pytorch/git/commits/215fe057ea0c178d5cbfe2e815a251351c7d3263
a8625e016ae7cd112358bfd5acb20e0fb3243cb1,Spelling fix in MultivariateNormal docstring (#7915),aryamccarthy,https://api.github.com/repos/pytorch/pytorch/git/commits/a8625e016ae7cd112358bfd5acb20e0fb3243cb1
c72c08315126da556d8593843f4769185a075bcd,Moved condition for dilated grouped convolutions to CUDNN convolution implementation (#7465),AnnaPetrovicheva,https://api.github.com/repos/pytorch/pytorch/git/commits/c72c08315126da556d8593843f4769185a075bcd
38dbe6e6057a7186afcdb5e5aa66b29255cdeffb,"Updates to caffe2 operator documentation (#7917)

* Significant updates to the operator docs in prep for merge",inkawhich,https://api.github.com/repos/pytorch/pytorch/git/commits/38dbe6e6057a7186afcdb5e5aa66b29255cdeffb
f0c09203b022d3c74f2bfc3b7e7d60289ab44615,[caffe2] YellowFin parameter update GPU code fix. (#6993),edubois,https://api.github.com/repos/pytorch/pytorch/git/commits/f0c09203b022d3c74f2bfc3b7e7d60289ab44615
fd3048708931d47fca3b559a9c1bea86073282e6,"Fix a couple of typos (#7998)

* Fix typo

* Fix typo

* Fix typo

* Fix typo",dmitriy-serdyuk,https://api.github.com/repos/pytorch/pytorch/git/commits/fd3048708931d47fca3b559a9c1bea86073282e6
8f421159fdede979a8f00c832e32f9c2cad36d75,"Fix profiler crash when no events register (#8034)

* Fix profiler crash when no events register

When trying to profile, attempting to print the event table throws a vague error because the event list is empty:

....
max_name_length = max(len(evt.key) for evt in events)
ValueError: max() arg is an empty sequence

This change fixes the error by returning an empty string.

* Update profiler.py",rbrigden,https://api.github.com/repos/pytorch/pytorch/git/commits/8f421159fdede979a8f00c832e32f9c2cad36d75
afa75fa6b208f60bbaecf68e36961316f1c4c62b,"Remove NO_PYTHON macros from Exceptions.h/cpp (#8007)

Removes cases where NO_PYTHON was unnecessary in Exception.h/cpp",zrphercule,https://api.github.com/repos/pytorch/pytorch/git/commits/afa75fa6b208f60bbaecf68e36961316f1c4c62b
68948306bc18597adb63f62a12f4f146546c0cdf,"Support to run ONNX Upsample operator (mode=nearest) in Caffe2 (#8037)

* Added support to run ONNX Upsample operator (mode=nearest) in Caffe2

* adding error checks to upsample

* adding error checks to upsample

* adding error checks to upsample

* changing to np.isclose

* Revert onnx submodule update

* still fixing",varunjain99,https://api.github.com/repos/pytorch/pytorch/git/commits/68948306bc18597adb63f62a12f4f146546c0cdf
f5cd479b59d9db9164cc1d13ba6605a64c11a089,"fix type mismatch while call torch._C._cuda_setDevice (#8065)

* fix type mismatch while call torch._C._cuda_setDevice

* fix type mismatch in scatter

* fix type mismatch in scatter

* fix type mismatch while call torch._C._cuda_setDevice

* fix type mismatch while call torch._C._cuda_setDevice

* fix type mismatch while call torch._C._cuda_setDevice",HisiFish,https://api.github.com/repos/pytorch/pytorch/git/commits/f5cd479b59d9db9164cc1d13ba6605a64c11a089
ee0b75a3d2b60cf82ccc0acb0fb2b1e581e1bfee,"docs: Add warning to torch.repeat() (#8116)

* docs: Add warning to torch.repeat()

closes #7993

* docs: Add links for numpy functions

* docs: Break the too long line",Ir1d,https://api.github.com/repos/pytorch/pytorch/git/commits/ee0b75a3d2b60cf82ccc0acb0fb2b1e581e1bfee
ffde23d45eeefaf4a83b353e59dc853c43f4d5fe,use the correct datatype format (#8144),seravee,https://api.github.com/repos/pytorch/pytorch/git/commits/ffde23d45eeefaf4a83b353e59dc853c43f4d5fe
bae82f726d35969474037c35c3a9ad0b3a338ee3,fix caffe2 docker build (#7411),qigtang,https://api.github.com/repos/pytorch/pytorch/git/commits/bae82f726d35969474037c35c3a9ad0b3a338ee3
4d025a6a54e785f357d502c525e8a808fbc78d17,"add wipe_cache option (#8204)

as title",lly-zero-one,https://api.github.com/repos/pytorch/pytorch/git/commits/4d025a6a54e785f357d502c525e8a808fbc78d17
d2271dcee370f1827bd1a60b84a0c2c5fee23c1b,Fix: gradcheck forced float32 (#8230),bhushan23,https://api.github.com/repos/pytorch/pytorch/git/commits/d2271dcee370f1827bd1a60b84a0c2c5fee23c1b
ce122cc2d34135f6cf7fa2fc02335346722118c1,"Relax CUDA_HOME detection logic, to build when libraries are found. (#8244)

Log when no cuda runtime is found, but CUDA is found",dashesy,https://api.github.com/repos/pytorch/pytorch/git/commits/ce122cc2d34135f6cf7fa2fc02335346722118c1
4c2a1a1a64db08f24219ce5507ae93c0de926173,"Added backward function for kl_div target (#7839)

* added backward fn for target

* added module test for kl_div target, and assuming targets are probabilities",weiyangfb,https://api.github.com/repos/pytorch/pytorch/git/commits/4c2a1a1a64db08f24219ce5507ae93c0de926173
96876d9e7ef6baf9d11541454b5f4d22b092de77,"Name the thread pools (#8137)

Caffe thread pools currently inherit the thread names from the thread that starts them, which can be misleading. Give them an explicit name instead.",ot,https://api.github.com/repos/pytorch/pytorch/git/commits/96876d9e7ef6baf9d11541454b5f4d22b092de77
922adf8d09524db8697aac657a4707076155b7c0,"Skip calling ncclCommDestroy in destructor (#8352)

There is a bug in NCCL that causing seg faults while calling ncclCommDestroy() in the destructor during program exit. According to Nvidia, ""Whether the NCCL destructor will be called before or after the CUDA runtime destructor is undefined, which can lead to crashes.""

For the immediate workaround, skip calling ncclCommDestroy ihe NCCL destructor. This is UGLY and we'll follow up with Nvidia to solve this ASAP.",xw285cornell,https://api.github.com/repos/pytorch/pytorch/git/commits/922adf8d09524db8697aac657a4707076155b7c0
c6db1bc952f565752380cf550cb3ee8d4ec974df,"Add gt lt ge le to the supported operators list (#8375)

Add gt lt ge le to the supported operators list",yueyericardo,https://api.github.com/repos/pytorch/pytorch/git/commits/c6db1bc952f565752380cf550cb3ee8d4ec974df
db14f3f33c8ddc9c910ca2188f8787dd81b97b52,"More efficient kernels that avoid deprecated shuffles in Embedding and LookupTable (#8400)

* More efficient kernel that avoids deprecated shuffles in Embedding.cu and THCUNN/LookupTable.cu

* Using WARP_BALLOT from THCDeviceUtils.cuh, also changing WARP_BALLOT to return unsigned",mcarilli,https://api.github.com/repos/pytorch/pytorch/git/commits/db14f3f33c8ddc9c910ca2188f8787dd81b97b52
21609e0fd061fbf1a2161d818ee3d51928598e66,"``bincount`` feature implementation (#6688)

* Implement CPU bincount feature support

* Incorporate feedback on renaming to SummaryOps file and other nits

* bincount gpu implementation

* refactor cuda code and incorporate nits

* doc fix

* cuda bincount - cast weights to double if integral type

* fix: signed unsigned comparison error

* fix: ssize_t error

* refactor

* make template typenames readable and other nist

* make compatible with v0.5

* incorporate comments

* update test cases to ensure CUDA code coverage",chintak,https://api.github.com/repos/pytorch/pytorch/git/commits/21609e0fd061fbf1a2161d818ee3d51928598e66
fa277e6785aaaab2d1c4a1b61ecc4e92e8331885,"[IDEEP] [fix bug] Fix bug in ideep SkipOutputCopy strategy (#8372)

* fix a bug for SkipIndices

* IDEEP bug, revise the output to CPUTensor in SkipOutputCopy strategy

* [IDEEP] Add IDEEP fallbacks for Style-Transfer ops",wuhuikx,https://api.github.com/repos/pytorch/pytorch/git/commits/fa277e6785aaaab2d1c4a1b61ecc4e92e8331885
b10c94b5072f288ca915adb24fe1545ca64a773d,"Update operator documentation with markdown descriptions and interfaces (#8085)

* Update operator documentation with markdown descriptions and interfaces

* Added rest of updated operator documentation to source files

* Commiting local changes for rebase

* fixed bracket typo in sqrt_op.cc file

* Added updated markdown documentation to remaining completed ops",MatthewInkawhich,https://api.github.com/repos/pytorch/pytorch/git/commits/b10c94b5072f288ca915adb24fe1545ca64a773d
b492d103ee7611525c89e0c115a13ebe31fa0be4,fix formatting in :math: in fold docstring (#8696),JackLangerman,https://api.github.com/repos/pytorch/pytorch/git/commits/b492d103ee7611525c89e0c115a13ebe31fa0be4
73ce21a313aef84aa9bdf00ca6eea567be4b82d4,"Create captured inputs recursively for loop to resolve loop-carried dependencies across nested blocks (#8345)

* enable captured inputs for if Stmt to fix the carried deps bug in nested
blocks

* postpone captured inputs deletion and add new test case

* recursively generate captured values for nested loops

* check asSimple when recursively create captured input",wanchaol,https://api.github.com/repos/pytorch/pytorch/git/commits/73ce21a313aef84aa9bdf00ca6eea567be4b82d4
bd95f8f948cf727505e1ad7b6433533850b263fc,"Resolve name conflict of ContextManager (#7244)

* Resolve conflicting name, ContextManager

Concept name `Context Manager` is taken by Python. See https://docs.python.org/3.6/reference/datamodel.html#with-statement-context-managers

It says,
A context manager is an object that defines the runtime context to be established when executing a with statement. The context manager handles the entry into, and the exit from, the desired runtime context for the execution of the block of code.

The `ContextManager` here is more like a registry. 
And there is a C++ registry in caffe2 codebase `caffe2/caffe2/core/registry.h`.
There is also a Caffe2DBRegistry, declared by calling `CAFFE_DECLARE_REGISTRY(Caffe2DBRegistry, DB, const string&, Mode);` in `caffe2/caffe2/core/db.h`.

I think we can follow the concept name `Registry`, calling it `ContextRegistry`.

* Make Classes and Functions internal to this module start with ""_""

Make Classes and Functions internal to this module start with ""_""

* Update context.py

* Update context.py",xush6528,https://api.github.com/repos/pytorch/pytorch/git/commits/bd95f8f948cf727505e1ad7b6433533850b263fc
7fbd57091dc39e86b2996ea132bbdcf7ae4997ce,Doc: specify batch_first is True by default in RNN (#8807),jongwook,https://api.github.com/repos/pytorch/pytorch/git/commits/7fbd57091dc39e86b2996ea132bbdcf7ae4997ce
49a3e49627c71a4abe551c832e4720e8b21704fe,"Fixes #8508. Upcasted loc to 1-d if a scalar loc is provided to MultivariateNormal (#8543)

* Fixes #8508 Broadcasted loc to 1-d if a scalar loc is provided to MultivariateNormal.

* move to non-inplace",praveen-palanisamy,https://api.github.com/repos/pytorch/pytorch/git/commits/49a3e49627c71a4abe551c832e4720e8b21704fe
6e28d4d364246f35f5bdbd18bd8e12edc28538aa,"Add pos_weight argument to nn.BCEWithLogitsLoss (#5660) (#6856)

* Add pos_weight argument to nn.BCEWithLogitsLoss and F.binary_cross_entropy_with_logits (#5660)
- Add an option to control precision/recall in imbalanced datasets
- Add tests (but new_criterion_tests)

* Move pos_weight to the end of args list in the documentation.

`pos_weight` was moved to the end because it is the last argument in both
`nn.BCEWithLogitsLoss` and `binary_cross_entropy_with_logits`",velikodniy,https://api.github.com/repos/pytorch/pytorch/git/commits/6e28d4d364246f35f5bdbd18bd8e12edc28538aa
22ba8726da84ac246c945c095953c505a6091fab,"Mention MPICH_MAX_THREAD_SAFETY=multiple. (#8580)

Currently, this is a common step to enable level 3 support on MPICH based systems.",rainwoodman,https://api.github.com/repos/pytorch/pytorch/git/commits/22ba8726da84ac246c945c095953c505a6091fab
f52c2ca1c6bc60fc3c3c6720ad4f6994c1f41a86,"net_async tracing use enable_profile arg from NetDef (#8927)

Summary:
Closes https://github.com/pytorch/pytorch/pull/8927

Closes https://github.com/pytorch/pytorch/pull/8855

- Add parameter `enable_tracing` to the Arg field of NetDef. `net_async_tracing` will only enable Tracer for Net instances that have this field set (unless the command line argument also include the net name).
- Append a unique id to the json profiling result file because there could be multiple instances of the same net running.
- Dump json profling file regularly instead of just when the Tracer object is destroyed

Reviewed By: ilia-cher

Differential Revision: D8372378

fbshipit-source-id: 8adc9d59f48b67456beed2e3a88235c298fdfd01",duc0,https://api.github.com/repos/pytorch/pytorch/git/commits/f52c2ca1c6bc60fc3c3c6720ad4f6994c1f41a86
8d384600b8d11a4f4833398a592c3dd8045ed55c,"Add ShapeTypeInference for Conditional operator (#8924)

Summary:
Closes https://github.com/pytorch/pytorch/pull/8924

Closes https://github.com/pytorch/pytorch/pull/8915

As desc

Reviewed By: ezyang

Differential Revision: D8649582

fbshipit-source-id: d08a456b9861dd7edd19ed18e16d4778b4240c90",snie2012,https://api.github.com/repos/pytorch/pytorch/git/commits/8d384600b8d11a4f4833398a592c3dd8045ed55c
512c49e831bd96861f6db4077a4e292e3f421bc9,"Correct link flag order for GNU ld in utils.cpp_extension.load (#9021)

Summary:
Any flags linking libraries only take effect on inputs preceding them,
so we have to call `$cxx $in $ldflags -o $out` instead of the other way
around.

This was probably not detected so far since the torch libraries are
already loaded when loading JIT-compiled extensions, so this only has an
effect on third-party libraries.

This also matches our behavior on windows.
Closes https://github.com/pytorch/pytorch/pull/9021

Reviewed By: soumith

Differential Revision: D8694049

Pulled By: ezyang

fbshipit-source-id: e35745fc3b89bf39c14f07ce90d6bd18e6a3d7cc",xqms,https://api.github.com/repos/pytorch/pytorch/git/commits/512c49e831bd96861f6db4077a4e292e3f421bc9
b479494ed4e2821e47722331fd11c5f26f36671c,"loss plugin: Fix indexing into a scalar (#9143)

Summary:
The loss plugin was using the old-style loss[0] access, which in PyTorch 0.4 and
later is an attempt to index into a scalar, generating a warning.
Replaced that with loss.item().

This fixes
https://github.com/pytorch/pytorch/issues/9142
Closes https://github.com/pytorch/pytorch/pull/9143

Differential Revision: D8726403

Pulled By: ezyang

fbshipit-source-id: 6c496b140a74d22c8423f511db901b18615fd6fa",rfejgin,https://api.github.com/repos/pytorch/pytorch/git/commits/b479494ed4e2821e47722331fd11c5f26f36671c
e3dbdb2a17827d011c0d407eb0c6683677c4c895,"Fix the comments: code and comments dimensions mis-match (#9070)

Summary:
This will resolve the code and comments mis-match issue.
Closes https://github.com/pytorch/pytorch/pull/9070

Differential Revision: D8712261

Pulled By: ezyang

fbshipit-source-id: a8a7d8af890a41ec246e11c2a62b0bde297be9c1",nkhuyu,https://api.github.com/repos/pytorch/pytorch/git/commits/e3dbdb2a17827d011c0d407eb0c6683677c4c895
eadc5071e8a6aa71a24348a959a2f3ae7d0220a1,"Use torch.save in _StorageBase.__reduce__ (#9184)

Summary:
Previously this used the ``.toliist`` method, which converted the
storage object into a list of Python objects, and then sent those to
pickle.  For storage objects of non-trivial size, this was very slow.

Now we reuse the logic of the ``torch.save`` function to efficiently
turn the Storage object into bytes, and send those instead.  This
reduces the semantic information (it's harder to interpret the bytes)
but should be orders of magnitude more efficient when serializing data
with the pickle protocol or with copy

For future work it would be nice to develop a mechanism to get a buffer
of bytes out of a Storage object, and use that alongside the current
``from_buffer`` method.

See #9168 for context
Closes https://github.com/pytorch/pytorch/pull/9184

Differential Revision: D8747794

Pulled By: soumith

fbshipit-source-id: ac598e660c043788ed1ffab3d0303812886edf79",mrocklin,https://api.github.com/repos/pytorch/pytorch/git/commits/eadc5071e8a6aa71a24348a959a2f3ae7d0220a1
4e5369349f1f535db77564ca6c38c52af19c960b,"Add FTRL Optimzier with Group Lasso regularizer (#9074)

Summary:
Closes https://github.com/pytorch/pytorch/pull/9074

Implement an optimzier based on FTRL Optimzier which support Group
Lasso regularizer.

The relevant paper list for this optimizer:
1. About the FTRL Optimizer: https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf,
2. About the group lasso regularizer solver: http://www.cse.cuhk.edu.hk/~king/PUB/ICML2010-Yang-473.pdf

Differential Revision: D8623146

fbshipit-source-id: 40e08aa6319d1ad7aa95e8716e3de83b9cfb8452",xiuyanni,https://api.github.com/repos/pytorch/pytorch/git/commits/4e5369349f1f535db77564ca6c38c52af19c960b
66dc97e51ca872e01d6430363770256c76f94361,"#8714 Improve Error Messages for module re-assignment (#9212)

Summary:
Here's an improved error message.  Let me know if this change makes the errors a little clearer.
Closes https://github.com/pytorch/pytorch/pull/9212

Reviewed By: soumith

Differential Revision: D8752896

Pulled By: jramseyer

fbshipit-source-id: d2bd8462c3ddf14acd3de56a4c1aeb75a9bc4067",jramseyer,https://api.github.com/repos/pytorch/pytorch/git/commits/66dc97e51ca872e01d6430363770256c76f94361
b4c66459c5d59a38cbce2f13396215c2d94c5551,"Add pyHIPIFY scripts needed for ROCm transpilation to PyTorch (#8812)

Summary:
As discussed in call, this will allow us to keep this integral part of the effort to run PyTorch on ROCm in sync with the main code.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/8812

Reviewed By: ezyang

Differential Revision: D8796245

Pulled By: bddppq

fbshipit-source-id: 8e12c2acf6a7e0740f31b21e50be74e10ed8b12c",iotamudelta,https://api.github.com/repos/pytorch/pytorch/git/commits/b4c66459c5d59a38cbce2f13396215c2d94c5551
fb9f9c9ba2bf58791943e6c899340c1260bcd313,"Implement Sinh and Cosh (#9213)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9213

Closes https://github.com/pytorch/pytorch/pull/9213

Added hyperbolic trig functions Sinh and Cosh

Reviewed By: BIT-silence

Differential Revision: D8752566

fbshipit-source-id: 5a58336a5153ec804404b9ac7b10b5662ede3cb7",hl475,https://api.github.com/repos/pytorch/pytorch/git/commits/fb9f9c9ba2bf58791943e6c899340c1260bcd313
8da936ab5226f4d4f390080a41e57fea63989c52,"Fix the build break for python3.7 PyUnicode_AsUTF8AndSize() prototype changing (#9259)

Summary:
https://docs.python.org/3.7/c-api/unicode.html#c.PyUnicode_AsUTF8AndSize
The return type changes from ""char*"" to ""const char*"".
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9259

Reviewed By: orionr

Differential Revision: D8776219

Pulled By: pjh5

fbshipit-source-id: e5eadf71264002ba57cfb68dd39686a7ec074092",JerryShih,https://api.github.com/repos/pytorch/pytorch/git/commits/8da936ab5226f4d4f390080a41e57fea63989c52
e30ff68410e57840ef0533385ebb04fc52125f9c,"Add Hardtanh Export (#8804)

Summary:
Added hartanh CPU/GPU Implementations and backend tests to Caffe2
Pull Request resolved: https://github.com/pytorch/pytorch/pull/8804

Reviewed By: bddppq

Differential Revision: D8813987

Pulled By: houseroad

fbshipit-source-id: 2480296eab3373425b9e1734a10c009b4f5d3e26",Ac2zoom,https://api.github.com/repos/pytorch/pytorch/git/commits/e30ff68410e57840ef0533385ebb04fc52125f9c
483ae8cb5d28791ed668240965c01ca015a46733,"Replaces const ref with && for apply (#9175)

Summary:
Addresses https://github.com/pytorch/pytorch/issues/5011
Tested with python test/test_autograd.py
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9175

Reviewed By: zdevito

Differential Revision: D8736377

Pulled By: marymcbreen

fbshipit-source-id: ff86f427f7b2cf0cab5912e7f32812bd0f49a712",marymcbreen,https://api.github.com/repos/pytorch/pytorch/git/commits/483ae8cb5d28791ed668240965c01ca015a46733
34554d6adb0d1594b07051b87722402cabe9c4ec,"Enable standalone build of ATen (#9377)

Summary:
This PR changes the ATen `CMakeLists.txt` slightly, to enable standalone build of ATen inside PyTorch. Currently, the tests in ATen gets linked to `libcaffe.so libcaffe2.so`. As a result, ATen can't be built standalone without building from the root pytorch directory. I know that there is a big merge happening between caffe2 and pytorch and hence, the purpose of this PR is to really start a conversation on what would be the proper way of migrating the CMakeLists to enable clean builds. We should also follow up on this PR: https://github.com/pytorch/pytorch/pull/7275. For your reference, that PR has the explanation for why `-Wl --no-as-need` is needed. Moreover, without `set(ATen_CUDA_SRCS ${all_cuda_cpp})`, the standalone build will throw unresolved references.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9377

Reviewed By: smessmer

Differential Revision: D8825921

Pulled By: orionr

fbshipit-source-id: c521159b4885639fc7990a9819202051455d07db",syed-ahmed,https://api.github.com/repos/pytorch/pytorch/git/commits/34554d6adb0d1594b07051b87722402cabe9c4ec
bcd20f96e0c41aebff5e608184513b6f2f73b7bf,"update docs (#9423)

Summary:
minor modification: fixed the incorrect comment format for ```split_size_or_sections``` (https://pytorch.org/docs/master/torch.html#torch.split)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9423

Differential Revision: D8841367

Pulled By: soumith

fbshipit-source-id: 2d09a38ce8d278ac29b3864e8d09a91cd296196c",LiyuanLucasLiu,https://api.github.com/repos/pytorch/pytorch/git/commits/bcd20f96e0c41aebff5e608184513b6f2f73b7bf
c4bff252821743cf64c30336d5ce6ce0457e2a34,"Additional operator information values (#9153)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9153

Closes https://github.com/pytorch/pytorch/pull/9153

Modified the values reported by the benchmarking platform to include tensor_shape and op_args. These values have a different naming scheme to values like flops and latency.

Reviewed By: sf-wind

Differential Revision: D8729791

fbshipit-source-id: f050200be01c6d0794bf5faaa6e8cef12a00affe",bstocks101,https://api.github.com/repos/pytorch/pytorch/git/commits/c4bff252821743cf64c30336d5ce6ce0457e2a34
11fc16dc98791dbf550e700a834d956287e9ba7c,"Remove HTML tags from README.md (#9296)

Summary:
This change makes README.md compatible with both Github and VSTS markdown engines. Images can be reduced if necessary
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9296

Differential Revision: D8874931

Pulled By: soumith

fbshipit-source-id: 0c530c1e00b06fc891301644c92c33007060bf27",thiagocrepaldi,https://api.github.com/repos/pytorch/pytorch/git/commits/11fc16dc98791dbf550e700a834d956287e9ba7c
7d2a17876fc0b7b251f7e206d84aea13ae08ce3f,"test_cuda: ensure tests use float and adjust HalfTensor tolerances (#9475)

Summary:
test_cuda.py uses routine 'number' to prepare many testscases.
number should return a floating point value for float-type tensor
types, or integer otherwise. But number's test to classify the type
is incorrect, so it always returns the integer value.
(type(t).__name__ is always 'torch.tensortype' so never matches
'Double', 'Float', or 'Half'.)

Update number to use the existing is_floating() helper to make the
check.

The change to number causes a few tests to fail for HalfTensor. Relax
the tolerance for those in line with other HalfTensor testcases. The
failing tests--for addcdiv and fill--were not previously relaxed for
HalfTensor so are held to the over-strict 1e-5 default tolerance.

Finally, update a couple other tests for HalfTensor type to use the
existing is_half() helper.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9475

Reviewed By: yf225

Differential Revision: D8872112

Pulled By: ezyang

fbshipit-source-id: 016e3e15adb23f6606bd4c08218954c1396699db",hartb,https://api.github.com/repos/pytorch/pytorch/git/commits/7d2a17876fc0b7b251f7e206d84aea13ae08ce3f
0fe980c748b82b04e873de43e166564dd1b9c636,"Memory usage measurement -- Caffe2 (#9017)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9017

Closes https://github.com/pytorch/pytorch/pull/9017

Added ""get_blob_size_bytes"" to ""pybind_state.cc"" in Caffe2 to expose the size of blob in bytes.

Reviewed By: kuttas

Differential Revision: D8685696

fbshipit-source-id: 9a9d38f207c8c59ef534217181e8ce1514617628",lilinyy09,https://api.github.com/repos/pytorch/pytorch/git/commits/0fe980c748b82b04e873de43e166564dd1b9c636
5c695e3a60e570d67d3eaa0cbab92af4c768fdd6,"Implement 2D and 3D alpha_dropout (#9073)

Summary:
It implements per-channel alpha_dropout. It also creates corresponding function classes and unifies the process of dropout and alpha_dropout.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9073

Differential Revision: D8727008

Pulled By: ezyang

fbshipit-source-id: 9d509f9c5db4e98f7b698cdfc4443505a4d2b331",tippisum,https://api.github.com/repos/pytorch/pytorch/git/commits/5c695e3a60e570d67d3eaa0cbab92af4c768fdd6
89db578e665f11e5964c15b98136b883e354d673,"Fixed a typo

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/9523

Differential Revision: D8890124

Pulled By: soumith

fbshipit-source-id: dea8d153fc352c36b219298c52f2c97caf9999f4",lewha0,https://api.github.com/repos/pytorch/pytorch/git/commits/89db578e665f11e5964c15b98136b883e354d673
6557856671f022c630a5a0857bc17405eb614b78,"Fix l2 normalization when handling zero vector (#9594)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9594

When the input vector is a zero vector, the previous GPU code will give Nan in backward. We fix this.

Reviewed By: pjh5

Differential Revision: D8849732

fbshipit-source-id: 87b1fb1ee05dfdb0d43bcbe67e36f15896fe1706",bairdzhang,https://api.github.com/repos/pytorch/pytorch/git/commits/6557856671f022c630a5a0857bc17405eb614b78
5651b274586bf0fddead4e5ee20fdb58f3abc5f2,"Add CAFFE_STATIC_EVENT to Stats (#9501)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9501

Added a new stat value to log static states like CPU and memory usage.

Reviewed By: pjh5

Differential Revision: D8872254

fbshipit-source-id: 469e94cab99029a3da55f8986dddeadac076e2a8",roshikouhai,https://api.github.com/repos/pytorch/pytorch/git/commits/5651b274586bf0fddead4e5ee20fdb58f3abc5f2
d3688861ec8607af0e7bf4119911053f13aee500,"Fixed a missing '=' in LPPoolNd repr function (#9629)

Summary:
In the repr funciton of LPPoolNd(..) class, there was a missing '='. (`kernel_size{kernel_size}`)

Link to line in the code: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/pooling.py#L694

Original:

       return 'norm_type={norm_type}, kernel_size{kernel_size}, stride={stride}, ' \
              'ceil_mode={ceil_mode}'.format(**self.__dict__)

Fixed:

       return 'norm_type={norm_type}, kernel_size={kernel_size}, stride={stride}, ' \
              'ceil_mode={ceil_mode}'.format(**self.__dict__)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9629

Differential Revision: D8932913

Pulled By: soumith

fbshipit-source-id: 9030dff6b14659b5c7b6992d87ef53ec8891f674",vmirly,https://api.github.com/repos/pytorch/pytorch/git/commits/d3688861ec8607af0e7bf4119911053f13aee500
5e84403d5fc92ad2ed594b86a436c571dac55d30,"Fix for half conversion for ROCm 1.8.2 (#9663)

Summary:
This PR contains the change for explicit conversion between ushort and __half required for ROCm 1.8.2 support
bddppq
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9663

Differential Revision: D8943937

Pulled By: bddppq

fbshipit-source-id: 16102f9dbc68ed4ece2e8fc244825c3992c24901",ashishfarmer,https://api.github.com/repos/pytorch/pytorch/git/commits/5e84403d5fc92ad2ed594b86a436c571dac55d30
3bb8c5eab1029a7de971b7368e43be06ef97182d,"Allow MKLDNN on macOS, and any other OS where CMake is able to detect it.

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/9638

Reviewed By: soumith

Differential Revision: D8946130

Pulled By: resistor

fbshipit-source-id: 87bd9cb12608467b05bd4998fdb00bfdbd038ca2",resistor,https://api.github.com/repos/pytorch/pytorch/git/commits/3bb8c5eab1029a7de971b7368e43be06ef97182d
029cf1d78a1f95a3ef4998c82b1e338b79ee0ac9,"Improve error messages of wrong dimensions (#9694)

Summary:
Updated the error message terms _matrices_ and _vectors_ to _2D tensors_ and _1D tensors_ respectively.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9694

Differential Revision: D8949589

Pulled By: ezyang

fbshipit-source-id: 2cdcd72e0e9a4459f3691c133bb16ef218b5cf3f",idansc,https://api.github.com/repos/pytorch/pytorch/git/commits/029cf1d78a1f95a3ef4998c82b1e338b79ee0ac9
f6496229a5fbf626875488ecabb7dae5c3856bed,"Fixes xcode 10 beta 4 compile error (#9748)

Summary:
When building iOS apps with a caffe2 dependency, we were seeing the `caffe2/caffe2/mobile/contrib/ios/mpscnn/mpscnn.mm:33:17: error: method 'copyWithZone:' in protocol 'NSCopying' not implemented [-Werror,-Wprotocol]`. This fixes it by implementing a shallow copy with that method.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9748

Reviewed By: jerryzh168

Differential Revision: D8954332

Pulled By: williamtwilson

fbshipit-source-id: 0cd44408257c0bd3f4ffb80312ea9d13d13e5ff3",williamtwilson,https://api.github.com/repos/pytorch/pytorch/git/commits/f6496229a5fbf626875488ecabb7dae5c3856bed
4b6176073803c7b3243e923654c2ebefe340626f,"Add Adadelta optimizer to caffe2 (#9088)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9088

Closes https://github.com/pytorch/pytorch/pull/9088

- Added CPU/GPU implementations of Adadelta and SparseAdadelta.
- Added corresponding Python unittests

Reviewed By: BIT-silence

Differential Revision: D8712169

fbshipit-source-id: 544e99e13b230a919672a7341b3715d64597c0be",sidgoyal78,https://api.github.com/repos/pytorch/pytorch/git/commits/4b6176073803c7b3243e923654c2ebefe340626f
bca10ad70601f02674c4985e0f5d6541ac7239fd,"Implementation of Weibull distribution (#9454)

Summary:
This implements the two-parameter Weibull distribution, with scale $\lambda$ and shape $k$ parameters as described on [Wikipedia](https://en.wikipedia.org/wiki/Weibull_distribution).

**Details**
- We implement as a transformed exponential distribution, as described [here](https://en.wikipedia.org/wiki/Weibull_distribution#Related_distributions).
- The `weibull_min` variance function in scipy does not yet support a vector of distributions, so our unit test uses a scalar distribution instead of a vector.

Example of the bug:

```
>>> sp.stats.expon(np.array([0.5, 1, 2])).var() # fine
array([1., 1., 1.])
>>> sp.stats.weibull_min(c=np.array([0.5, 1, 2])).var() # buggy
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py"", line 490, in var
    return self.dist.var(*self.args, **self.kwds)
  File ""/usr/local/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py"", line 1242, in var
    res = self.stats(*args, **kwds)
  File ""/usr/local/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py"", line 1038, in stats
    if np.isinf(mu):
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9454

Differential Revision: D8863574

Pulled By: SsnL

fbshipit-source-id: 1ad3e175b469eee2b6af98e7b379ea170d3d9787",tonyduan,https://api.github.com/repos/pytorch/pytorch/git/commits/bca10ad70601f02674c4985e0f5d6541ac7239fd
c14e17eced9d6c4607e207a6acfa4bacf703e619,"Co-disitillation with different archs and/or feature set (#9793)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9793

Enable co-distillation with different archs

Reviewed By: pjh5

Differential Revision: D8888479

fbshipit-source-id: eac14d3d9bb6d8e7362bc91e8200bab237d86754",kinjad,https://api.github.com/repos/pytorch/pytorch/git/commits/c14e17eced9d6c4607e207a6acfa4bacf703e619
eb338878162c9306f2fa473ad4a365733b5356ec,"Addressed issue identified by static code analysis: potential buffer â€¦ (#9889)

Summary:
â€¦overrun
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9889

Differential Revision: D9026278

Pulled By: soumith

fbshipit-source-id: ee2ee255f34731ddc581261984c3caf56faa0e12",davidbrownellWork,https://api.github.com/repos/pytorch/pytorch/git/commits/eb338878162c9306f2fa473ad4a365733b5356ec
a709f232257450fd899ec829164bc488a0354b63,"revise a little spell mistake in tensor.py (#9868)

Summary:
Hello! I just find a small spell mistake while reading this source code. Just PR it, Thx!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9868

Reviewed By: gchanan, ezyang

Differential Revision: D9016030

Pulled By: soumith

fbshipit-source-id: fc3877177be080adbdbda99a169e401691292ebb",tomguluson92,https://api.github.com/repos/pytorch/pytorch/git/commits/a709f232257450fd899ec829164bc488a0354b63
7b375ed362c4da833c38691b6bf3ade0941d3bf1,"fix ParameterDict doc

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/9918

Differential Revision: D9026402

Pulled By: soumith

fbshipit-source-id: d0459dcda631e8921ab39725b9045e03960da5c9",wandering007,https://api.github.com/repos/pytorch/pytorch/git/commits/7b375ed362c4da833c38691b6bf3ade0941d3bf1
bf32ea80942ce720b105efcd517fd11182edeb08,"Fix dimension check in 1D instance norm, allowing 2D tensors alongside 3D. (#9924)

Summary:
Fixes #9776.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9924

Differential Revision: D9028328

Pulled By: soumith

fbshipit-source-id: d5f22abb2be83b34aee95ebe144c97519a6854f8",veugene,https://api.github.com/repos/pytorch/pytorch/git/commits/bf32ea80942ce720b105efcd517fd11182edeb08
c3fe071483457a84d892b1060ee7b08caa84f156,"Update hip files (#9826)

Summary:
The goal of this PR is to update the hip files to reflect relevant changes in cuda source files.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9826

Differential Revision: D9032840

Pulled By: bddppq

fbshipit-source-id: 504e55c46308eebfee3c9a7beea1f294fe03470f",rohithkrn,https://api.github.com/repos/pytorch/pytorch/git/commits/c3fe071483457a84d892b1060ee7b08caa84f156
12a1af37316315784a0413a9e44e43c4915a7e28,"Adding conv tests with explicit algo definition

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/9798

Differential Revision: D9034663

Pulled By: virtan

fbshipit-source-id: d722f25f1dd00231ccc3ad5960bbbef63af02c2d",virtan,https://api.github.com/repos/pytorch/pytorch/git/commits/12a1af37316315784a0413a9e44e43c4915a7e28
46d800280050ef531e1a446e173b38bb1c492755,"Fix bug that always uses the same blob when repeating poolings

Reviewed By: houseroad

Differential Revision: D9027902

fbshipit-source-id: 957702ad9736812ec5aa32066d286c2c3adffc49",huayuli00,https://api.github.com/repos/pytorch/pytorch/git/commits/46d800280050ef531e1a446e173b38bb1c492755
56974a06b50ec1219d2f13091b8e13f9efce7798,"Revert D8909766: [caffe2] Simplify order switch operators

Differential Revision:
D8909766

Original commit changeset: 17a302d5bf4a

fbshipit-source-id: 56c75a8ce27873ed1d5f194b9d6bf0049d8f21ba",anshuljain1,https://api.github.com/repos/pytorch/pytorch/git/commits/56974a06b50ec1219d2f13091b8e13f9efce7798
5ff1551eb9f544b3fa7f677b1efb7724512869c6,"ATen's emscripten support (#9803)

Summary:
Not sure if anybody is interested but I managed to infer a `GRU` fine in `wasm` using ATen's compiled with emscripten. It was quite trivial to fix the configuration.
It also passes most of the tests, specially all scalar tensor tests.

The command line to configure was, but could be simplified:
```
emconfigure cmake -DAT_LINK_STYLE=STATIC -DCAFFE2_CMAKE_BUILDING_WITH_MAIN_REPO=OFF -DCMAKE_C_FLAGS=""-Wno-implicit-function-declaration -DEMSCRIPTEN -s DISABLE_EXCEPTION_CATCHING=0"" -DCMAKE_CXX_FLAGS=""-Wno-implicit-function-declaration -DEMSCRIPTEN -s DISABLE_EXCEPTION_CATCHING=0"" -DCMAKE_INSTALL_PREFIX=/home/sugar/aten-wasm ../
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9803

Differential Revision: D9004610

Pulled By: ezyang

fbshipit-source-id: db26c59f27162ed80f6aee2973c4cb9252d3d1e4",sinkingsugar,https://api.github.com/repos/pytorch/pytorch/git/commits/5ff1551eb9f544b3fa7f677b1efb7724512869c6
c2d9d2888bbbc697f5512e980d476f466287e621,"Fix typo in tensors.rst (#10073)

Summary:
An tensor -> A tensor
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10073

Differential Revision: D9087421

Pulled By: soumith

fbshipit-source-id: 6713f5a5e11fb11dff0ab5d2d6274f7837c6625f",mhsekhavat,https://api.github.com/repos/pytorch/pytorch/git/commits/c2d9d2888bbbc697f5512e980d476f466287e621
799c947cf3fc6edd50a78af65fa1342a2be0a5f6,"add .gitattributes for EOL conversion. (#9813)

Summary:
`.bat` file's EOL is LF, so a build is failed on some Windows machines.
To fix this, add `.gitattributes` and set batch file's EOL to CRLF.

Discussion is in #9677.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9813

Differential Revision: D9026486

Pulled By: soumith

fbshipit-source-id: 341eaa677c35f8476a7eda1bac9827385072eb29",shkit,https://api.github.com/repos/pytorch/pytorch/git/commits/799c947cf3fc6edd50a78af65fa1342a2be0a5f6
294c06538416f5882e9f329695dcae7ff77c218b,"Changed serialization mechanism of LambdaLR scheduler (#9927)

Summary:
I opened an issue explaining some of my frustrations with the current state of schedulers.
While most points that I raised in [that issue](https://github.com/pytorch/pytorch/issues/8741#issuecomment-404449697) need to be discussed more thoroughly before being implemented, there are some that are not so difficult to fix.

This PR changes the way the LambdaLR scheduler gets serialized:
> The lr_lambda functions are only saved if the are callable objects (which can be stateful).
> There is no point in saving functions/lambdas as you need their definition before unpickling and they are stateless.

This has the big advantage that the scheduler is serializable, even if you use lambda functions or locally defined functions (aka a function in a function).

Does this functionality need any unit tests?
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9927

Differential Revision: D9055505

Pulled By: soumith

fbshipit-source-id: 6c1cec588beedd098ec7d2bce6a9add27f29e48f",0phoff,https://api.github.com/repos/pytorch/pytorch/git/commits/294c06538416f5882e9f329695dcae7ff77c218b
5bd43a7af86cd660d3b5bd91fad9c79d8b903c21,"Refactor Seq2SeqModelCaffe2EnsembleDecoder (#10035)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10035

This is an initial diff which refactors some of the components in the Seq2SeqModelCaffe2EnsembleDecoder class.

Reviewed By: jmp84

Differential Revision: D9026372

fbshipit-source-id: 449635208f24494209ae2fb78a19fca872970ea8",pritamdamania,https://api.github.com/repos/pytorch/pytorch/git/commits/5bd43a7af86cd660d3b5bd91fad9c79d8b903c21
1f6888b70a48c3031e251f2182a9e58bf26a75ee,"Allow mobile exporter to export string arrays (#10017)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10017

Allow mobile exporter to export string arrays

Reviewed By: pjh5

Differential Revision: D9061213

fbshipit-source-id: b6c5257eb2f0f964dba255b97dc5d32af8ce15a7",pushkartripathi,https://api.github.com/repos/pytorch/pytorch/git/commits/1f6888b70a48c3031e251f2182a9e58bf26a75ee
191482fa39b3bca085668fa5161ebf2cdb06bcc7,"Distinguish TupleLiteral from ListLiteral (#10128)

Summary:
Previously, the parser was emitting list literals for tuples, but the IR was representing list literals internally with TupleTypes.

For implementing most list operations, I think it will be helpful distinguish between lists (dynamic size, homogeneous types) and tuples (fixed arity, heterogeneous types)

This diff modifies the parser logic to emit tuple literals. This frees us to represent lists as ListType in the IR, while still properly mapping tuple literals to TupleTypes.

A following diff will actually switch over list literals to emit ListTypes.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10128

Differential Revision: D9121305

Pulled By: michaelsuo

fbshipit-source-id: e0cad07ae8bac680f7f8113d10e5129d5a1a511d",suo,https://api.github.com/repos/pytorch/pytorch/git/commits/191482fa39b3bca085668fa5161ebf2cdb06bcc7
7dc870bd7b138222409d947e8c87cd6b2d5ef346,"Delete invalid 'template' keyword (#10173)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10173

With D9024330, `Extend` fundtion is no more a template, which makes
the `template` keyword here invalid. For some reason current version of LLVM
doesn't catch this, but the latest one does.

Reviewed By: jerryzh168

Differential Revision: D9133462

fbshipit-source-id: 54ac9aad01f81b9b4e7b6e2864b8961478d2d860",taewookoh,https://api.github.com/repos/pytorch/pytorch/git/commits/7dc870bd7b138222409d947e8c87cd6b2d5ef346
5df8547ff91ffc548b8509feb52661941d812eb7,"Fix ONNX LogSoftmax export. (#9576)

Summary:
This fixes an issue with incorrect `axis=-1` in the exported ONNX.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9576

Reviewed By: yinghai

Differential Revision: D9125463

Pulled By: houseroad

fbshipit-source-id: 6f4cb1067d1aa6bb0a9f56690fc21816c98eebfa",kit1980,https://api.github.com/repos/pytorch/pytorch/git/commits/5df8547ff91ffc548b8509feb52661941d812eb7
ab293924bb6cc3abe8e07bb6606b54e7a886f7e4,"support generic feature in DPER2 (#10197)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10197

Support generic feature in DPER2

For now since we only have one generic type 1, we are directly adding the parsed feature record to embedding feature.

For new feature types with specific structure, there should also be corresponding coding changes expected.

Reviewed By: itomatik

Differential Revision: D8788177

fbshipit-source-id: 9aaa6f35ece382acb4072ec5e57061bb0727f184",huginhuangfb,https://api.github.com/repos/pytorch/pytorch/git/commits/ab293924bb6cc3abe8e07bb6606b54e7a886f7e4
25b2e88750e81111d609c04911e631ee21998c19,"Stop propagating std flags to downstream gcc/nvcc (#10098)

Summary:
When we directly use -std=c++11, it propagates to the downstream applications.

Problems:
1. Gcc flags propagating to nvcc.
2. nvcc flags propagating to nvcc. (Which throws an error like redeclaration of std flag)

This PR will fix these propagation issues!

Similar problem:
https://github.com/FloopCZ/tensorflow_cc/pull/92
https://github.com/CGAL/cgal/issues/2775

Requires: Cmake 3.12
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10098

Differential Revision: D9187110

Pulled By: ezyang

fbshipit-source-id: 0e00e6aa3119c77a5b3ea56992ef3bbfecd71d80",achalshah20,https://api.github.com/repos/pytorch/pytorch/git/commits/25b2e88750e81111d609c04911e631ee21998c19
f1cf3105dec663c2d2eb2b3ae58247be4e7576b8,"Revert D9169049: [pytorch][PR] Add new mkldnn fallback operators

Differential Revision:
D9169049

Original commit changeset: 3bc30250d734

fbshipit-source-id: 65a91594bda699ff9535b27dccd0d1e5d1a8036a",jgeboski,https://api.github.com/repos/pytorch/pytorch/git/commits/f1cf3105dec663c2d2eb2b3ae58247be4e7576b8
ab6afc2b238deb9e3b731399a367385518b788e5,"Optimize max_pooling for inference for MKL-DNN/IDEEP device (#10156)

Summary:
Optimize the max_pooling operation for inference path by setting the ""inference"" flag to the underlying MKL-DNN, saving the computation and store of max indices which is only needed for training. To make the API compatible, training mode is still the default and inference mode is set in the optimizeForIdeep path.
Test shows the speed-up of a single max_pooling operation is up to 7X on BDW.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10156

Differential Revision: D9276755

Pulled By: yinghai

fbshipit-source-id: ad533d53aabb8ccb3b592da984d6269d9b794a8a",jgong5,https://api.github.com/repos/pytorch/pytorch/git/commits/ab6afc2b238deb9e3b731399a367385518b788e5
ffb59e5f20ef56158ad2bce33581cfba0655bec0,"adding stochastic quantization caffe2 operators (encoder and decoder in CPU are implemented. GPU mode is pending)

Summary:
This operator implements b (1/2/4/8) bit stochastic quantization of a floating
matrix in a row-wise fashion. 8/b floating values are concatenated to a byte
and returned in uint8 tensor. PR: https://github.com/pytorch/pytorch/pull/8629

Reviewed By: harouwu

Differential Revision: D8493264

fbshipit-source-id: 01f64066568a1e5a2b87c6d2134bd31cdf119c02",wenwei202,https://api.github.com/repos/pytorch/pytorch/git/commits/ffb59e5f20ef56158ad2bce33581cfba0655bec0
ef44faece2cd4045f58cbbac6c74842b84ac6c45,"check attribute existence in torch.legay.nn.SpatialFullConvolution in method type (#8740)

Summary:
This is related to #5255
When adding cuda support for the model, this error comes:
``
AttributeError: 'SpatialFullConvolution' object has no attribute 'finput'
``
here is my short code for test.
https://gist.github.com/kaleaht/26518c3deea5d1d3dda722fbf1f3ecdc

I converted torch7's model also from here.
https://github.com/art-programmer/FloorplanTransformation
Pull Request resolved: https://github.com/pytorch/pytorch/pull/8740

Differential Revision: D8872735

Pulled By: SsnL

fbshipit-source-id: 8d97f8b59cdf4049e87be14b78c4608fd973d149",kaleaht,https://api.github.com/repos/pytorch/pytorch/git/commits/ef44faece2cd4045f58cbbac6c74842b84ac6c45
3a40baa15cd845fbea17daeb12ed0e240c282839,"fix a grammatical error: accelerate compute (#10204)

Summary:
""accelerate compute""
a verb shouldn't go with another verb.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10204

Differential Revision: D9316699

Pulled By: fmassa

fbshipit-source-id: f1126c594905c3236ffd6b7e57a92552d3d4c1f1",ladyrick,https://api.github.com/repos/pytorch/pytorch/git/commits/3a40baa15cd845fbea17daeb12ed0e240c282839
eea8ab1861d92b56386fcb659148166c2d8d5eb5,"Move common code to RNNCellBase. (#10399)

Summary:
There are three classes `RNNCell`, `LSTMCell`, `GRUCell` inherited from `RNNCellBase`, all defining the identical initialization function `reset_parameters`. Lets move it to the common base.
Another option is to have different initialization for RNN, LSTM and GRU. Maybe those weights whose output is processed with sigmoid (i.e. gain=1) should be initialized differently from those going to tanh (gain=5/3)?
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10399

Differential Revision: D9316978

Pulled By: SsnL

fbshipit-source-id: a2d9408f0b5c971a3e6c3d42e4673725cf03ecc1",striajan,https://api.github.com/repos/pytorch/pytorch/git/commits/eea8ab1861d92b56386fcb659148166c2d8d5eb5
16ecd6f99c468027e4a08feb273e4fb31bee50bb,"Fix Debug Build On Windows (#10359)

Summary:
compile files in torch/csrc with /MDd runtime library option for debug build on Windows
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10359

Differential Revision: D9316946

Pulled By: SsnL

fbshipit-source-id: c84bfad81d61cd49f39b7bce7177edd2b1e8bd69",lara-hdr,https://api.github.com/repos/pytorch/pytorch/git/commits/16ecd6f99c468027e4a08feb273e4fb31bee50bb
e41528a5cc1f58c1b3936e6def26839c9985ed69,"Also set stdin to subprocess pipe in FindCUDA windows popen call (#10379)

Summary:
Background: we run pytorch in embedded C++ pipelines, running in C++ GUIs in https://github.com/Kitware/VIAME and without this addition, the call was failing with the below error, but only on certain windows platforms/configurations:

OSError: [WinError6] The handle is invalid
At:
C:\Program Files\VIAME\Python36\site-packages\torch\cuda_init_.py(162):_lazy_init
C:\Program Files\VIAME\Python36\site-packages\torch\nn\modules\module.py(249): <lambda>
C:\Program Files\VIAME\Python36\site-packages\torch\nn\modules\module.py(182): _apply
C:\Program Files\VIAME\Python36\site-packages\torch\nn\modules\module.py(176): _apply
C:\Program Files\VIAME\Python36\site-packages\torch\nn\modules\module.py(249): cuda
C:\Program Files\VIAME\lib\python3.6None\site-packages\kwiver\arrows\pytorch\pytorch_resnet_f_extractor.py(74):_init_
C:\Program Files\VIAME\lib\python3.6None\site-packages\kwiver\processes\resnet_descriptors.py(132): _configure
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10379

Differential Revision: D9330772

Pulled By: ezyang

fbshipit-source-id: 657ae7590879004558158d3c4abef2ec11d9ed57",mattdawkins,https://api.github.com/repos/pytorch/pytorch/git/commits/e41528a5cc1f58c1b3936e6def26839c9985ed69
9646d689626f9b55bfbb087f685c3037fc266652,"support broadcasting in _kl_categorical_categorical (#10533)

Summary:
Support broadcasting in _kl_categorical_categorical

this makes it possible to do:
```
import torch.distributions as dist
import torch
p_dist = dist.Categorical(torch.ones(1,10))
q_dist = dist.Categorical(torch.ones(100,10))
dist.kl_divergence(p_dist, q_dist)
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10533

Differential Revision: D9341252

Pulled By: soumith

fbshipit-source-id: 34575b30160b43b6c9e4c3070dd7ef07c00ff5d7",joh4n,https://api.github.com/repos/pytorch/pytorch/git/commits/9646d689626f9b55bfbb087f685c3037fc266652
c5b1aa93ee5b8b500e2eb4e7d504a368ab765b9f,"Export uint8 tensors as byte string in mobile_exporter and add GivenTensorByteStringToUInt8FillOp (#10385)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10385

Pull Request resolved: https://github.com/pytorch/pytorch/pull/10354

Pull Request resolved: https://github.com/pytorch/pytorch/pull/10316

Because Protobuf encodes uint8_t tensors using a less space efficient varint uin32_t encoding, we are adding a new operator that reads back a byte string into a uint8_t tensor.

Reviewed By: harouwu

Differential Revision: D9004839

fbshipit-source-id: dfd27085c813fdeff13fee15eef4a2e7fef72845",3l1,https://api.github.com/repos/pytorch/pytorch/git/commits/c5b1aa93ee5b8b500e2eb4e7d504a368ab765b9f
40a070422db35f8348077676e858db6bc8612cf3,"Adding new allreduce bcube routines to ops supported by gloo (#10494)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10494

Adding the AllredubeBcube routines as they are now available in gloo.

Reviewed By: wesolwsk

Differential Revision: D8269473

fbshipit-source-id: 6a3a32291bbf1fbb328b3ced0f2a753dc5caf4e5",kirteshpatil,https://api.github.com/repos/pytorch/pytorch/git/commits/40a070422db35f8348077676e858db6bc8612cf3
d87b4e941b2c2c629e77860688d3195c4ebd87a4,"fix python interpreter can not be found without `PYTHON_EXECUTABLE` (#10659)

Summary:
Take 2 of #10543
The problem was that between commit and merge there was added one more run point `tools/build_libtorch.py`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10659

Differential Revision: D9393540

Pulled By: soumith

fbshipit-source-id: 8ebfed600fc735fd1cb0489b161ec80e3db062e0",pohmelie,https://api.github.com/repos/pytorch/pytorch/git/commits/d87b4e941b2c2c629e77860688d3195c4ebd87a4
9767951ca8a0af26d3caf2e84a52b10f81b9c2cb,"Remove regex matching from undefined_tensor_test, fixes #10013 (#10702)

Summary:
Don't regex against strings that may have come from the backtrace.
Better to just not regex at all.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/10702

Reviewed By: ezyang

Differential Revision: D9406154

Pulled By: jsrmath

fbshipit-source-id: 9b17abee2a6e737a32c05f1e3963aef4b6638a47",jsrmath,https://api.github.com/repos/pytorch/pytorch/git/commits/9767951ca8a0af26d3caf2e84a52b10f81b9c2cb
5ca2713a8b0452ab48fcde52dc5932b4c8c4790a,"Fix performance of WeightedRandomSampler (#10636)

Summary:
Since https://github.com/pytorch/pytorch/pull/8958 was merged, the BatchSampler samples 0d tensors from WeightedRandomSampler instead of integers. It significantly reduces performance. This PR fix it the same way as https://github.com/pytorch/pytorch/pull/10361 fix DistributedSampler.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10636

Differential Revision: D9423869

Pulled By: zou3519

fbshipit-source-id: f94da2d4cccf70e63beea6cfc3d1230b5610ae44",Chetter2,https://api.github.com/repos/pytorch/pytorch/git/commits/5ca2713a8b0452ab48fcde52dc5932b4c8c4790a
529fc68df280663e1c7d145a70ef3ddb20c62e36,"Update docs with clean (#10819)

Summary:
Add tip about cleaning if installing ninja after a build.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10819

Reviewed By: soumith

Differential Revision: D9480095

Pulled By: erikbrinkman

fbshipit-source-id: 96ae1387038afe6964a1bd1e2186468f6a5ea12f",erikbrinkman,https://api.github.com/repos/pytorch/pytorch/git/commits/529fc68df280663e1c7d145a70ef3ddb20c62e36
432b3adffc159869c54baaa21f4d8c39301cb611,"Print blob sizes on fatal signal (#10766)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10766

Added a `Workspace::ForEach(...)` API for accessing the global set of
existing Workspace instances. This is used in the signal handler to print blob
info on the thread receiving a fatal signal.

Reviewed By: mraway

Differential Revision: D9147768

fbshipit-source-id: a94d0b5e6c88390a969ef259ecb8790173af01a4",andreimaximov,https://api.github.com/repos/pytorch/pytorch/git/commits/432b3adffc159869c54baaa21f4d8c39301cb611
ee022a476a2747abed4a950ed88a27f620f1d1f7,"Added this-consts to all methods on SymbolicVariable (#10805)

Summary:
Self explanatory. See https://github.com/pytorch/pytorch/issues/9109 or T32954812 for more details
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10805

Reviewed By: ezyang

Differential Revision: D9477686

Pulled By: hakobyant

fbshipit-source-id: 73dd84e5295e4c749bd6416ce2f6eb7590f05cbc",hakobyant,https://api.github.com/repos/pytorch/pytorch/git/commits/ee022a476a2747abed4a950ed88a27f620f1d1f7
474684cf037a1d9034e10443fddfc73679645667,Re-sync with internal repository (#10868),yns88,https://api.github.com/repos/pytorch/pytorch/git/commits/474684cf037a1d9034e10443fddfc73679645667
1421a9d7041d0c877ffdecb92ea90ead5425c0a5,"added num_directions explanation to docstrings (#10786)

Summary:
Resolving [https://github.com/pytorch/pytorch/issues/10741](https://github.com/pytorch/pytorch/issues/10741). The current docs use `num_directions` quite a bit, without any explanation for them. `num_directions` is set to 2 if the RNN is bidirectional, or 1 otherwise. This change simply adds that to the docs.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10786

Differential Revision: D9480235

Pulled By: zou3519

fbshipit-source-id: f61d1b0d2b943f84d5b7ff83df6fe0965a508a5e",rohan-varma,https://api.github.com/repos/pytorch/pytorch/git/commits/1421a9d7041d0c877ffdecb92ea90ead5425c0a5
5c58cda8cabfdf89faf4c8dda73783a2c53f708a,"Add subname to console output for assertExpected (#10559)

Summary:
Running `--accept` on a test doesn't tell you explicitly which sub-test is being updated, this PR fixes that
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10559

Differential Revision: D9353977

Pulled By: driazati

fbshipit-source-id: a9d4014386ff0fe388a092f3dcf50f157e460f04",driazati,https://api.github.com/repos/pytorch/pytorch/git/commits/5c58cda8cabfdf89faf4c8dda73783a2c53f708a
23b0c90e710026cc9014d6035ea3e5d9ccb7b71c,"caffe2: fix gcc8 warnings

Summary:
The warnings are erroneous as far as i can see,
so tweak things to avoid. The (unsigned int) cast is
to avoid passing -1 to a size_t time.  This was triggered
in gcc8's lto build only, giving:

  caffe2/aten/src/TH/generic/THTensor.cpp: In function â€˜THFloatTensor_squeeze1dâ€™:
  lto1: error: â€˜__builtin_memsetâ€™ specified size 18446744073709551608
  exceeds maximum object size 9223372036854775807 [-Werror=stringop-overflow=]
  In function â€˜newImplâ€™,
    inlined from â€˜operator newâ€™ at common/memory/OperatorOverride.cpp:86:23,
    inlined from â€˜allocateâ€™ at third-party-buck/platform007/build/libgcc/include/c++/7.3.0/ext/new_allocator.h:111:0,
    inlined from â€˜allocateâ€™ at third-party-buck/platform007/build/libgcc/include/c++/7.3.0/bits/alloc_traits.h:436:0,
    inlined from â€˜_M_allocateâ€™ at third-party-buck/platform007/build/libgcc/include/c++/7.3.0/bits/stl_vector.h:172:0,
    inlined from â€˜_M_default_appendâ€™ at third-party-buck/platform007/build/libgcc/include/c++/7.3.0/bits/vector.tcc:571:0,
    inlined from â€˜resizeâ€™ at third-party-buck/platform007/build/libgcc/include/c++/7.3.0/bits/stl_vector.h:671:0,
    inlined from â€˜THTensor_resizeDimâ€™ at caffe2/aten/src/TH/THTensor.hpp:123:0,
    inlined from â€˜THFloatTensor_squeeze1d.part.198â€™ at caffe2/aten/src/TH/generic/THTensor.cpp:429:0,
    inlined from â€˜THFloatTensor_squeeze1dâ€™:
  common/memory/OperatorOverride.cpp:86:23: error:
  argument 1 value â€˜18446744073709551608â€™ exceeds maximum object size 9223372036854775807 [-Werror=alloc-size-larger-than=]
   void* ptr = malloc(size);

Reviewed By: soumith

Differential Revision: D9568621

fbshipit-source-id: 4569a4be897d669caa3f283f4b84ec829e8d77ad",pixelb,https://api.github.com/repos/pytorch/pytorch/git/commits/23b0c90e710026cc9014d6035ea3e5d9ccb7b71c
9fae8fcdff03aeaab39f418f2ca71e9e83957ddb,"framework for committed serialized tests (#10594)

Summary:
Generate serialized test inputs/outputs/backward graphs of tests inside `caffe2/python/operator_test` that call assertSerializedOperatorCheck(). Tests should be decorated with serialized_test.collect_tests.given_and_seeded to run hypothesis tests that are actually random and a single fixed seeded hypothesis tests.

To use:
1. Refactor your test to be a SerializedTestCase
1a. Decorate it with given_and_seeded
1b. Call testWithArgs in main
2. Run your test with -g to generate the output. Check it in.
3. Subsequent runs of the test without generating the output will check against the checked in test case.

Details:
Run your test with `python caffe2/python/operator_test/[your_test].py -g`
Outputs are in `caffe2/python/serialized_test/data`. The operator tests outputs are in a further subdirectory `operator_test`, to allow for other tests in the future (model zoo tests?)

Currently, we've only refactored weighted_sum_test to use this, but in the next diff, we'll refactor as many as possible. The directory structure may also change as usually there are multiple tests in a single file, so we may create more structure to account for that.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10594

Reviewed By: ezyang

Differential Revision: D9370359

Pulled By: ajyu

fbshipit-source-id: 2ce77389cd8bcc0255d3bccd61569833e545ede8",ajyu,https://api.github.com/repos/pytorch/pytorch/git/commits/9fae8fcdff03aeaab39f418f2ca71e9e83957ddb
0e8088d6f6ae8fd9079f0837c5aae80cd2573abb,"Fix typo in data_parallel_model

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/11086

Differential Revision: D9581297

fbshipit-source-id: b164177bdbb309f56ff3231c1ffc0973f6c5299b",Bellaktris,https://api.github.com/repos/pytorch/pytorch/git/commits/0e8088d6f6ae8fd9079f0837c5aae80cd2573abb
02114e877f3b1acc4966968af73ff453ee1f9ad1,"fix #10838 incorrect bidirectional output format (#11368)

Summary:
Fixes the issue discussed in #10838. `hidden_size` should be the last dimension regardless if we're in ONNX or PyTorch.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11368

Differential Revision: D9734814

Pulled By: soumith

fbshipit-source-id: 7f69947a029964e092c7b88d1d79b188a417bf5f",tengyifei,https://api.github.com/repos/pytorch/pytorch/git/commits/02114e877f3b1acc4966968af73ff453ee1f9ad1
3a8e39b21506752f77c24229008a48a0e33a66c9,"Support load and store between Py_complex and std::complex (#11493)

Summary: Printing for complex numbers requires loading and storing between `Py_complex` and `std::complex`. This patch aims to support this for the plugin.

Differential Revision: D9771808

Pulled By: ezyang

fbshipit-source-id: 024865f1945d63ddb5efc775a35438c8ea06408e",Roger-luo,https://api.github.com/repos/pytorch/pytorch/git/commits/3a8e39b21506752f77c24229008a48a0e33a66c9
35348dab103316a929ae55d91a9d03e049d7fc43,"WIP: Include note on cudnn determinism in each function backed by cudnn (#11434)

Summary:
Ping ezyang
This addresses your comment in #114. Strangely, when running the doc build (`make html`) none of my changes are actually showing, could you point out what I'm doing wrong?

Once #11329 is merged it might make sense to link to the reproducibility note everywhere.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11434

Differential Revision: D9751208

Pulled By: ezyang

fbshipit-source-id: cc672472449564ff099323c39603e8ff2b2d35c9",themightyoarfish,https://api.github.com/repos/pytorch/pytorch/git/commits/35348dab103316a929ae55d91a9d03e049d7fc43
f129da1a47274615bce094e949b77c8791de9a3b,"Add max to the ValueError for EmbeddingBag mode check (#11655)

Summary:
Related to #11624
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11655

Differential Revision: D9815454

Pulled By: SsnL

fbshipit-source-id: 8dd82e0c0aa68362e12b301e095a85af7d7fd71a",zippeurfou,https://api.github.com/repos/pytorch/pytorch/git/commits/f129da1a47274615bce094e949b77c8791de9a3b
05e06f7de2c84b8dd770c2f5008f5b7620e97d39,"migrating deprecated calls without abc module for containers (#11515)

Summary:
Implementing #10540.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11515

Reviewed By: apaszke

Differential Revision: D9771045

Pulled By: jeffreyksmithjr

fbshipit-source-id: 85ea39abaa9b465805a969f122b626b11fc85ef6",jeffreyksmithjr,https://api.github.com/repos/pytorch/pytorch/git/commits/05e06f7de2c84b8dd770c2f5008f5b7620e97d39
91b6458e2d0dba935da2cc7c2cdc6d7907bc3f48,"Container __getitem__ slicing for subclasses (#11694)

Summary:
Simple change to allow ModuleList subclasses's `__getitem__(slice)` to return class of subclass rather than ModuleList
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11694

Differential Revision: D9892824

Pulled By: ezyang

fbshipit-source-id: b75e9c196487f55cb93f0dab6c20d850e8e759ff",nehz,https://api.github.com/repos/pytorch/pytorch/git/commits/91b6458e2d0dba935da2cc7c2cdc6d7907bc3f48
4ee0a78ee6452b7c1584b6020aef791b9d3e8ab6,"varargs for meshgrid (#11600)

Summary:
Adds vararg support for meshgrid and adds checks for all the tensor arguments to have the same dtype and device.

Fixes: [#10823](https://github.com/pytorch/pytorch/issues/10823), #11446

The earlier pull request closed without any changes because I had some rebasing issues, so I made another pull request to close out #10823. Sorry for the inconvenience.

Differential Revision: D9892876

Pulled By: ezyang

fbshipit-source-id: 93d96cafc876102ccbad3ca2cc3d81cb4c9bf556",upriser7,https://api.github.com/repos/pytorch/pytorch/git/commits/4ee0a78ee6452b7c1584b6020aef791b9d3e8ab6
e585f2fb480f1bdf2a570a0f049ab3e4f45ca3a1,"Polish CPP docs, Minor Python Docs Fixes (#11722)

Differential Revision: D9919120

Pulled By: goldsborough

fbshipit-source-id: bf14cbe4ab79524495957cb749828046af864aab",svenevs,https://api.github.com/repos/pytorch/pytorch/git/commits/e585f2fb480f1bdf2a570a0f049ab3e4f45ca3a1
ae1a972d78950abc4dab372f496914b5e78b9637,"Fix #11752: correct numerical issue with log_softmax (#11866)

Summary:
This fixes the numerical problem in log_softmax cpu code when inputs are big but their differences are small.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11866

Differential Revision: D9946799

Pulled By: soumith

fbshipit-source-id: 11fe8d92b91ef6b7a66f33fbce37ec2f0f0929be",sytrus-in-github,https://api.github.com/repos/pytorch/pytorch/git/commits/ae1a972d78950abc4dab372f496914b5e78b9637
83740eae4af33d53fb4c41012b14cb14725879be,"Avoid using PyThreadState.frame as it is not a public member. (#11855)

Summary:
The doc of PyThreadState [1] emphasizes that interp is its only public member. Use PyEval_GetFrame() instead.

[1] https://docs.python.org/3/c-api/init.html#c.PyThreadState
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11855

Differential Revision: D9954430

Pulled By: ezyang

fbshipit-source-id: 92da6781e45e2bcb5e3a37b162fa40e49d823215",xuhdev,https://api.github.com/repos/pytorch/pytorch/git/commits/83740eae4af33d53fb4c41012b14cb14725879be
b91b15d86e274611f448caa3f9b73eb4bf24a625,"Implementing Matrix Norm for torch.norm (#11261)

Summary:
Currently, norm function only supports vector norm. This PR extends vector norm to matrix norm.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11261

Reviewed By: li-roy

Differential Revision: D9652379

Pulled By: yya007

fbshipit-source-id: 519b3fb80b563c17c56a24675c7b0e46bf5a3a1c",yya007,https://api.github.com/repos/pytorch/pytorch/git/commits/b91b15d86e274611f448caa3f9b73eb4bf24a625
17cd426c7248d26e90bb3b0ae0c5c958cbd27b47,"Updated docs styles (#11835)

Summary:
Updated requirements.txt and conf.py.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11835

Reviewed By: SsnL

Differential Revision: D9941160

Pulled By: brianjo

fbshipit-source-id: fbac91214558e6d17beff74261d990c7dc762038",brianjo,https://api.github.com/repos/pytorch/pytorch/git/commits/17cd426c7248d26e90bb3b0ae0c5c958cbd27b47
9068a46dba7090df80079dc0a88bcda4b472e8cd,"Fix deprecated function warning in ONNX model test. (#11827)

Summary:
When running /test/onnx/test_models.py, we see deprecation warnings in the test points for `super_resolution` and `squeezenet` models. This change updates those models to use the recommended methods, instead of the deprecated ones.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11827

Reviewed By: houseroad

Differential Revision: D10023998

Pulled By: ezyang

fbshipit-source-id: ee4e14304678c532ebd574e7bd143e3b311995ab",spandantiwari,https://api.github.com/repos/pytorch/pytorch/git/commits/9068a46dba7090df80079dc0a88bcda4b472e8cd
b7b9e3c7e8cf869bcc4bb5aa73a149d2e1f92f9d,"Fix ""identifier following the 'template' keyword does not refer to a template"" (#12037)

Summary:
LLVM trunk emits an error diagnostic when attempting to compile caffe2. The
identifiers following the `template` keywords are not templates, so the use of
the keyword does not make sense in this context.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12037

Reviewed By: ezyang

Differential Revision: D10024531

Pulled By: modocache

fbshipit-source-id: da4b9ba405d9f7fd633ab8c1a61c77da9c1a1f89",modocache,https://api.github.com/repos/pytorch/pytorch/git/commits/b7b9e3c7e8cf869bcc4bb5aa73a149d2e1f92f9d
db2f7de5c3b91776ce4f46b1bddf1c2c17069153,"Fallback CreateMutex/AtomicIter operators for mkl-dnn

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/11685

Reviewed By: pjh5

Differential Revision: D9928058

Pulled By: wesolwsk

fbshipit-source-id: 734e19c35a684481d9a4d4f0c596e4dceae51ad4",PenghuiCheng,https://api.github.com/repos/pytorch/pytorch/git/commits/db2f7de5c3b91776ce4f46b1bddf1c2c17069153
c2f8f5076c5824890fc076ac05bfb00abd72b53f,"add narrow() support for sparse tensors re: #8853 (#11342)

Summary:
Couple questions:

1) I used the log1p implementation in #8969 as a guide especially for testing.  I'm not sure what the ```skipIfROCM``` annotation is for, so unsure if i need it for my test.

2) I implemented the branching logic in the narrow function itself; is this the right place to do so?  I noticed that there a number of places where sparse-specific logic is handled with just an if statement in this file.  Or should I implement a separate dispatch in native_functions.yml as in the log1p?

And of course, happy to make any any other updates/changes that I may have missed as well.  This is my first PR to the project.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11342

Differential Revision: D9978430

Pulled By: weiyangfb

fbshipit-source-id: e73dc20302ab58925afb19e609e31f4a38c634ad",realdoug,https://api.github.com/repos/pytorch/pytorch/git/commits/c2f8f5076c5824890fc076ac05bfb00abd72b53f
d9c27f4d8d2e71a5dd3b3c3f82790d14a296124f,"T33898723: Simple put operators for caffe2 stats (#12057)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12057

Add simple put operators for various types of stats

Reviewed By: mlappelbaum

Differential Revision: D9925268

fbshipit-source-id: cec02b0027d2d0ef3d35741be4b02c429d492810",jdshi-fb,https://api.github.com/repos/pytorch/pytorch/git/commits/d9c27f4d8d2e71a5dd3b3c3f82790d14a296124f
6ff568df4dc536e4dfbb090caa9db8b3a71f2bf1,"Add full namespace resolution in CAFFE_DURATION (#12065)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12065

Had compilation issues using CAFFE_DURATION in some contexts, specifically due to namespace resolution. Since this is a macro, it should fully qualify.

Reviewed By: heslami

Differential Revision: D10036132

fbshipit-source-id: b8d55dfe5e991ca702ce5b7483f0ffc699882c85",vladbelous,https://api.github.com/repos/pytorch/pytorch/git/commits/6ff568df4dc536e4dfbb090caa9db8b3a71f2bf1
04c0971679374148aa4105e2e998de6478eca1eb,"Special case BatchGather and BatchGatherGradient for block_size=1. (#11349)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11349

Special case BatchGather and BatchGatherGradient for block_size=1. This makes BatchGather 3-4X faster and BatchGatherGradient 10X for this case.

Reviewed By: jspark1105, ilia-cher

Differential Revision: D7218043

fbshipit-source-id: ea12042239a8adc92b9efcbd0b66e354fb43f4c7",nrsatish,https://api.github.com/repos/pytorch/pytorch/git/commits/04c0971679374148aa4105e2e998de6478eca1eb
ab9a5976a025b3a56cfc64b86981ad52f8fe4137,"Disable inlinining of EnforceFailMessage (#12078)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12078

The constructor is inlined multiple times

Reviewed By: salexspb

Differential Revision: D9358084

fbshipit-source-id: c8d4177a3fcccac574ee4f63336a6fa8bfb07d11",aditya7fb,https://api.github.com/repos/pytorch/pytorch/git/commits/ab9a5976a025b3a56cfc64b86981ad52f8fe4137
3010dc4208df1d2bd8058cfafeffe645f454595c,"Revert D10123245: Back out ""codemod cuda_gpu_id to device_id""

Differential Revision:
D10123245

Original commit changeset: d83da8e00a12

fbshipit-source-id: fca91fea58b7df208edc2e218a1d514f9821ec7b",rratmansky,https://api.github.com/repos/pytorch/pytorch/git/commits/3010dc4208df1d2bd8058cfafeffe645f454595c
ecace9eb217903cd7f15d97516b7ef93c9820e5b,"Move crf in caffe2 from fb to oss (#12200)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12200

moved crf_viterbi_op, copied crf_predict and crf_viterbi_test to oss

Reviewed By: Yangqing

Differential Revision: D10118341

fbshipit-source-id: 51e30e57d280d6ca75fc0b488f743794f23b589f",seayoung1112,https://api.github.com/repos/pytorch/pytorch/git/commits/ecace9eb217903cd7f15d97516b7ef93c9820e5b
080266e79ce680a7bc36cf5fdfe8bac523447051,"Document CUDAHOSTCXX environment variable (#12265)

Summary:
This variable is already being used so this just serves to document that. I think it's an important variable, too, so it should definitely be documented there somewhere.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12265

Differential Revision: D10162261

Pulled By: soumith

fbshipit-source-id: e0d01e012c2fedea63372de9967a8eaa3745fe94",svenstaro,https://api.github.com/repos/pytorch/pytorch/git/commits/080266e79ce680a7bc36cf5fdfe8bac523447051
16e21e14e398b90cfada8a0c6a2f628505e1dfac,"Fix Caffe2 build on 64-bit Android (#12340)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12340

`long` and `int64_t` are the same type on 64-bit Android.

Reviewed By: Yangqing

Differential Revision: D10204892

fbshipit-source-id: 2d5bf707bf87b99fc597c9292b59f032e9004620",dreiss,https://api.github.com/repos/pytorch/pytorch/git/commits/16e21e14e398b90cfada8a0c6a2f628505e1dfac
b3cdaee6dbe88e49750514107a64e3cb0c674083,"Update README.md of ATen Documentation (#12367)

Summary:
The changes are made to clarify how the parsing between the yaml files and header files of THNN and THCUNN works. As issue #12320 shows it is not that easy to understand the existing code without a hint to the important files.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12367

Differential Revision: D10217459

Pulled By: ezyang

fbshipit-source-id: 9b3e64dea4f156843814840e736dc3230332060c",ohlr,https://api.github.com/repos/pytorch/pytorch/git/commits/b3cdaee6dbe88e49750514107a64e3cb0c674083
0ebbfc25f3d3441912a51976c8550c310b102c54,"Add utility function make_tensor (#12288)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12288

Current implementation of Tensor takes an intrusive_ptr as an argument for storing data. But instead of requiring users to explicitly pass an intrusive_ptr we want them to pass args for intrusive ptr directly which are forwarded internally through new helper function called make_tensor

Reviewed By: ezyang

Differential Revision: D10152661

fbshipit-source-id: bfa72de161ace3fd1c4573427abcd1bfbd12e29e",devashisht,https://api.github.com/repos/pytorch/pytorch/git/commits/0ebbfc25f3d3441912a51976c8550c310b102c54
c7e8044fc8fcafdbff15b13bfd3b6d56cb506fa0,"Support additional device types (#12293)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12293

Adding support for additional device types besides cuda and cpu.

Reviewed By: ezyang

Differential Revision: D10175683

fbshipit-source-id: 7a8a35c3f1b13a3b6ed84dd2d835f3902a418a6c",nairbv,https://api.github.com/repos/pytorch/pytorch/git/commits/c7e8044fc8fcafdbff15b13bfd3b6d56cb506fa0
8689d8af369edc45c5f2500fb664d3e4c2bcbc9a,"Format inline code block. (#12441)

Summary:
Signed-off-by: Marcela Morales Quispe <marcela.morales.quispe@gmail.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12441

Differential Revision: D10236743

Pulled By: SsnL

fbshipit-source-id: c0e446a81a388cf6a558bf7ab8ba0e59703dc169",marcemq,https://api.github.com/repos/pytorch/pytorch/git/commits/8689d8af369edc45c5f2500fb664d3e4c2bcbc9a
a3fb004b1829880547dd7b3e2cd9d16af657b869,"(#12474)

Summary:
Modifies the DistributedSampler logic. Now each process samples elements with
a given interval, instead of a consecutive section.

  This eliminates the possibility where the DataLoader uses padded data while
dropping the real data. It happens when:
  1. DistributedSampler padded data; and
  2. DataLoader drops_last is effectively true, and drops less then the number
of padded data.
  from the example down, we see that data (10, 11, 12) are padded through
duplicating data sample (1, 2, 3)
  The old sampler drops legit original data (3, 6, 9) and introduces duplication
(10, 11) into the training set; while the new sampler logic samples correct data
points from the data set.
  This example has been added to dataloader unit test

example:
```
  data after shuffle: 1, 2, 3, 4, 5, 6, 7, 8, 9
  padded data : 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12

  old sampler:       ->  DataLoader with (batch_size=2 and drop_last=True)
   p 1: 1, 2, 3          1, 2
   p 2: 4, 5, 6          4, 5
   p 3: 7, 8, 9          7, 8
   p 4:10,11,12         10,11

  new sampler:       ->
   p 1: 1, 5, 9          1, 5
   p 2: 2, 6,10          2, 6
   p 3: 3, 7,11          3, 7
   p 4: 4, 8,12          4, 8
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12474

Differential Revision: D10260410

Pulled By: SsnL

fbshipit-source-id: 710856571260f42ce25955b81a5b8008e04938cf",jjsjann123,https://api.github.com/repos/pytorch/pytorch/git/commits/a3fb004b1829880547dd7b3e2cd9d16af657b869
2b033332c8b7720b16b7a442311aeb4adfaab84c,"Allow linking to backwards-compatible cuDNN at runtime (#12239)

Summary:
Fixes #12193
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12239

Differential Revision: D10321744

Pulled By: soumith

fbshipit-source-id: bf437f7f9b6231158a1585d2dabae8d937396478",sclarkson,https://api.github.com/repos/pytorch/pytorch/git/commits/2b033332c8b7720b16b7a442311aeb4adfaab84c
1c7832c854a519becad19af4837181881b10f671,"CUDA 10 warnings fixed (#12442)

Summary:
Deprecation warning against `cudaPointerAttributes`, where `memoryType` field has been deprecated in favor of `type`, see https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__UNIFIED.html#contents-end
for details
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12442

Differential Revision: D10251239

Pulled By: zou3519

fbshipit-source-id: 500f1e02aa8e11c510475953ef5244d5fb13bf9e",drnikolaev,https://api.github.com/repos/pytorch/pytorch/git/commits/1c7832c854a519becad19af4837181881b10f671
7acb1458934abd3c44aa6d494cf5a29b0ff37e86,"Fixed print issue for TensorTypeId (#12402)

Summary:
Fixed printing issue for TensorTypeID. It used to print a hex of the ID, e.g.
   /x1
Now it prints the ID as a string, e.g.
  1
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12402

Reviewed By: ezyang

Differential Revision: D10224026

Pulled By: lauragustafson

fbshipit-source-id: a9ca841d08c546fccbb948a17f06a29fea66f3fb",lauragustafson,https://api.github.com/repos/pytorch/pytorch/git/commits/7acb1458934abd3c44aa6d494cf5a29b0ff37e86
038d5ca943d5aa732c94c3af2692db79f30de327,"Remove incompatibility MSVC, Cuda and Debug (#12572)

Summary:
Experimentally this works.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12572

Differential Revision: D10342468

Pulled By: ezyang

fbshipit-source-id: dc36587c32ab0910aa14b7351ca12532acd41c7d",fmder,https://api.github.com/repos/pytorch/pytorch/git/commits/038d5ca943d5aa732c94c3af2692db79f30de327
7da46432328a5c3ebb4bf6cf7147c0ab7b037984,"Caffe2: fix error C2398 and syntax error with Visual Studio 2015 (#10089)

Summary:
Similar fix to [pull #7024](https://github.com/pytorch/pytorch/pull/7024).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10089

Differential Revision: D10363341

Pulled By: ezyang

fbshipit-source-id: bc9160e2ea75fc77acf3afe9a4e20f327469592e",dzung-hoang,https://api.github.com/repos/pytorch/pytorch/git/commits/7da46432328a5c3ebb4bf6cf7147c0ab7b037984
7a1b668283ef80098d00f621803863c492900389,"Implement Tensor.__cuda_array_interface__. (#11984)

Summary:
_Implements pytorch/pytorch#11914, cc: ezyang_

Implements `__cuda_array_interface__` for non-sparse cuda tensors,
providing compatibility with numba (and other cuda projects...).

Adds `numba` installation to the `xenial-cuda9` jenkins test environments via direct installation in `.jenkins/pytorch/test.sh` and numba-oriented test suite in `test/test_numba_integration.py`.

See interface reference at:
https://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11984

Differential Revision: D10361430

Pulled By: ezyang

fbshipit-source-id: 6e7742a7ae4e8d5f534afd794ab6f54f67808b63",asford,https://api.github.com/repos/pytorch/pytorch/git/commits/7a1b668283ef80098d00f621803863c492900389
e986f307c3939bde8ee992038c7a190502ae1a15,"Fix math formatting of PairwiseDistance docs (#12628)

Summary:
`:math:` was being displayed in the docs for https://pytorch.org/docs/stable/nn.html#torch.nn.PairwiseDistance.

I haven't tested this locally, but I assume it works.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12628

Differential Revision: D10373778

Pulled By: SsnL

fbshipit-source-id: 6eb918c521e73c17f6662d83f69e0e4b14dec860",adamjstewart,https://api.github.com/repos/pytorch/pytorch/git/commits/e986f307c3939bde8ee992038c7a190502ae1a15
a1bbe80e2199f81973bbeb8e6f0d40c5707d02e7,"Remove NervanaGPU operators from Caffe2 (#12564)

Summary:
Fix #12540
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12564

Reviewed By: orionr

Differential Revision: D10379775

Pulled By: soumith

fbshipit-source-id: a925b116f2687e56bf54465fc02ca2eb1e7c8eb0",mratsim,https://api.github.com/repos/pytorch/pytorch/git/commits/a1bbe80e2199f81973bbeb8e6f0d40c5707d02e7
bbe6ef38642cda127ece82f984ee3971e8750a45,"torch.finfo and torch.iinfo to mimic the numpy equivalent (#12472)

Summary:
This pull request intends to provide the functionality requested in https://github.com/pytorch/pytorch/issues/10742 by adding a new torch.finfo and torch.iinfo API.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12472

Differential Revision: D10250829

Pulled By: benoitsteiner

fbshipit-source-id: eb22ca55d5b0064bef381fa7f1eb75989977df30",benoitsteiner,https://api.github.com/repos/pytorch/pytorch/git/commits/bbe6ef38642cda127ece82f984ee3971e8750a45
52cbf4b7749a94dad280f96388dd358c96d36cbd,"Update eigen submodule to fix CUDA arch>=5.3 build issue. (#12191)

Summary:
Discussed in #11379, #12545. Eigen submodule needs to be updated to https://github.com/eigenteam/eigen-git-mirror/commit/f59336cee358f92b959de6a0daf07c4ab2318022 to support building with CUDA arch >= 5.3.

It seems there was a similar fix checked in from #6746, but later the Eigen submodule is switched to the current mirror #7793 at a point the fix was not included.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12191

Differential Revision: D10362557

Pulled By: ezyang

fbshipit-source-id: 548541e2c93f412bf6680ee80b8da572846f80d2",BowenBao,https://api.github.com/repos/pytorch/pytorch/git/commits/52cbf4b7749a94dad280f96388dd358c96d36cbd
7a521177926ce0534673d13ea908709d902cb0d6,"Add AdaptiveAvgPool2d and AdaptiveMaxPool2d to ONNX.symbolic (#9711)

Summary:
Add AdaptiveAvgPool2d and AdaptiveMaxPool2d to ONNX.symbolic
Due to limitations in ONNX only output_size=1 is supported.
AdaptiveAvgPool2d -> GlobalAveragePool
AdaptiveMaxPool2d -> GlobalMaxPool
Fixes #5310
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9711

Differential Revision: D10363462

Pulled By: ezyang

fbshipit-source-id: ccc9f8ef036e1e54579753e50813b09a6f1890da",ksemianov,https://api.github.com/repos/pytorch/pytorch/git/commits/7a521177926ce0534673d13ea908709d902cb0d6
23c4dbd6d7f672945c58dd02e18b0051a9e89591,"Fix ONNX upsample mode (#12648)

Summary:
Fixes #12647
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12648

Differential Revision: D10389124

Pulled By: houseroad

fbshipit-source-id: 53bc17b592d0d7f1884b555f3a12a33dbf18b4a0",ahirner,https://api.github.com/repos/pytorch/pytorch/git/commits/23c4dbd6d7f672945c58dd02e18b0051a9e89591
5416260b1e64e49136cf90396e9c7c95b8449fb8,"Add the OpenMP optimization for BatchPermutation. (#12153)

Summary:
This is for Caffe2 optimization.
WIth this optimization, the following two ops can boost a lot. (Test with MaskRCNN, on SKX8180 one socket)
BatchPermutation op: reduced from 8.296387 ms to 1.4501984 ms.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12153

Differential Revision: D10362823

Pulled By: ezyang

fbshipit-source-id: 04d1486f6c7db49270992cd8cde41092154e62ee",ChongyuIntel,https://api.github.com/repos/pytorch/pytorch/git/commits/5416260b1e64e49136cf90396e9c7c95b8449fb8
90737f7f5d66d38426890540ce388e5a34247695,"Fix missing final activation in NLLLoss second example (#12703)

Summary:
Fixed the second example in NLLLoss.
The LogSoftmax activation was missing after the convolution layer. Without this activation, the second example loss was sometimes negative.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12703

Differential Revision: D10419694

Pulled By: ezyang

fbshipit-source-id: 98bfefd1050290dd5b29d3ce18fe075103db4674",grjhuard,https://api.github.com/repos/pytorch/pytorch/git/commits/90737f7f5d66d38426890540ce388e5a34247695
1a6071d436c63cb9174b07726d302de70989ec0b,"fixing `seq` to `tensors` in documentation (#12741)

Summary:
Fixes #12251

In the docs the actual key word argument was supposed to be `tensors` but instead it is given as `seq` for doing `torch.cat` operation.

zou3519 can you review this code? I don't have access to request for code reviews.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12741

Differential Revision: D10419682

Pulled By: ezyang

fbshipit-source-id: a0ec9c3f4aeba23ac3a99e2ae89bd07d2b9ddb58",greed2411,https://api.github.com/repos/pytorch/pytorch/git/commits/1a6071d436c63cb9174b07726d302de70989ec0b
cffeb03a2db7dd1b1f4542a0b7050cce370a9053,"fix forward and backward for norm with negative infinity norm (#12722)

Summary:
I found a bug in norm() and fixed it (and added tests to make sure it's fixed)
here is how to reproduce it:
```python
import torch
x = torch.FloatTensor([[10, 12, 13], [4, 0, 12]])
print(torch.norm(x, -40, dim=0, keepdim=True)) #output is tensor([[ 4.0000,  0.0000, 11.9853]])
print(torch.norm(x, float('-inf'), dim=0, keepdim=True)) #output is tensor([[1., 1., 1.]]) which is wrong!
from numpy.linalg import norm as np_norm
x = x.numpy()
print(np_norm(x, ord=-40, axis=0)) #output is array([[4., 0., 11.985261]])
print(np_norm(x, ord=float('-inf'), axis=0)) #output is array([[4., 0., 12.0]])
```
it's related to [#6817](https://github.com/pytorch/pytorch/issues/6817) and [#6969](https://github.com/pytorch/pytorch/pull/6969)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12722

Differential Revision: D10427687

Pulled By: soumith

fbshipit-source-id: 936a7491d1e2625410513ee9c39f8c910e8e6803",Separius,https://api.github.com/repos/pytorch/pytorch/git/commits/cffeb03a2db7dd1b1f4542a0b7050cce370a9053
3fe35300eda292019190cc4ca602304a974ebd71,"Revert D10417038: [pytorch][PR] Use C locale in lexer

Differential Revision:
D10417038

Original commit changeset: 1d5f2f9a24ec

fbshipit-source-id: 5780fed8e29551ec5b0a56ad6966a560c02bc171",mkatsevVR,https://api.github.com/repos/pytorch/pytorch/git/commits/3fe35300eda292019190cc4ca602304a974ebd71
63cd051867b830ec62ecdf16da319e4522c506fd,"Guard all Caffe2 protobuf string serializations with CAFFE_ENFORCE (#12799)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12799

Updated all non-test uses of protobuf::MessageLite::SerializeAsString to call
SerializeAsString_EnforceCheck so that the return value is checked and can
throw an exception if failing.

Most of the affected code was called from classes derived from  BlobSerializeBase.
Didn't touch most tests and ENFORCE calls because they usually do checks
anyway.

Reviewed By: ezyang

Differential Revision: D10416438

fbshipit-source-id: cb842e3e26b0918829d71267a375d4dd40600d58",mike7ant,https://api.github.com/repos/pytorch/pytorch/git/commits/63cd051867b830ec62ecdf16da319e4522c506fd
89bf98ac4ce84178c4144419776076898b6fda13,"Update '__all__' in '__init.py__' (#12762)

Summary:
It's the best coding practice to always include dynamically declared module level methods in the ""__all__"" field. Otherwise,  IDEs (such as PyCharm) with referenced module inspectors will complain  ""Cannot find reference ..."" .

This PR adds 'rand' and 'randn' in __init.py__' .
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12762

Differential Revision: D10427541

Pulled By: ezyang

fbshipit-source-id: ec0704dfd91e78d7ad098b42cfd4bd1ad0e119df",x1angli,https://api.github.com/repos/pytorch/pytorch/git/commits/89bf98ac4ce84178c4144419776076898b6fda13
40ff69b796f44660319fcf5026c84ca82c60d299,"Add attribute exhaustive_search in caffe2 blacklist args (#12815)

Summary:
Currently while converting from caffe2 to onnx it doesn't
    blacklist the exhaustive_search attribute in support_onnx_export.
    So conversion fails when onnx model is verified using C.check_model.

Signed-off-by: Parth Raichura <parth.raichura@softnautics.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12815

Differential Revision: D10457777

Pulled By: ezyang

fbshipit-source-id: dc2183d8abef8cd753b348f2eaa62c952a058920",parthr-sib,https://api.github.com/repos/pytorch/pytorch/git/commits/40ff69b796f44660319fcf5026c84ca82c60d299
373b5080dacc4c4fa958e36182f8c41a0a9dc425,"Warn that tensor.resize_() resets strides (#12816)

Summary:
As discussed in #1570, this adds a warning to the docstring of `tensor.resize_()` to prevent people from naively using it as an in-place view or reshape.

For your convenience, the updated docstring renders as follows:
![torch_resize_docstring](https://user-images.githubusercontent.com/629706/47148782-f1b57900-d2d1-11e8-9749-e9c7387113ed.png)

Fixes #1570.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12816

Differential Revision: D10457755

Pulled By: ezyang

fbshipit-source-id: dd4b3a821e8c76dc534d81c53084abdb336e690a",f0k,https://api.github.com/repos/pytorch/pytorch/git/commits/373b5080dacc4c4fa958e36182f8c41a0a9dc425
6190408e2418c05ba03097aa83fb05721c0e1cf8,"caffe2: UpsampleBilinear support for scales (#12736)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12736

This updates UpsampleBilinearOp and UpsampleBilinearGradientOp to support scales to bring it inline with ResizeNearestOp https://github.com/pytorch/pytorch/pull/12720.

Reviewed By: houseroad

Differential Revision: D10416228

fbshipit-source-id: f339b7e06979c9c566afb4cee64a2d939b352957",d4l3k,https://api.github.com/repos/pytorch/pytorch/git/commits/6190408e2418c05ba03097aa83fb05721c0e1cf8
c774cb8913fac5eb0291dfd03771ae785d215c70,"Rephrase unclear error message for shape mismatch (#12870)

Summary:
I spent a couple of minutes trying to understand which shape corresponds to checkpoint and which one to the model
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12870

Differential Revision: D10466600

Pulled By: SsnL

fbshipit-source-id: 3b68530b1b756462a2acd59e3a033ff633567a6b",Randl,https://api.github.com/repos/pytorch/pytorch/git/commits/c774cb8913fac5eb0291dfd03771ae785d215c70
e64f75a1d8bec5547ba81463c1a14f0a437a8ad4,"fix ZeroDivisionError in utils.bottleneck (#11987)

Summary:
**ZeroDivisionError** occurs when `cuda_prof_exec_time` is small enough.
This situation is normal for a project that has little CUDA work.

Or someone does not make his work transferred to CUDA successfully. In this time he profiles the code, this error occurs.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11987

Differential Revision: D10488568

Pulled By: soumith

fbshipit-source-id: db8c1e9e88a00943c100958ebef41a1cb56e7e65",egg-west,https://api.github.com/repos/pytorch/pytorch/git/commits/e64f75a1d8bec5547ba81463c1a14f0a437a8ad4
1b07eb714876ddc047ba596f3eaf5283d410895c,"torch.utils.cpp_extension.verify_ninja_availability() does not return True as documented

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/12922

Differential Revision: D10502167

Pulled By: ezyang

fbshipit-source-id: 2e32be22a310e6e014eba0985e93282ef5764605",jat001,https://api.github.com/repos/pytorch/pytorch/git/commits/1b07eb714876ddc047ba596f3eaf5283d410895c
21285e73da4d15a8e5ffcb1a6b523feefd226755,"Add Google pixel code

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/12998

Differential Revision: D10515096

Pulled By: JoelMarcey

fbshipit-source-id: 7f97014451448a70ea7f91d7d8bd96fbf6e83f7f",patmellon,https://api.github.com/repos/pytorch/pytorch/git/commits/21285e73da4d15a8e5ffcb1a6b523feefd226755
821b04e8198096b60188fc86ed1a23320c599014,"Nomnigraph: Remove Copy constructor and copy assign operator from BasicBlock, add move constructor.

Summary:
We cannot use copying as it loses recorded callbacks and thus after copying
tracked values are no longer tracked.

Reviewed By: bwasti, duc0

Differential Revision: D10510057

fbshipit-source-id: b64fdef3fb28fc26fe55eba41f4b5007ba6894de",ZolotukhinM,https://api.github.com/repos/pytorch/pytorch/git/commits/821b04e8198096b60188fc86ed1a23320c599014
569a29b81a51cbe5f1f4112f0ec2614a0915581f,"Make chunk size configurable in SaveOp (#12949)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12949

Currently the default chunk size in save operation is 1MB and I don't find a way to configure it at runtime. Add a parameter to configure chunk size in SaveOp.

Reviewed By: mraway, xsh6528

Differential Revision: D10454037

fbshipit-source-id: a5cd8f9846aea4b1e3612a3fcfa431b68bda8104",TailofJune,https://api.github.com/repos/pytorch/pytorch/git/commits/569a29b81a51cbe5f1f4112f0ec2614a0915581f
1bec8f773b87ca95dee0f6deae664bb04b8b541b,"Move ConstantPadNd into ATen (#10885)

Summary:
Addresses #9499. Completed work on the forward function, tests should be passing for that. Working on backward function now.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10885

Differential Revision: D9643786

Pulled By: SsnL

fbshipit-source-id: 2930d6f3d2975c45b2ba7042c55773cbdc8fa3ac",wdhorton,https://api.github.com/repos/pytorch/pytorch/git/commits/1bec8f773b87ca95dee0f6deae664bb04b8b541b
33b00bdbb81b95ec5d49185df0b9919ab7d1a4b7,"cwd arg in shell function of run_test set to optional (#13247)

Summary:
Tiny fix.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13247

Differential Revision: D12830311

Pulled By: soumith

fbshipit-source-id: 405620e3a1de5bfc7e039f9aaf2f7cb7a3bca1b1",verhoek,https://api.github.com/repos/pytorch/pytorch/git/commits/33b00bdbb81b95ec5d49185df0b9919ab7d1a4b7
9d9e5f8d1e62129f2c2a30c8bb1ace2cce6325a1,"Solve bug of DistributedDataParallel (#13248)

Summary:
Fixed bug [https://github.com/facebookresearch/maskrcnn-benchmark/issues/52](https://github.com/facebookresearch/maskrcnn-benchmark/issues/52)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13248

Reviewed By: pietern

Differential Revision: D12830451

Pulled By: teng-li

fbshipit-source-id: ab33faf3f6f4545f8fe07da7ecbeb2f0a2ea23f0",unlimblue,https://api.github.com/repos/pytorch/pytorch/git/commits/9d9e5f8d1e62129f2c2a30c8bb1ace2cce6325a1
518b0d06008b286aafbfa94122981e5b8e695f8b,"Fix add out=None to digamma docstring (Fixes #13225) (#13307)

Summary:
Fixes #13225
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13307

Differential Revision: D12840231

Pulled By: SsnL

fbshipit-source-id: 2732a2466ac1d2f3fdabfd1eaccddec96e89ba1b",ragulpr,https://api.github.com/repos/pytorch/pytorch/git/commits/518b0d06008b286aafbfa94122981e5b8e695f8b
4d141bee9855bbc9f153689d5377bd313f8fb743,"Skip test_sum_noncontig in ROCm (#13341)

Summary:
Since it fails due to insufficient precision for DoubleTensor .sum() on ROCm
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13341

Differential Revision: D12851335

Pulled By: bddppq

fbshipit-source-id: e211c3868b685aa705160ce98a2a18a915ad493f",jithunnair-amd,https://api.github.com/repos/pytorch/pytorch/git/commits/4d141bee9855bbc9f153689d5377bd313f8fb743
a25d3b4d8c632589828e923df8f5328084e9b833,"Use byte tensor for mnist labels. (#13363)

Summary:
The C++ mnist example https://github.com/goldsborough/examples/blob/cpp/cpp/mnist/mnist.cpp
 does not work because the labels are not correctly loaded.  Currently it achieves 100 % accuracy.  Specifying byte dtype fixes the issue.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13363

Differential Revision: D12860258

Pulled By: goldsborough

fbshipit-source-id: ad7b9256e4fc627240e25c79de9d47b31da18d38",serega,https://api.github.com/repos/pytorch/pytorch/git/commits/a25d3b4d8c632589828e923df8f5328084e9b833
479b8266bf7c31e48a1e0188266e313377fff450,"Back out ""[pytorch][PR] Support upsample"" (#13413)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13413

Original commit changeset: d5db200365f1

Reviewed By: houseroad

Differential Revision: D12870356

fbshipit-source-id: be115d2370636786901c822895664ccace2a9bc2",zrphercule2,https://api.github.com/repos/pytorch/pytorch/git/commits/479b8266bf7c31e48a1e0188266e313377fff450
54d63c5752d1be64ba3b9ec0dfb4497462d78659,added fbgemm as submodule (#13354),dskhudia,https://api.github.com/repos/pytorch/pytorch/git/commits/54d63c5752d1be64ba3b9ec0dfb4497462d78659
d843f63f2a49c339fe79c3016a082c60fbf3fed5,"optimization on cpu conv3d (#11884)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11884

In cpu mode, current convNd uses Im2ColNdNCHWImpl, which is generic implementation to handle convolutional layer for arbitrary number of dimensions. In video modeling, we use convNd for filter dimension=3.

The problem of current convNd is that Im2ColNdNCHWImpl is much slower than Im2Col used by conv2d for the filters with same Flops. For example, a (1, 7, 7) 3d filter takes 5 times longer than a (7, 7) 2d filter at inference time.

This diff extends Im2Col to 3d case (Im2Col3dNCHWImpl), and this optimization for 3d convolution gives 4~5 times faster inference time on cpu for various video models:

{F128300920}

i-am-not-moving-c2-to-c10

Reviewed By: BIT-silence

Differential Revision: D8245940

fbshipit-source-id: 75231d65c9dd56059dfe31701e26021fd1ff2a85",feiyu1990,https://api.github.com/repos/pytorch/pytorch/git/commits/d843f63f2a49c339fe79c3016a082c60fbf3fed5
3d392cc5ecf9885e2bf24b9214bcf20525cdc5d4,"Migrate dnnlowp code to open source directory (#13500)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13500

This diff migrate dnnlowp related files and operators from deeplearning/quantization/caffe2 and deeplearning/quantization/dnnlowp to the open source directory.

Reviewed By: jspark1105

Differential Revision: D10842192

fbshipit-source-id: 53d0666d0ae47a01db9c48114345d746b0a4f11f",hx89,https://api.github.com/repos/pytorch/pytorch/git/commits/3d392cc5ecf9885e2bf24b9214bcf20525cdc5d4
cc3cecdba0cd74cdd7c0cb3d97f70c6f81bebf14,"Fix the bug when compile using nvcc compiler. (#13509)

Summary:
I found a bug about compiling the cuda file when I install maskrcnn-benchmark lib.

`python setup.py build develop` will throw the error:
```
  File ""/usr/local/lib/python2.7/dist-packages/torch/utils/cpp_extension.py"", line 214, in unix_wrap_compile
    original_compile(obj, src, ext, cc_args, cflags, pp_opts)
  File ""/usr/lib/python2.7/distutils/unixccompiler.py"", line 125, in _compile
    self.spawn(compiler_so + cc_args + [src, '-o', obj] +
TypeError: coercing to Unicode: need string or buffer, list found
```

For more information, please see [issue](https://github.com/facebookresearch/maskrcnn-benchmark/issues/99).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13509

Differential Revision: D12902675

Pulled By: soumith

fbshipit-source-id: b9149f5de21ae29f94670cb2bbc93fa368f4e0f7",GuoxiaWang,https://api.github.com/repos/pytorch/pytorch/git/commits/cc3cecdba0cd74cdd7c0cb3d97f70c6f81bebf14
d03c6ba50d4e7cb16fc595211b5d061dd703b5d8,"Adding Fetching Real number representation

Summary: Adding Fetching Real number representation for int8 tensor in workpace.py

Reviewed By: harouwu

Differential Revision: D12936556

fbshipit-source-id: f8756a37bce21c93d44d52faf5da9c9bd6473f4a",bigrabithong,https://api.github.com/repos/pytorch/pytorch/git/commits/d03c6ba50d4e7cb16fc595211b5d061dd703b5d8
76c1b5cd794c44e4fec8da1d87ec8f0ccc045e68,"Fix overflow error in stats_put_ops

Summary:
I was hitting this error:

caffe2/caffe2/operators/stats_put_ops.h:66:25: runtime error: 9.22337e+18 is outside the range of representable values of type 'long'

So, the assignment from int64_t to float loses some precision and because of that we overflow.

Reproduced this issue with this diff D12945013

Reviewed By: mlappelbaum, jdshi-fb

Differential Revision: D12927086

fbshipit-source-id: 7eae7fe25ab49d5ac15279335bd5b1fa89d6e683",pradeepdfb,https://api.github.com/repos/pytorch/pytorch/git/commits/76c1b5cd794c44e4fec8da1d87ec8f0ccc045e68
a132a7d9ce4fef1d5057c66e136935a9b9379bbf,"Add autodiff support for a few additional operators (#13288)

Summary:
Added aten::{avg_pool2d, log_softmax, max_pool2d_with_indices, threshold},
enabled aten::{expand, view}.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13288

Differential Revision: D12954929

Pulled By: soumith

fbshipit-source-id: 6fba58af82cafbc7446705d8c8145cdeaf4954ca",asuhan,https://api.github.com/repos/pytorch/pytorch/git/commits/a132a7d9ce4fef1d5057c66e136935a9b9379bbf
674e23bbab121ddebd60f43f8e80839b2ee84e51,"Fixed a small error in docstrings for ConvTranspose3d (#13668)

Summary:
In the example for ConvTranspose3d, the docstring had ""Conv3d"" instead of ""ConvTranspose3d"" in one instance.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13668

Differential Revision: D12958372

Pulled By: soumith

fbshipit-source-id: 5ec901e20b90f4eed2bf04c5b417183ec2096447",alekhka,https://api.github.com/repos/pytorch/pytorch/git/commits/674e23bbab121ddebd60f43f8e80839b2ee84e51
9900a8dd8953dd2516ef2224f45febcda30b7d3e,"Remove outdated css and font files in html docs (#13699)

Summary:
The stylesheet at docs/source/_static/css/pytorch_theme.css is no longer necessary for the html docs build. The new html docs theme styles are located at https://github.com/pytorch/pytorch_sphinx_theme.

The Lato font is also no longer used in the new theme.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13699

Differential Revision: D12967448

Pulled By: soumith

fbshipit-source-id: 7de205162a61e3acacfd8b499660d328ff3812ec",brsoff,https://api.github.com/repos/pytorch/pytorch/git/commits/9900a8dd8953dd2516ef2224f45febcda30b7d3e
de41d1ae0bcd0ac9270676c4a2ecf6abc5ca18e9,"Enable junk fill for the default CPU allocator (#13377)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13377

* Enable junk fill for the default CPU allocator. The first diff only enables this for the tests. A second diff will change the default of zero-fill to false.
* Fix tests to use 64-bit counters that IterOp and LearningRateOp demands.
* Fix kernels that uses uninitialized memory.

Reviewed By: salexspb

Differential Revision: D10866512

fbshipit-source-id: 17860e77e63a203edf46d0da0335608f77884821",zheng-xq,https://api.github.com/repos/pytorch/pytorch/git/commits/de41d1ae0bcd0ac9270676c4a2ecf6abc5ca18e9
51f58f09903fe45e5a3b98863cc53efc7d621d5d,"Fix typo in CTC loss doc comments. (#13727)

Summary:
`target_lenghts` -> `target_lengths`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13727

Differential Revision: D12981582

Pulled By: zou3519

fbshipit-source-id: e5e02b26cf3030a91494655ff863273333cc4133",dan-zheng,https://api.github.com/repos/pytorch/pytorch/git/commits/51f58f09903fe45e5a3b98863cc53efc7d621d5d
55964abb1182e489533d63a53a2a34c7352bf4a2,"Change all namespace fbgemm in the old fbgemm to namespace fbgemm0 (#13701)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13701

We would like to rename the old fbgemm to â€œfbgemm0â€, and the new fbgemm2 to â€œfbgemmâ€:

This DIFF changes all namespace fbgemm to namespace fbgemm0.

Reviewed By: jspark1105

Differential Revision: D12848727

fbshipit-source-id: 47935e9e2c4714a7ce1bfc3f7e4d6a334130132e",jianyuh,https://api.github.com/repos/pytorch/pytorch/git/commits/55964abb1182e489533d63a53a2a34c7352bf4a2
8752214fb7534d3d3f83c1a459f24c57db86cd10,"Apply weight-decay before momentum in the SGD optimizer. (#13801)

Summary:
While trying to understand why two implementations of the same model, one in Python, one using the C++ api (via some [ocaml wrappers](https://github.com/LaurentMazare/ocaml-torch)) did not perform equally well, I noticed that the Python and C++ implementation of SGD slightly differ on weight decay.

- In the [Python version](https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py#L91-L93) weight decay is applied *before* momentum (and so momentum applies to the weight decay).
- In the C++ implementation the weight decay is applied *after* momentum.

In the couple computer-vision models I have looked at the Python version performs a little better so this PR tweaks the C++ implementation to perform weight-decay *before* momentum. This is possibly caused by having more regularization - maybe increasing the weight decay while keeping the current code would hold the same improvements however a nice advantage of this change is to put the C++ and Python version in line. After this change my Python and C++/ocaml models performed similarly when using the same weight-decay parameter.

Maybe there was some real reason to have weight decay after momentum in the C++ version but I haven't found any.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13801

Differential Revision: D13020709

Pulled By: soumith

fbshipit-source-id: 7c2ac245577dd04bc3728aec4af0477120a60f13",LaurentMazare,https://api.github.com/repos/pytorch/pytorch/git/commits/8752214fb7534d3d3f83c1a459f24c57db86cd10
f112aa746a3200fb12737ee6cc96ac6c2b5ec725,"Fix document about torch.get_default_dtype() (#13890)

Summary:
Minor fix.
```
torch.get_default_dtype() â†’ :class:`torch.dtype`
```
â†’
```
torch.get_default_dtype() â†’ torch.dtype
```
:class: is not rendered in https://pytorch.org/docs/stable/torch.html
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13890

Differential Revision: D13040704

Pulled By: colesbury

fbshipit-source-id: 5fadb01ad365042d5df2bac058f4ae89b281d3b7",taekinkim,https://api.github.com/repos/pytorch/pytorch/git/commits/f112aa746a3200fb12737ee6cc96ac6c2b5ec725
f6e4fc071a63b8ffe89bbea892e97c598f008120,"Fix a bug that causes nvcc to emit an unknown option error (#13904)

Summary:
Using `""-Xcompiler -fPIC""` causes nvcc to emit the following:

    nvcc fatal   : Unknown option 'Xcompiler -fPIC'

As per fixes lower down in the file (see also issue #7126 on GitHub),
the fix is to replace it with `""-Xcompiler"" ""-fPIC""`. This one was
apparently missed when the original fix was applied.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13904

Differential Revision: D13043189

Pulled By: soumith

fbshipit-source-id: 6dc6d325671e4d08cd8e6242ffc93b3bd1f65351",sgolodetz,https://api.github.com/repos/pytorch/pytorch/git/commits/f6e4fc071a63b8ffe89bbea892e97c598f008120
0bedaf9cf6c2f393df3b505ba45c350b5e83fc86,"Update setup.py to support Nvidia TX2 (#13939)

Summary:
add platform.machine() == 'aarch64' for supporting Nvidia TX2
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13939

Differential Revision: D13055834

Pulled By: soumith

fbshipit-source-id: 0fadc87adf9e6b796978ce743e824eb98b006856",jario-jin,https://api.github.com/repos/pytorch/pytorch/git/commits/0bedaf9cf6c2f393df3b505ba45c350b5e83fc86
35a24a9a948d5a41a6e9c8c42e9c88cb427146d3,"Example with edge case 0 for torch.sign (#13771)

Summary:
The behavior of the edge case 0 is not self-evident for the `torch.sign` function ( I personally expected a result of 1):
```python
>>> a = torch.tensor([0.7, -1.2, 0., 2.3])
>>> a
tensor([ 0.7000, -1.2000,  0.0000,  2.3000])
>>> torch.sign(a)
tensor([ 1., -1.,  0.,  1.])
```
This is not currently documented, I think it is worth it to give a simple example showing this behavior.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13771

Differential Revision: D13044520

Pulled By: ailzhang

fbshipit-source-id: c3011ccbdf1c13348f6c7242b06a9aa52ebc9204",lberrada,https://api.github.com/repos/pytorch/pytorch/git/commits/35a24a9a948d5a41a6e9c8c42e9c88cb427146d3
c5afad5579c2e932066ca33b639527ce32b25f89,"Fix skip logic in caffe_translator_test.py (#13627)

Summary:
Avoid false failure by checking for the presence of the test data in setup.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13627

Differential Revision: D13090324

Pulled By: ezyang

fbshipit-source-id: e85571943d168c0007212d7b1a5b99ffa0c39235",mtbrandy,https://api.github.com/repos/pytorch/pytorch/git/commits/c5afad5579c2e932066ca33b639527ce32b25f89
1b1cdd944c73a45c483afc6fc5c8ee2bdc9fa271,"Keep `ModuleList` consistent with python `list` in `__setitem__` function. (#13102)

Summary:
`ModuleList` class function `__setitem__` has implicit rist
```
In [26]: mlist = nn.ModuleList([nn.ReLU(), nn.Conv2d(10, 10, 3, 1)])

In [27]: mlist
Out[27]:
ModuleList(
  (0): ReLU()
  (1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))
)

In [28]: mlist[-1] = nn.ReLU()

In [29]: mlist
Out[29]:
ModuleList(
  (0): ReLU()
  (1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))
  (-1): ReLU()
)

In [30]: mlist[-1]
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-30-229d1b6823a0> in <module>()
----> 1 mlist[-1]

~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py in __getitem__(self, idx)
    134             return ModuleList(list(self._modules.values())[idx])
    135         else:
--> 136             return self._modules[self._get_abs_string_index(idx)]
    137
    138     def __setitem__(self, idx, module):

KeyError: '2'

```

modified as
```
    def __setitem__(self, idx, module):
        idx = self._get_abs_string_index(idx)
        return setattr(self, str(idx), module)
```
to fix it.

```
In [31]: class NewModuleList(nn.ModuleList):
    ...:     def __setitem__(self, idx, module):
    ...:         idx = self._get_abs_string_index(idx)
    ...:         return setattr(self, str(idx), module)
    ...:

In [32]: mlist = NewModuleList([nn.ReLU(), nn.Conv2d(10, 10, 2, 1)])

In [33]: mlist[-1] = nn.ReLU()

In [34]: mlist
Out[34]:
NewModuleList(
  (0): ReLU()
  (1): ReLU()
)
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13102

Differential Revision: D13092480

Pulled By: ezyang

fbshipit-source-id: 7ff7688f66e44bbd263a10d2d09db7bb0df4b749",lyuwenyu,https://api.github.com/repos/pytorch/pytorch/git/commits/1b1cdd944c73a45c483afc6fc5c8ee2bdc9fa271
8e91da4cb3d645b178b515bab54331c917340cd5,"Windows shared build (#13550)

Summary:
Hi guys,

I'd like to build Caffe2 with more supported options in Windows with Microsoft Visual Studios.
This is the first pull request.
Running scripts/build_windows_shared.bat is able to build Caffe2 with both CMAKE_BUILD_TYPE=Debug and CMAKE_BUILD_TYPE=Release with Visual Studio 14 2015.
CUDA is 9.0, cudnn is 7.0.5, glog, gflags and lmdb are supported on my system.
Python is 3.5, Detectron works from python interface as well.
It was even possible to debug detectron code and step into caffe2_gpu.dll with pdbs built.

What is disappointing, that c10/experimental ops don't build with this Visual Studio generator, I added special option INCLUDE_EXPERIMENTAL_C10_OPS (default ON) to deal with it in build_windows_shared.bat.

After this pull request the next step is to add Visual Studio 2017 support in the script.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13550

Reviewed By: ezyang

Differential Revision: D13042597

Pulled By: orionr

fbshipit-source-id: f313f909f599cd582a1d000eff766eef3a9fc4fc",ArutyunovG,https://api.github.com/repos/pytorch/pytorch/git/commits/8e91da4cb3d645b178b515bab54331c917340cd5
1224ef9ea18279e8061547ae5725acb2bbf0c609,"Delete backwards compatibility StorageImpl.h and TensorImpl.h (#14230)

Summary:
Since they directly include the real ones in core.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14230

Differential Revision: D13140323

Pulled By: tugrulates

fbshipit-source-id: d7e3b94e891b2d7fa273d01c0b7edfebdbd7e368",tugrulates,https://api.github.com/repos/pytorch/pytorch/git/commits/1224ef9ea18279e8061547ae5725acb2bbf0c609
44cb43bcc168a559040beb20b266bc2806d30385,"Jaliyae/samplers (#13870)

Summary:
Make Samplers optionally accept new size in their reset() method. This helps dataloader or dataset to reset the sampler for an epoch or a chunk of data with different sizes.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13870

Differential Revision: D13240120

Pulled By: soumith

fbshipit-source-id: 19c53f8be13c0fdcf504f0637b0d3e6009a8e599",jaliyae,https://api.github.com/repos/pytorch/pytorch/git/commits/44cb43bcc168a559040beb20b266bc2806d30385
7c24a16f8231c1eb4d4ba80fc7f6a279263a5b62,"Fixed typo for BCEWithLogitLoss doc comments (#14532)

Summary:
The math symbol was missing a prefix `:`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14532

Differential Revision: D13256077

Pulled By: soumith

fbshipit-source-id: 2359819d8aa664f915be1c436cbb0c0756504028",heidmotron,https://api.github.com/repos/pytorch/pytorch/git/commits/7c24a16f8231c1eb4d4ba80fc7f6a279263a5b62
bdaa0e38b8a6b6de1e7e8a964e1cc07f4850b154,"Fix tautological-compare in aten/src/ATen/native/cuda/SummaryOps.cu (#14540)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14540

refactor the HANDLE_SWITCH_CASE to avoid tautological-compare in macro

Reviewed By: ezyang

Differential Revision: D13255725

fbshipit-source-id: cfa64bb7bc53d19c93a693015202f207567690b4",lhuang04,https://api.github.com/repos/pytorch/pytorch/git/commits/bdaa0e38b8a6b6de1e7e8a964e1cc07f4850b154
dc7498c84d8a0090731fd1565baa489d345f7bdb,"add gloo support for reduce on GPU (#14443)

Summary:
as titled
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14443

Reviewed By: pietern

Differential Revision: D13222907

Pulled By: janewangfb

fbshipit-source-id: f418c5d84880196f97089114d02957cf739243f8",janewangfb,https://api.github.com/repos/pytorch/pytorch/git/commits/dc7498c84d8a0090731fd1565baa489d345f7bdb
ff91de43de9e27c3482cc10b12be0a7fcebc54e1,"Set output of aten::mm to have the same output type as the original node after op canonicalization. (#14602)

Summary:
In CanonalizeOp, addmm is separated into mm and add. But output dimension and type are not preserved for the aten::mm node. Fixing this so that the dumped graph after this pass contains accurate information.
sample output:
before:
%6 : Dynamic = aten::mm(%input.2, %5), scope: LinearModel/Sequential[model]/Linear[full0]
after:
%6 : Float(32, 200) = aten::mm(%input.2, %5), scope: LinearModel/Sequential[model]/Linear[full0]
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14602

Differential Revision: D13273754

Pulled By: soumith

fbshipit-source-id: 82e22b5f30e9eb6ba9249c5a2216955421f39cc7",Tixxx,https://api.github.com/repos/pytorch/pytorch/git/commits/ff91de43de9e27c3482cc10b12be0a7fcebc54e1
da2c3afa475cd496b561eb3c0b008973b67542fe,"Fixed typo in README.md (#14346)

Summary:
Fixed the typo in the Docker image section of README.md file
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14346

Differential Revision: D13290403

Pulled By: soumith

fbshipit-source-id: 1d848027a773f0cfc875c33d69a66e96abc7ac8b",ravivats,https://api.github.com/repos/pytorch/pytorch/git/commits/da2c3afa475cd496b561eb3c0b008973b67542fe
5ed9dfad982a4e20a51fdf30d3620e76b87ecff4,"Replace at::Half non-vectorized conversions with implementations from FP16 (#14411)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14411
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14579

Folded the fp16 codes into c10.

Reviewed By: ezyang

Differential Revision: D13206450

fbshipit-source-id: 472208dd230dc49d33935622ff3286b17eeb0894",chandlerzuo,https://api.github.com/repos/pytorch/pytorch/git/commits/5ed9dfad982a4e20a51fdf30d3620e76b87ecff4
82903dda9b13f982b9fa9d6a4e34e668fb30c561,"Fixes for some Windows compiler warnings (#14490)

Summary:
Implement some simple fixes to clean up windows build by fixing compiler warnings. Three main types of warnings were fixes:

1. GCC specific pragmas were changed to not be used on windows.
2. cmake flags that don't exist on windows were removed from windows build
3. Fix a macro that was defined multiple times on Windows.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14490

Differential Revision: D13241988

Pulled By: ezyang

fbshipit-source-id: 38da8354f0e3a3b9c97e33309cdda9fd23c08247",vaeksare,https://api.github.com/repos/pytorch/pytorch/git/commits/82903dda9b13f982b9fa9d6a4e34e668fb30c561
5e307bd1be48665ea7fc7c8d0efefb7ca91c8bb9,"use ""Extension"" instead of the unimported ""setuptools.Extension"" (#14475)

Summary:
use ""Extension"" instead of the unimported ""setuptools.Extension""
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14475

Differential Revision: D13356219

Pulled By: ezyang

fbshipit-source-id: 5a3e7eb73a32d6bf09676efd9eddded5586435cd",intHbl,https://api.github.com/repos/pytorch/pytorch/git/commits/5e307bd1be48665ea7fc7c8d0efefb7ca91c8bb9
12addc64a6a6787c224e4d96057a797ca7de1535,"Fixed MIOpen RNN Segfault issue and enabled RNN test (#14810)

Summary:
This pull request contains changes for:
1. Added MIOpen RNN API miopenGetRNNLayerBiasSize and miopenGetRNNLayerParamSize.
2. Fixed usage of API miopenGetRNNLayerParam.
3. Modifying the RNN test to run using MIOpen engine.

Differential Revision: D13355699

Pulled By: bddppq

fbshipit-source-id: 6f750657f8049c5446eca893880b397804120b69",lcskrishna,https://api.github.com/repos/pytorch/pytorch/git/commits/12addc64a6a6787c224e4d96057a797ca7de1535
eb3cabffd69e37162a3fe0bb1bbfa3de83404f3a,"Consistent formatting in losses' docs

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/14739

Differential Revision: D13356143

Pulled By: ezyang

fbshipit-source-id: 9ae8316dd8ba6e910247b64cec22db63df10e11c",RicCu,https://api.github.com/repos/pytorch/pytorch/git/commits/eb3cabffd69e37162a3fe0bb1bbfa3de83404f3a
81dc78d871b9b14df6de39bffc048a487b562758,"Update pooling.py (#14998)

Summary:
Strange line in the documentation.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14998

Differential Revision: D13413235

Pulled By: soumith

fbshipit-source-id: 80d05ec1185719b785f0aac914bc2369c1174f2f",paland3,https://api.github.com/repos/pytorch/pytorch/git/commits/81dc78d871b9b14df6de39bffc048a487b562758
c2a754c58b0ff15abe4d16441118f88ceb2352c6,"Fix CMakeLists.txt for Int8 python bindings (#15047)

Summary:
Currently in caffe2, one cannot properly fetch the content of Int8 blobs.

Upon digging the source code, it turns out that the relevant source code is not being compiled. Adding the source to CMakeLists.txt fixes this issue.

First time ever doing a pull request. Please let me know if there's any rule I should follow. Thanks.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15047

Differential Revision: D13417583

Pulled By: bddppq

fbshipit-source-id: dd39575971a3012635edbf97a045d80e4b62a8eb",TerryTsao,https://api.github.com/repos/pytorch/pytorch/git/commits/c2a754c58b0ff15abe4d16441118f88ceb2352c6
04b65dfd1ff78cdda327aeb6ad33ce1bb444fb9d,"Issue 14984: Remove divide by zero error in index_put_ (#14986)

Summary:
No check for zero index tensor was done in the accumulate=True (serial) case in the new TensorIterator code since https://github.com/pytorch/pytorch/pull/13420.

https://github.com/pytorch/pytorch/issues/14984
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14986

Differential Revision: D13417861

Pulled By: colesbury

fbshipit-source-id: e6ed1af8f708b53a35803fc157ed1f043169ec89",jotsif,https://api.github.com/repos/pytorch/pytorch/git/commits/04b65dfd1ff78cdda327aeb6ad33ce1bb444fb9d
5c2c40ad871544b4721955c4472fdc7a72628976,"Add error type to raise statement

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/15039

Differential Revision: D13419566

Pulled By: zou3519

fbshipit-source-id: f67a3aebce937e3e640e91e81eb3e184cfdf269c",daniel-s-ingram,https://api.github.com/repos/pytorch/pytorch/git/commits/5c2c40ad871544b4721955c4472fdc7a72628976
1423c0d9f1f1924d94868c1e2c991878b9ea15b9,"Add EmptyNameScope to allow you jump out from current scope. (#14631)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14631

adding a empty name scope to allow people jump out from current namescope.

This could be useful when you want to access blob from parent or sibling scope.

 Facebook:

e.g: we encoutered a potential usecase in D13124249 (it's a large diff, please search by EmptyNameScope in that diff), we need to access to a blob declared in root namescope from a device namescope (device namescope has been used by parallel_GPU API). `EmptyNameScope` can help us do that with ease.

I referenced to `EmptyDeviceScope` D6103412 while implementing this one.

Reviewed By: yinghai

Differential Revision: D13272240

fbshipit-source-id: d4cde5abcc2336e456b6c6ef086266ef94d86da8",ZephyrSails,https://api.github.com/repos/pytorch/pytorch/git/commits/1423c0d9f1f1924d94868c1e2c991878b9ea15b9
64b336420918d31ee3da623bbcb7afadc73edb6d,"Move adaptive avg pooling 2d to ATen native (#14714)

Summary:
adaptive_avg_pool1d, adaptive_avg_pool2d, and adaptive_avgpool3d are neural network functions that are currently implemented in our legacy THNN (CPU) / THCUNN (CUDA) libraries.  It is generally better if these live in our new library ATen, since it is more feature complete and reduces cognitive overhead.

This change moves currently to adaptive_avg_pool1d and adaptive_avg_pool2d to ATen.

timed relevant cpu tests with this change:
```
[ialex@devgpu064.ash5 ~/pytorch] time python test/test_nn.py
test_AdaptiveAvgPool1d (__main__.TestNN)
test_AdaptiveAvgPool1d_cuda (__main__.TestNN)
test_AdaptiveAvgPool2d_single (__main__.TestNN)
test_AdaptiveAvgPool2d_single_cuda (__main__.TestNN)
test_AdaptiveAvgPool2d_tuple (__main__.TestNN)
test_AdaptiveAvgPool2d_tuple_cuda (__main__.TestNN)
test_AdaptiveAvgPool2d_tuple_none (__main__.TestNN)
test_AdaptiveAvgPool2d_tuple_none_cuda (__main__.TestNN)
test_AdaptiveAvgPool3d_single (__main__.TestNN)
test_AdaptiveAvgPool3d_single_cuda (__main__.TestNN)
test_AdaptiveAvgPool3d_tuple (__main__.TestNN)
test_AdaptiveAvgPool3d_tuple_cuda (__main__.TestNN)
test_AdaptiveAvgPool3d_tuple_none (__main__.TestNN)
test_AdaptiveAvgPool3d_tuple_none_cuda (__main__.TestNN)
test_adaptive_log_softmax (__main__.TestNN)
test_adaptive_pooling_input_size (__main__.TestNN)
test_adaptive_pooling_size_none (__main__.TestNN)
.s.s.s.s.s.s.s...
----------------------------------------------------------------------
Ran 17 tests in 6.273s

OK (skipped=7)

real	0m7.164s
user	3m1.289s
sys	0m0.905s
```

compared to master:
```
[ialex@devgpu064.ash5 ~/pytorch] time python test/test_nn.py
test_AdaptiveAvgPool1d (__main__.TestNN)
test_AdaptiveAvgPool1d_cuda (__main__.TestNN)
test_AdaptiveAvgPool2d_single (__main__.TestNN)
test_AdaptiveAvgPool2d_single_cuda (__main__.TestNN)
test_AdaptiveAvgPool2d_tuple (__main__.TestNN)
test_AdaptiveAvgPool2d_tuple_cuda (__main__.TestNN)
test_AdaptiveAvgPool2d_tuple_none (__main__.TestNN)
test_AdaptiveAvgPool2d_tuple_none_cuda (__main__.TestNN)
test_AdaptiveAvgPool3d_single (__main__.TestNN)
test_AdaptiveAvgPool3d_single_cuda (__main__.TestNN)
test_AdaptiveAvgPool3d_tuple (__main__.TestNN)
test_AdaptiveAvgPool3d_tuple_cuda (__main__.TestNN)
test_AdaptiveAvgPool3d_tuple_none (__main__.TestNN)
test_AdaptiveAvgPool3d_tuple_none_cuda (__main__.TestNN)
test_adaptive_log_softmax (__main__.TestNN)
test_adaptive_pooling_input_size (__main__.TestNN)
test_adaptive_pooling_size_none (__main__.TestNN)
.s.s.s.s.s.s.s...
----------------------------------------------------------------------
Ran 17 tests in 7.232s

OK (skipped=7)

real	0m8.065s
user	3m34.714s
sys	0m2.440s
```

also timed relevant cuda tests with this change:
```
[ialex@devgpu064.ash5 ~/pytorch] time python test/test_nn.py
test_AdaptiveAvgPool1d (__main__.TestNN)
test_AdaptiveAvgPool1d_cuda (__main__.TestNN)
test_AdaptiveAvgPool2d_single (__main__.TestNN)
test_AdaptiveAvgPool2d_single_cuda (__main__.TestNN)
test_AdaptiveAvgPool2d_tuple (__main__.TestNN)
test_AdaptiveAvgPool2d_tuple_cuda (__main__.TestNN)
test_AdaptiveAvgPool2d_tuple_none (__main__.TestNN)
test_AdaptiveAvgPool2d_tuple_none_cuda (__main__.TestNN)
test_AdaptiveAvgPool3d_single (__main__.TestNN)
test_AdaptiveAvgPool3d_single_cuda (__main__.TestNN)
test_AdaptiveAvgPool3d_tuple (__main__.TestNN)
test_AdaptiveAvgPool3d_tuple_cuda (__main__.TestNN)
test_AdaptiveAvgPool3d_tuple_none (__main__.TestNN)
test_AdaptiveAvgPool3d_tuple_none_cuda (__main__.TestNN)
test_adaptive_log_softmax (__main__.TestNN)
test_adaptive_pooling_input_size (__main__.TestNN)
test_adaptive_pooling_size_none (__main__.TestNN)
.................
----------------------------------------------------------------------
Ran 17 tests in 21.049s

OK

real	0m24.106s
user	0m20.890s
sys	0m4.026s
```

compared to master
```
[ialex@devgpu064.ash5 ~/pytorch] time python test/test_nn.py
test_AdaptiveAvgPool1d (__main__.TestNN)
test_AdaptiveAvgPool1d_cuda (__main__.TestNN)
test_AdaptiveAvgPool2d_single (__main__.TestNN)
test_AdaptiveAvgPool2d_single_cuda (__main__.TestNN)
test_AdaptiveAvgPool2d_tuple (__main__.TestNN)
test_AdaptiveAvgPool2d_tuple_cuda (__main__.TestNN)
test_AdaptiveAvgPool2d_tuple_none (__main__.TestNN)
test_AdaptiveAvgPool2d_tuple_none_cuda (__main__.TestNN)
test_AdaptiveAvgPool3d_single (__main__.TestNN)
test_AdaptiveAvgPool3d_single_cuda (__main__.TestNN)
test_AdaptiveAvgPool3d_tuple (__main__.TestNN)
test_AdaptiveAvgPool3d_tuple_cuda (__main__.TestNN)
test_AdaptiveAvgPool3d_tuple_none (__main__.TestNN)
test_AdaptiveAvgPool3d_tuple_none_cuda (__main__.TestNN)
test_adaptive_log_softmax (__main__.TestNN)
test_adaptive_pooling_input_size (__main__.TestNN)
test_adaptive_pooling_size_none (__main__.TestNN)
.................
----------------------------------------------------------------------
Ran 17 tests in 23.021s

OK

real	0m27.095s
user	0m20.121s
sys	0m3.668s
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14714

Differential Revision: D13384084

Pulled By: xnder

fbshipit-source-id: 344442103ccbbda72d3c010d2feea00e9985d226",xnder,https://api.github.com/repos/pytorch/pytorch/git/commits/64b336420918d31ee3da623bbcb7afadc73edb6d
342e62f1e3849118034958cc012d68b1b5e25418,"Minor documentation mistake (#15068)

Summary:
keepdim is a optional parameter for torch.max()
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15068

Differential Revision: D13437745

Pulled By: zou3519

fbshipit-source-id: b5198c7d4ae17758cd136f6e5aecc6cb5838f174",imran3180,https://api.github.com/repos/pytorch/pytorch/git/commits/342e62f1e3849118034958cc012d68b1b5e25418
90f9e8103c9b1108ae923f5ab67bcc566e942699,"Implement torch.tril_indices and torch.triu_indices (#12653) (#14904)

Summary:
This is an optimized implementation that does the following:

1. created an empty Tensor of correct size.
2. fill the Tensor with correct values.

The following three designs to fill in the Tensor result in roughly the same performance. Hence, the 2nd option is taken for simpler code, and to return contiguous tensors.

1. Sequential: fill row coordinates first, then columns. This results in two for-loop and more arithmetic operations.
2. Interleaved: fill in index coordinates one by one, which jumps between the two output Tensor rows in every iteration.
3. Transpose: create a n X 2 Tensor, fill the Tensor sequentially, and then transpose it.

<img width=""352"" alt=""screen shot 2018-12-10 at 3 54 39 pm"" src=""https://user-images.githubusercontent.com/16999635/49769172-07bd3580-fc94-11e8-8164-41839185e9f9.png"">

NOTE:

This implementation returns a 2D tensor, instead of a tuple of two tensors. It means that users will not be able to do the following:

```python
x = torch.ones(3, 3)
i = torch.tril_indices(3, 3)
x[i]  # need to first convert the 2D tensor into a tuple of two 1D tensors.
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14904

Reviewed By: zou3519

Differential Revision: D13433027

Pulled By: mrshenli

fbshipit-source-id: 41c876aafcf584832d7069f7c5929ffb59e0ae6a",mrshenli,https://api.github.com/repos/pytorch/pytorch/git/commits/90f9e8103c9b1108ae923f5ab67bcc566e942699
895cb8fcea85d1879ab04599487a678f3b8b61d7,"Fix resize for edge case tensors (#14874)

Summary:
Certain tensor shapes failed when being resized. This pull request addresses the bug found in #13404.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14874

Differential Revision: D13429788

Pulled By: soumith

fbshipit-source-id: 8aa6451dbadce46d6d1c47a01cb26e6559bcfc8c",mtmoncur,https://api.github.com/repos/pytorch/pytorch/git/commits/895cb8fcea85d1879ab04599487a678f3b8b61d7
1e93317b99bf04cd9ee9029b66eb492b6eb58980,"Remove ""early-release beta"" disclaimer from README (#15136)

Summary:
Now that PyTorch 1.0 is out, this should be updated :)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15136

Differential Revision: D13447377

Pulled By: soumith

fbshipit-source-id: bd4e662c53d0699f25d4d90c1b4c1e182b4427c2",rkaplan,https://api.github.com/repos/pytorch/pytorch/git/commits/1e93317b99bf04cd9ee9029b66eb492b6eb58980
bf7a2b912587116e6f540882c717882e7d376172,"Unify SparseTensorImpl::size_ and TensorImpl::sizes_

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/15130

Differential Revision: D13434981

Pulled By: VitalyFedyunin

fbshipit-source-id: 98bd4d66834a3c3d2ea577adb0c8413852da095d",VitalyFedyunin,https://api.github.com/repos/pytorch/pytorch/git/commits/bf7a2b912587116e6f540882c717882e7d376172
dc72a5e02c1ecb105ea58cafcf10ef3a6f7d9c25,"For rotated proposals, replace cv::rotatedRectangleIntersection with a correct version that doesn't have underflow problem (#15113)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15113

cv::rotatedRectangleIntersection has a known float underflow bug that would cause failure in ```CV_Assert(intersection.size() <= 8)```

For rotated proposals, replace cv::rotatedRectangleIntersection with a correct version that doesn't have underflow problem.

Otherwise, when ```USE_CPP_GENERATE_PROPOSALS = true```, the training would fail.

Reviewed By: viswanathgs

Differential Revision: D13429770

fbshipit-source-id: 5e95d059f3c668f14059a0a83e8e53d8554cdb99",SuperIRabbit,https://api.github.com/repos/pytorch/pytorch/git/commits/dc72a5e02c1ecb105ea58cafcf10ef3a6f7d9c25
5e09c7bc80910917a855903f6971c857102b26aa,"record unit time in torch.cuda.event (#15221)

Summary: Record unit of time for torch.cuda.Event's elapsed_time

Differential Revision: D13467646

Pulled By: zou3519

fbshipit-source-id: 4f1f4ef5fa4bc5a1b4775dfcec6ab155e5bf8d6e",krishnakalyan3,https://api.github.com/repos/pytorch/pytorch/git/commits/5e09c7bc80910917a855903f6971c857102b26aa
3681bf7cff5c41f7e177837d55c898065f1aaee8,"add dense vector to id_list operator (#15090)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15090

as title
step 2 of the linked task

Reviewed By: ellie-wen

Differential Revision: D13425977

fbshipit-source-id: f3538ed68f42470ba39c5b779af764d4a5591a9d",goodmasterli,https://api.github.com/repos/pytorch/pytorch/git/commits/3681bf7cff5c41f7e177837d55c898065f1aaee8
3fdf567752b484c5542efea02afc64e4f875eeca,"Adding CUDA version for C2 operators generate proposals and nms (#13694)

Summary:
Related to issue #13684
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13694

Reviewed By: wat3rBro

Differential Revision: D13017791

Pulled By: newstzpz

fbshipit-source-id: 4bdc58e474d8e1f6cd73a02bf51f91542a2b9d0b",hugovbraun,https://api.github.com/repos/pytorch/pytorch/git/commits/3fdf567752b484c5542efea02afc64e4f875eeca
d6cbcb43c59cee08df6555a7d3bc3d6212f93741,"allow numpy-like boolean-list indexing in pytorch (#14932)

Summary:
Suggested fix to issue #6773, the fix allows numpy-like boolean-list indexing in pytorch
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14932

Differential Revision: D13398795

Pulled By: ezyang

fbshipit-source-id: 67f8daf9829db2550ff76d2bde673be6dd2708cd",ruochunz,https://api.github.com/repos/pytorch/pytorch/git/commits/d6cbcb43c59cee08df6555a7d3bc3d6212f93741
3a6d473b49fc04dc1f086ee7bd8f0a90578e8de6,"collect_env fix (#15447)

Summary:
fixes #15214
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15447

Differential Revision: D13531523

Pulled By: ezyang

fbshipit-source-id: 8f24f5ae9f3e78f6c5c9ee702ba14faca7aa297a",surgan12,https://api.github.com/repos/pytorch/pytorch/git/commits/3a6d473b49fc04dc1f086ee7bd8f0a90578e8de6
52699f07547366d93ec886972bd8e7da65be6927,"Change default value of unique to 'sorted=True'

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/15379

Differential Revision: D13531287

Pulled By: ezyang

fbshipit-source-id: 1512da7d660dc413688d99264e6434897c3ac78c",weihuangxu,https://api.github.com/repos/pytorch/pytorch/git/commits/52699f07547366d93ec886972bd8e7da65be6927
614121c1ef6a34e7653f43cb9be89c28d6766b19,"Replace getargspec with getfullargspec (#15396)

Summary:
Replace `getargspec` with `getfullargspec` to resolve test warnings. Fixes #15344 .
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15396

Differential Revision: D13529548

Pulled By: zou3519

fbshipit-source-id: 50d3be92423a9ce89bc4895b67569663e1abbaa6",epicfaace,https://api.github.com/repos/pytorch/pytorch/git/commits/614121c1ef6a34e7653f43cb9be89c28d6766b19
60b13d1f719f556264739e59697981262316a5cc,"Use at::zeros instead of torch::zeros in non-differentiable example (#15527)

Summary:
There was a typo in C++ docs.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15527

Differential Revision: D13547858

Pulled By: soumith

fbshipit-source-id: 1f5250206ca6e13b1b1443869b1e1c837a756cb5",a-rodin,https://api.github.com/repos/pytorch/pytorch/git/commits/60b13d1f719f556264739e59697981262316a5cc
d4712ee218cd6af3c2096ca7a76fef350173b703,"Added correct isinf handling for Integral tensors (#15489)

Summary:
Currently torch.isinf on integral tensor will raise RuntimeError: value cannot be converted to type int16_t without overflow: inf.
This pr will suppress the error and return false(0) for all integral tensors. The behavior will also be consistent with np.isinf
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15489

Reviewed By: zou3519

Differential Revision: D13540786

Pulled By: flashhack

fbshipit-source-id: e730dea849da6a59f3752d347bcfbadfd12c6483",flashhack,https://api.github.com/repos/pytorch/pytorch/git/commits/d4712ee218cd6af3c2096ca7a76fef350173b703
62151aa259154a9206d812b16525fd5b55ce4a89,"Added deviceCount() virtual method to DeviceGuardImplInterface (#15574)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15574

Added deviceCount() virtual method to DeviceGuardImplInterface, also added correspondent implementation for CPUGuardImpl, CUDAGuardImpl, FakeGuardImpl, VirtualGuardImpl, HIPGuardImplMasqueradingAsCUDA

Reviewed By: soumith

Differential Revision: D13554609

fbshipit-source-id: 913bf2aad44a0a356efe54505ee4abaf6c4622db",ifedan,https://api.github.com/repos/pytorch/pytorch/git/commits/62151aa259154a9206d812b16525fd5b55ce4a89
4047cdc6906c6e5c5e1fb837f7f479e0af149fa7,"Add a patch for OSX with SDK<10.12 (#15615)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/15614

Build passing on SDK 10.9
https://dev.azure.com/ramonaoptics/feedstock-builds/_build/results?buildId=13
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15615

Differential Revision: D13561737

Pulled By: soumith

fbshipit-source-id: 2ab0f78338d4949fa3f2735915fd96dce4bcd621",hmaarrfk,https://api.github.com/repos/pytorch/pytorch/git/commits/4047cdc6906c6e5c5e1fb837f7f479e0af149fa7
2a45050fdcf1634a3a931390c0476b127e63a892,"Concatenate directly into shared memory when constructing batches for numpy (#14534)

Summary:
Since #1323 tensors are shared with shared memory, but this feature is not active for numpy.
This PR fix this.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14534

Differential Revision: D13561649

Pulled By: soumith

fbshipit-source-id: b6bc9e99fb91e8b675c2ef131fba9fa11c1647c0",boeddeker,https://api.github.com/repos/pytorch/pytorch/git/commits/2a45050fdcf1634a3a931390c0476b127e63a892
eeb14675f12a19822d3a13f3d203c6396ee6cfdd,"Fix torch.gesv args in doc

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/15649

Differential Revision: D13564312

Pulled By: soumith

fbshipit-source-id: b3bba2ece600880077eb09b092ce17e331995bd6",kiendang,https://api.github.com/repos/pytorch/pytorch/git/commits/eeb14675f12a19822d3a13f3d203c6396ee6cfdd
879bccb1afff1950659b779cc13ac8d6f0cf090d,"Support for Jetson Xavier (#15660)

Summary:
The request changes are to support building Pytorch 1.0 on the Jetson Xavier with Openblas.  Jetson Xavier with Jetpack 3.3 has generic lapack installed. To pick up the CUDA accelerated BLAS/Lapack, I had to build Openblas and build/link pytorch from source. Otherwise, I got a runtime error indicating lapack routines were not cuda enabled.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15660

Differential Revision: D13571324

Pulled By: soumith

fbshipit-source-id: 9b148d081d6e7fa7e1824dfdd93283c67f69e683",rtarquini,https://api.github.com/repos/pytorch/pytorch/git/commits/879bccb1afff1950659b779cc13ac8d6f0cf090d
10c10b0990108e9d221093f681f0af51dcfe3ff4,"Ignore flake8 warning about whitespace before ':' (#15663)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15663

Ignore sometimes incorrect flake8 warning about whitespace before ':'

See https://github.com/ambv/black/issues/315

Reviewed By: soumith

Differential Revision: D13565818

fbshipit-source-id: 9d5ec2335899527ee71f4b505c00865a354e3bf0",ggoossen,https://api.github.com/repos/pytorch/pytorch/git/commits/10c10b0990108e9d221093f681f0af51dcfe3ff4
b740b92f3600840e09d4c93f3138333838d1e474,"Modify torch.gesv error message (#15654)

Summary:
[doc](https://pytorch.org/docs/stable/torch.html#torch.gesv) uses `B` uppercase so error message should follow to avoid confusion.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15654

Differential Revision: D13571297

Pulled By: soumith

fbshipit-source-id: 0b4e7797eceff92618f808bbfa65d13c1dcc2da0",Youngseok0001,https://api.github.com/repos/pytorch/pytorch/git/commits/b740b92f3600840e09d4c93f3138333838d1e474
04f5605ba158b5274b26390bdd84728a33916e83,"Fix several DeprecationWarning: invalid escape sequence (#15733)

Summary:
Hello,

This is a little patch to fix `DeprecationWarning: invalid escape sequence`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15733

Differential Revision: D13587291

Pulled By: soumith

fbshipit-source-id: ce68db2de92ca7eaa42f78ca5ae6fbc1d4d90e05",BoboTiG,https://api.github.com/repos/pytorch/pytorch/git/commits/04f5605ba158b5274b26390bdd84728a33916e83
734eb31035c45079a3b4a6a2aac28fb7a28773cf,"Cache workspace size in the BenchmarkCache. (#15742)

Summary:
Cache the workspace size information for MIOpen for a given configuration as opposed to inquiring it every time. This reduces overhead significantly as inquiring the workspace size forces a full read of the performance database in MIOpen and this database has grown significantly in recent releases. This caching gets us back to ideal performance.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15742

Differential Revision: D13598932

Pulled By: bddppq

fbshipit-source-id: 4e65d247b71dec828293cf0562aac3fbd4fad83a",mwootton,https://api.github.com/repos/pytorch/pytorch/git/commits/734eb31035c45079a3b4a6a2aac28fb7a28773cf
2b226122899c4fdb66432aa740044fcca9d25b2d,"Add NHWC support to Resize Operator (#15553)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15553

Add unit test and implementation of NHWC layout for Resize operator.

Also, add pragma parallel loop to old NCHWC layout.

Reviewed By: jspark1105

Differential Revision: D13540762

fbshipit-source-id: eebf252bf0d1efdff180a171d804181045f100a5",ccdavid,https://api.github.com/repos/pytorch/pytorch/git/commits/2b226122899c4fdb66432aa740044fcca9d25b2d
cb3241866917024a59955a5099a8eab461357d9c,"Add element-wise multiplication in formulas (#15834)

Summary:
Absence of element-wise multiplication can confused some beginners
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15834

Differential Revision: D13603369

Pulled By: soumith

fbshipit-source-id: 1d5c17c57778ddbb4b201122d826d1d6437204d1",marka17,https://api.github.com/repos/pytorch/pytorch/git/commits/cb3241866917024a59955a5099a8eab461357d9c
cdaeb0db545d50ee98d5dd1186ae258cbc744a86,"Make SGD match python (#15840)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/15530
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15840

Differential Revision: D13608503

Pulled By: goldsborough

fbshipit-source-id: aad17c110d64cbe2c126bccd36d228e4108ffa9a",an-kumar,https://api.github.com/repos/pytorch/pytorch/git/commits/cdaeb0db545d50ee98d5dd1186ae258cbc744a86
162ad945902e8fc9420cbd0ed432252bd7de673a,"Fixed typo in batchnorm docstrings

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/15975

Differential Revision: D13642271

Pulled By: soumith

fbshipit-source-id: 60ffa392bf1f916f2b93c943bb44a642a9815c42",jamestwebber,https://api.github.com/repos/pytorch/pytorch/git/commits/162ad945902e8fc9420cbd0ed432252bd7de673a
a7415787ace8090c5f480a4fb6233437a3f4327e,"fix RandomSampler length (#15991)

Summary:
Hi!

This PR addresses #15537  issue.
Please review.

Thanks!

Differential Revision: D13649890

Pulled By: soumith

fbshipit-source-id: 166212ae383331345423236dfc4fa2ea907d265d",truskovskiyk,https://api.github.com/repos/pytorch/pytorch/git/commits/a7415787ace8090c5f480a4fb6233437a3f4327e
620ff25bdb2b01d87e8f3f3838c862f361a16f8a,"Enhance cpu support on gloo based multi-nodes mode. (#11330)

Summary:
1. Add some gloo communication operators into related fallback list;
2. Work around to avoid compiling errors while using fallback operator whose CPU operator inherits from 'OperatorBase' directly like PrefetchOperator;
3. Add new cpu context support for some python module files and resnet50 training example file.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11330

Reviewed By: yinghai

Differential Revision: D13624519

Pulled By: wesolwsk

fbshipit-source-id: ce39d57ddb8cd7786db2e873bfe954069d972f4f",manofmountain,https://api.github.com/repos/pytorch/pytorch/git/commits/620ff25bdb2b01d87e8f3f3838c862f361a16f8a
b792bfec0e2ef73b7bc156cdc6e18ef51962209e,"s/fwdproxy.any/fwdproxy/g in fbsource (#16024)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16024

codemod with 'Yes to all': s/fwdproxy.any/fwdproxy/g in fbsource

Reviewed By: maxgeorg

Differential Revision: D13666336

fbshipit-source-id: a5a694d66efec5304a1c8c231d638441f88efe1d",kyl191,https://api.github.com/repos/pytorch/pytorch/git/commits/b792bfec0e2ef73b7bc156cdc6e18ef51962209e
e58cc6ab281e8e36b5780cfc285e8c219a532e0d,"Enable single graph sharing between multiple threads for onnxifiop (#16047)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16047

Implements single thead safe map enabling sharing of generated graph between
different ops.
Added model_id to every onnxified op to help create a unique id in the map.
Some formatting fix.

Reviewed By: yinghai

Differential Revision: D13663927

fbshipit-source-id: 27417e8fe752fdd48abb6a87966cd76d592e1206",kimishpatel,https://api.github.com/repos/pytorch/pytorch/git/commits/e58cc6ab281e8e36b5780cfc285e8c219a532e0d
c7a48da493e31424942caf830e37cca027c5b149,"Corresponding data type for BYTE (#15627)

Summary:
TensorProto.DataType in caffe2/proto/caffe2.proto has BYTE = 3 defined, while there is no corresponding TypeMeta defined in caffe2/core/types.cc: DataTypeToTypeMeta. This issue failed the C++ tutorial of MNIST + LMDB.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15627

Differential Revision: D13709602

Pulled By: ezyang

fbshipit-source-id: d4826d0f9b3975e6a8478d4bad1abbbedcaea197",fulltopic,https://api.github.com/repos/pytorch/pytorch/git/commits/c7a48da493e31424942caf830e37cca027c5b149
ded4ff87af6e4cd0d22ef091f8ba7d49a4a49d1c,"fix a little error in comments (#15922)

Summary:
There is a little error in the comment, ""A->B"",  so the Task B must start after task A finishes, not ""B"".
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15922

Differential Revision: D13709579

Pulled By: ezyang

fbshipit-source-id: 735afe83f4532b7c7456da3e96209b3e07071f37",QingfengLee,https://api.github.com/repos/pytorch/pytorch/git/commits/ded4ff87af6e4cd0d22ef091f8ba7d49a4a49d1c
334258e39e5f821f55a6dce805e81f1eb31f73ef,"Potential fix for model inference crash on Win10 (#15919) (#16092)

Summary:
Please refer to issue #15919
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16092

Differential Revision: D13712897

Pulled By: soumith

fbshipit-source-id: edcd1ed3504f1fa1af841a1757616382c745958f",DavidWongEA,https://api.github.com/repos/pytorch/pytorch/git/commits/334258e39e5f821f55a6dce805e81f1eb31f73ef
dbe6a7a9ff1a364a8706bf5df58a1ca96d2fd9da,"Unify the shape notation for all of the pytorch modules (#15741)

Summary:
PR to update the shape notation for all of the torch.nn modules to take a unified form. The goal is to make these definitions machine-readable and those checkable by unifying the style across all of the different modules.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15741

Differential Revision: D13709601

Pulled By: ezyang

fbshipit-source-id: fb89a03903fdf0cd0dcf76f3e469b8582b2f3634",srush,https://api.github.com/repos/pytorch/pytorch/git/commits/dbe6a7a9ff1a364a8706bf5df58a1ca96d2fd9da
6641b09fac63bba79fc53affcc165ecc5642db25,"respect grad guard for torch.jit._fork and torch.jit._wait (#16101)

Summary:
respect grad guard for torch.jit._fork and torch.jit._wait.

Verified that the test failed without the fix, and pass with the fix.

Ideally I would like to enable and disable grad inside the forked function.
It doesn't seems like it's supported at this moment. This code handles that
as well.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16101

Differential Revision: D13708374

Pulled By: gqchen

fbshipit-source-id: 0533f080c4d0253fb4c61d2a0d3cc22de5721a09",gqchen,https://api.github.com/repos/pytorch/pytorch/git/commits/6641b09fac63bba79fc53affcc165ecc5642db25
fe4ae9dfe49dae800eab3585396a7458e42d691c,"add if in register_buffer like register_parameters (#16110)

Summary:
without this ""if"", code below will throw error "" Linear' object has no attribute '_buffers' ""
And with this if, error would be ""cannot assign buffer before Module.\_\_init\_\_() call"", which I think it's more accurate, just like register_parameter.
```
import math
import torch
from torch.nn.parameter import Parameter
from torch.nn import functional as F
from torch.nn import Module
class Linear(Module):
    def __init__(self, in_features, out_features, bias=True):

        self.in_features = in_features
        self.out_features = out_features
        self.register_buffer('test', torch.Tensor(out_features, in_features))
        self.weight = Parameter(torch.Tensor(out_features, in_features))
        if bias:
            self.bias = Parameter(torch.Tensor(out_features))
        else:
            self.register_parameter('bias', None)

        super(Linear, self).__init__()

        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    def forward(self, input):
        return F.linear(input, self.weight, self.bias)

    def extra_repr(self):
        return 'in_features={}, out_features={}, bias={}'.format(
            self.in_features, self.out_features, self.bias is not None
        )

linear = Linear(3,4)
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16110

Differential Revision: D13715839

Pulled By: soumith

fbshipit-source-id: c300eff0a8655aade448354cf489a592f7db722a",FrankHui,https://api.github.com/repos/pytorch/pytorch/git/commits/fe4ae9dfe49dae800eab3585396a7458e42d691c
a9438ba62f02af5032171c3773bfc54c348de298,"Moving cuda-convnet2 to the internal fb dir to satisfy internal dependencies. (#16104)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16104

PyTorch PR 15784 removed cuda-convnet from the contrib directory. This broke
some internal-only fb dependencies. Moving this to the internal area.

Reviewed By: ezyang

Differential Revision: D13709112

fbshipit-source-id: 2d7811545da67489869b59c350a29817eff693cf",gottbrath,https://api.github.com/repos/pytorch/pytorch/git/commits/a9438ba62f02af5032171c3773bfc54c348de298
a28c0ff7b84cdc7d3b9d022ac952f61ccf39e454,"Allow for concurrent quantization in FullyConnectedDNNLowPOp (#16174)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16174

Our service creates a new caffe2 workspace for the same underlying network on multiple threads concurrently at service startup time (later these workspaces are being reused for sequential requests), resulting in concurrent quantization via FullyConnectedDNNLowPOp calling GetOrCreateFbgemmPackBMatrix(). The lazily performed quantizations during the first inference in each workspace are all funnelled through GetOrCreateFbgemmPackBMatrix()'s cache_mutex, which means quantization is serialized, so at service startup time only a single CPU core is being used for around a minute until the serial quantization is done.
An better solution would be to avoid the quantization of the same weight matrix of the operator copies in different net copies to begin with, but this here is the simpler solution for our current problem.

Reviewed By: jspark1105

Differential Revision: D13708785

fbshipit-source-id: 537519896b3b939c552d67f400bafc8a69ce11eb",KjellSchubert,https://api.github.com/repos/pytorch/pytorch/git/commits/a28c0ff7b84cdc7d3b9d022ac952f61ccf39e454
33d1ec396bad4ff8d53b154e64166b56c9054a75,"Fix LBFGS issue (#16167)

Summary:
Resolves #15923 where LBFGS threw ""Error: a leaf Variable that requires grad has been used in an in-place operation.""
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16167

Differential Revision: D13745822

Pulled By: soumith

fbshipit-source-id: 7d1d0511d06838c0c6f4c8a6b53cf15193283059",MiroFurtado,https://api.github.com/repos/pytorch/pytorch/git/commits/33d1ec396bad4ff8d53b154e64166b56c9054a75
53ae8bc64da8672b790f021f4be90762c0a08459,"Reserve vectors that we know the size in advance for. (#16201)

Summary:
Save reallocation costs, by reserving vectors according to how many elements we expect to put in.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16201

Differential Revision: D13762594

Pulled By: ezyang

fbshipit-source-id: 7e3bfe421489dde48a2ddb0920dd155f69baecc0",shahzadlone,https://api.github.com/repos/pytorch/pytorch/git/commits/53ae8bc64da8672b790f021f4be90762c0a08459
4cf76574b958b41ae1df7fba6e87af101414549e,"Raise CalledProcessError when torch.distributed launch process not return 0 (#16069)

Summary:
`torch.distributed.launch.py` will not raise error when `subprocess.Popen` is not return 0.
For better debugging it should always raise an error if processes launched have unusual behavior
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16069

Differential Revision: D13709467

Pulled By: ezyang

fbshipit-source-id: 31d32a5ec8fed7bccd62d845bfba0e670ed3fe20",jwwandy,https://api.github.com/repos/pytorch/pytorch/git/commits/4cf76574b958b41ae1df7fba6e87af101414549e
9521a15c889579d4a1ef88af47aece93d0108157,"hip-clang enablement (#16085)

Summary:
Initial enabling of the upcoming hip-clang compiler for the PyTorch source base.

Changes:
* update the Eigen submodule to a version including our upstreamed hip-clang enabling there
* modify a few ifdef guards with the `__HIP__` macro used by hip-clang
* use `__lane_id` instead of `hc::__lane_id`
* add Debug flags for ROCm to the cmake infrastructure
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16085

Differential Revision: D13709459

Pulled By: ezyang

fbshipit-source-id: 1b7b33fe810a0434766180580d4443ea177eb7c7",yxsamliu,https://api.github.com/repos/pytorch/pytorch/git/commits/9521a15c889579d4a1ef88af47aece93d0108157
e669f72466713000f4209bee41886e3f53d46510,"fix sigma in the middle of when word (#16227)

Summary:
there is a random sigma in the when word on :
https://pytorch.org/cppdocs/contributing.html
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16227

Differential Revision: D13762753

Pulled By: goldsborough

fbshipit-source-id: 3d4bf4be859a3069402fe8c3fbc8ebee4f25cc5a",ArmaanSethi,https://api.github.com/repos/pytorch/pytorch/git/commits/e669f72466713000f4209bee41886e3f53d46510
4b06c063a5259f50ec4c3cdde621857ea125fa97,"raise exception if try jit.load non-existent file (#16270)

Summary:
addresses https://github.com/pytorch/pytorch/issues/16267
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16270

Differential Revision: D13791773

Pulled By: suo

fbshipit-source-id: 256304a02dbf724a7c0baade48c94b3ee77f53cf",nlml,https://api.github.com/repos/pytorch/pytorch/git/commits/4b06c063a5259f50ec4c3cdde621857ea125fa97
f25322fb9756f2943c00800a086ef2733f864598,"Fix issues under caffe round 1

Summary: Some automation to fix uninitialized members for caffe2 code. Ran canary to make sure I don't have any regression in prod, but not sure how to test comprehensively for caffe2

Reviewed By: ezyang

Differential Revision: D13776185

fbshipit-source-id: fb2a479971cc0276d8784be1c44f01252410bd24",benjibc,https://api.github.com/repos/pytorch/pytorch/git/commits/f25322fb9756f2943c00800a086ef2733f864598
fdda533eb1d494da5e3dee8f321e0001db76adf9,"Update docs to include variable annotation example (#16324)

Summary:
Relates to this issue https://github.com/pytorch/pytorch/issues/16288
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16324

Reviewed By: ezyang

Differential Revision: D13805412

Pulled By: suo

fbshipit-source-id: 8b80f988262da2c717452a71142327bbc23d1b8f",sidazhang,https://api.github.com/repos/pytorch/pytorch/git/commits/fdda533eb1d494da5e3dee8f321e0001db76adf9
3c30cf323747229003929fcdf8da14acc1c2362d,"Update einsum documentation. (#16323)

Summary:
The documentation stated that operands to einsum should be a list of Tensors, not individual arguments. The function, however, now accepts individual arguments for each Tensor operand *and* a single argument consisting of a list of Tensors. The documentation was updated to reflect this change.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16323

Differential Revision: D13832647

Pulled By: soumith

fbshipit-source-id: c01c2b350f47674d3170337f493b0ee2ea381b3f",fadel,https://api.github.com/repos/pytorch/pytorch/git/commits/3c30cf323747229003929fcdf8da14acc1c2362d
2b6607065bcb358db168ce199f76cb1385971277,"Fix a typo in Parallel.h (#16419)

Summary:
Fix a typo in Parallel.h.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16419

Differential Revision: D13833705

Pulled By: soumith

fbshipit-source-id: 824ebe753e028fc8e2b5d7a51fdba98a365fd29a",gemfield,https://api.github.com/repos/pytorch/pytorch/git/commits/2b6607065bcb358db168ce199f76cb1385971277
fdaa77ae8b084eaa7535075538e9d80f4c4a8d1a,"Better error message when creating a module instance in jit.script (#16416)

Summary:
Made the change requested in #15555

PR was failing build due to a time out error while getting packages using pip.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16416

Differential Revision: D13833873

Pulled By: soumith

fbshipit-source-id: e2200e9e8015558fcd359dfa3d025b25802d62b5",rotuna,https://api.github.com/repos/pytorch/pytorch/git/commits/fdaa77ae8b084eaa7535075538e9d80f4c4a8d1a
6249442e900a6b445dd74b3a6cfd5d0fff945a07,"Chunk dataset implementation (#15932)

Summary:
This PR contains the implementation of chunk dataset, with the API proposed in PR https://github.com/pytorch/pytorch/pull/15562

A chunk dataset is derived from StatefulDataset. It utilizes worker threads to prefetches chunk data, splits it into batches and caches them into a queue. When get_batch is called from dataloader, batch data is retrieved from the queue, and data in new chunks will be pushed for later following batches.

Chunk dataset uses two samplers (chunk_sampler and example_sampler) to perform sampling. The chunk_sampler decides which chunk to load, and example_sampler shuffles the examples inside a specific chunk. More detail of this sampling approach can be found here: http://martin.zinkevich.org/publications/nips2010.pdf
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15932

Differential Revision: D13868688

Pulled By: soumith

fbshipit-source-id: a43000c478ca2a3c64cc84b3626d6b8b1ad9a07e",xzhu1900,https://api.github.com/repos/pytorch/pytorch/git/commits/6249442e900a6b445dd74b3a6cfd5d0fff945a07
a7796bc24d2fd25774f44dca1262bbb22d640dc3,"CUDA histogram implementation

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/15842

Reviewed By: zou3519

Differential Revision: D13868982

Pulled By: jaciefan

fbshipit-source-id: bce81dc121c4538d204047506f8f14d0b4d8f905",jaciefan,https://api.github.com/repos/pytorch/pytorch/git/commits/a7796bc24d2fd25774f44dca1262bbb22d640dc3
16e2e4f29f8c9bd076011992755c3d2962fc5eb5,"added example to clear ambiguity in torch.Tensor.view (#16563)

Summary:
Added example to the documentation of [torch.Tensor.view](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) to avoid the misunderstanding referenced in issue [#16560](https://github.com/pytorch/pytorch/issues/16560)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16563

Differential Revision: D13885008

Pulled By: soumith

fbshipit-source-id: b7e7fbea1f16124bc4e679ae9c50ab619e1f043d",ParticularlyPythonicBS,https://api.github.com/repos/pytorch/pytorch/git/commits/16e2e4f29f8c9bd076011992755c3d2962fc5eb5
bc53805f2efff483ee71a934b985768ffbd96cb5,"Remove redundant declarations (#16463)

Summary:
As there are no checks that all the functions are actually being used, we can end up with stale entries. This diff removes unused entries from Declarations.cwrap

Testing:
Successful build via ""python setup.py develop""
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16463

Differential Revision: D13885815

Pulled By: izdeby

fbshipit-source-id: 4e35c2ac9196167af74dff3d4f971210721285f8",izdeby,https://api.github.com/repos/pytorch/pytorch/git/commits/bc53805f2efff483ee71a934b985768ffbd96cb5
a44826e6599b84fd16c95ed4e2dad8c34f91dee0,"Fix: avoid race condition on model zoo directory creation (#16578)

Summary:
The current implementation of the `torch.utils.model_zoo.load_url`
function is prone to a race condition when creating the directory in
which it saves the loaded models, since it checks whether the
directory exists and then creates it in two separate steps. The
directory can be created after the check was made but before we
attempt to create the directory, resulting in an unhandled exception.

Instead, try to create the directory directly, and do nothing if it
already exists.

Note: for Python versions â‰¥ 3.2, we could simply use the
`exist_ok=True` flag on `os.makedirs`, but this is unavailable in
Python 2.7.

Signed-off-by: Antoine Busque <antoine.busque@elementai.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16578

Differential Revision: D13886470

Pulled By: soumith

fbshipit-source-id: 88815c8a65eec96caea32d6e9a7f83802502fdb9",abusque,https://api.github.com/repos/pytorch/pytorch/git/commits/a44826e6599b84fd16c95ed4e2dad8c34f91dee0
b109549bf374202809dfab1a043a8e76fe9a7075,"Replaced ""from_numpy"" with ""as_tensor"" in docs. (#16587)

Summary:
In the warning box on https://pytorch.org/docs/stable/tensors.html#torch.Tensor.new_tensor it says:

> new_tensor() always copies data. [...] If you have a numpy array and want to avoid a copy, use **torch.from_numpy()**.

But then further up the page we have another warning box with the message:

> torch.tensor() always copies data. [...] If you have a numpy array and want to avoid a copy, use **torch.as_tensor()**.

Now I believe this is just a small oversight, since from_numpy is to be deprecated in favour of as_tensor. See for example https://github.com/pytorch/pytorch/issues/6885 and https://github.com/pytorch/pytorch/issues/8611. I suggest to just use **torch.as_tensor()** in both of the warning boxes.

cc gchanan
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16587

Differential Revision: D13897038

Pulled By: gchanan

fbshipit-source-id: 2eb3cd47d2c0b5bf4350f980de3be9fe59b4a846",sebftw,https://api.github.com/repos/pytorch/pytorch/git/commits/b109549bf374202809dfab1a043a8e76fe9a7075
b0e692c8a66ed9cb174df12aa1ff747e8b7c8495,"Improving docs for MultiLabelSoftMarginLoss (#16644)

Summary:
Resolves #15863

Changed the documentation for MultiLabelSoftMarginLoss and MultiLabelMarginLoss to be more explicit about the `target` format.

More than happy to change the messaging based on discussion.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16644

Differential Revision: D13912395

Pulled By: soumith

fbshipit-source-id: 24a3c214c5f6f9d043e25b13ac758c1c1211b641",JamesTrick,https://api.github.com/repos/pytorch/pytorch/git/commits/b0e692c8a66ed9cb174df12aa1ff747e8b7c8495
b67b29b667877f9a49def4a673d9b4409d7e3de1,"add SingleLoadedNetSupplier (#16620)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16620

LogfiledbNetLoader loads all external input blobs into a workspace instance, we pack a shared pointer to this loaded workspace into the SingleLoadedNetSupplier.
SingleLoadedNetSupplier will pass this workspace to BlackBoxPredictor to be executed. (D13891759 is a WIP of how it all comes together)

Reviewed By: pjh5

Differential Revision: D13901467

fbshipit-source-id: 20589f898922f5f1aec50be131dad17a8c38e9b2",amylittleyang,https://api.github.com/repos/pytorch/pytorch/git/commits/b67b29b667877f9a49def4a673d9b4409d7e3de1
7078b2baf57bb6664309c680d4cc35bae23d6c9d,"Better bounds checks in ctcloss (#16269)

Summary:
Adds better bounds checks for target lengths in CTC loss, checks for integral types for target and prediction lengths, and adds tests for each, according to #15946
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16269

Differential Revision: D13847567

Pulled By: ezyang

fbshipit-source-id: 5d7a975565e02baf78fe388813a1d1ef56dfb212",ashermancinelli,https://api.github.com/repos/pytorch/pytorch/git/commits/7078b2baf57bb6664309c680d4cc35bae23d6c9d
6d407baedf5dc8aea941340782972ed28a60c603,"Replace resize_dim() with set_sizes_and_strides() in THTensor_(unsqueeze1d) in aten/src/TH/generic/THTensor.cpp (#16673)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16673

Replace resize_dim() with set_sizes_and_strides() in THTensor_(unsqueeze1d) in aten/src/TH/generic/THTensor.cpp, as described in T38058642.

Reviewed By: ezyang

Differential Revision: D13928879

fbshipit-source-id: d593cebcc82589cd362ac78884d4e367d0da0ce6",joshim5,https://api.github.com/repos/pytorch/pytorch/git/commits/6d407baedf5dc8aea941340782972ed28a60c603
9e31d6dbf1f02c759d739bed4f62554bdcefac5e,"Merge job-spec env variables of Pytorch/Caffe2 CI jobs (#16649)

Summary:
The idea is to unify the environment variables `JOB_BASE_NAME` and `BUILD_ENVIRONMENT` which controlled the Pytorch and Caffe2 jobs respectively. In this commit, we have converted all the `JOB_BASE_NAME` references in _.jenkins/pytorch/*_ files to `BUILD_ENVIRONMENT`. Then, did the same thing in ._circleci/config.yml_. One thing that we needed to be careful was when both `BUILD_ENVIRONMENT `and `JOB_BASE_NAME` were present under same declaration in _config.yml_ file (e.g., for ""caffe2-"" stuffs). To ensure that all ""=="" checks work as expected, we also had to add ""*"" in some if conditions in _.jenkins/caffe2/build.sh_ file. Finally, removed ""-build"", ""-test"", etc. suffixes from `COMPACT_JOB_NAME` variable assignment in the bash script files in _.jenkins/pytorch_ folder, e.g., modify `COMPACT_JOB_NAME=""${BUILD_ENVIRONMENT}-build""` to `COMPACT_JOB_NAME=""${BUILD_ENVIRONMENT}""`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16649

Differential Revision: D13946392

Pulled By: mmh683

fbshipit-source-id: 790de6abf96de184758e395c9098a50998e05bc5",mmh683,https://api.github.com/repos/pytorch/pytorch/git/commits/9e31d6dbf1f02c759d739bed4f62554bdcefac5e
2a85d98745bb5e8b2ade952dc181c2526f21c2b1,"Fix type-o in unsupported data type error message (#16537)

Summary:
-In the case where an operator does not support a given data type
 an error message is emitted to alert the user, this message is
incorrectly structured. This commit adds to and rearranges the
error message to make it a little clearer.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16537

Differential Revision: D13958859

Pulled By: zou3519

fbshipit-source-id: 935fc3adcef2f969042b1db902c9ec004488ea9c",rjknight,https://api.github.com/repos/pytorch/pytorch/git/commits/2a85d98745bb5e8b2ade952dc181c2526f21c2b1
d327965dac89e121852727c6c3421c151fdad42d,"Fix pip list format in collect_env (#16798)

Summary:
Since pip 18.0 (2018-07-22), `legacy` is no longer a valid choice for `pip list --format` as can be seen in the [Release Notes](https://pip.pypa.io/en/stable/news/#id62). Therefore, the options now are: `columns`, `freeze` and `json`. With `legacy`, this is how it looked like:

```
[...]
Versions of relevant libraries:
[pip3] numpy (1.16.1)
[pip3] torch (1.0.1)
[pip3] torchvision (0.2.1)
[...]
```

Changing to `freeze`, this is how it looks like:

```
[...]
Versions of relevant libraries:
[pip3] numpy==1.16.1
[pip3] torch==1.0.1
[pip3] torchvision==0.2.1
[...]
```

Currently, this is what happens:

```
[...]
Versions of relevant libraries:
[pip] Could not collect
[...]
```
The `freeze` option is also available in old pip, so this change is backwards compatible. Also, if we would like to keep the old style, which I think it is not necessary, I could easily change that.

 ---

In case anyone wants to know how `columns` looks like (I prefer `freeze`):

```
[...]
Versions of relevant libraries:
[pip3] numpy               1.16.1
[pip3] torch               1.0.1
[pip3] torchvision         0.2.1
[...]
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16798

Differential Revision: D13971793

Pulled By: soumith

fbshipit-source-id: 3721d9079a2afa245e1185f725598901185ea4cd",rodrigoberriel,https://api.github.com/repos/pytorch/pytorch/git/commits/d327965dac89e121852727c6c3421c151fdad42d
ea35d8e40a8ce070d4f1e60a4c2471745d7dbe31,"Replace resize_dim() with set_sizes_and_strides() (#16732)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16732

Use set_sizes_and_strides instead of resize_dim with.

Reviewed By: ezyang

Differential Revision: D13947867

fbshipit-source-id: 067b096b1fde14b039690992a5fe3ace386b2789",NarineK,https://api.github.com/repos/pytorch/pytorch/git/commits/ea35d8e40a8ce070d4f1e60a4c2471745d7dbe31
85ac272670ce7f6085558e1c56c2f383d8f9c6fa,"Update Docker file section in README.md (#16812)

Summary:
Emphasize on the fact that docker build should be triggered from pytorch repo directory.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16812

Differential Revision: D13985531

Pulled By: soumith

fbshipit-source-id: c6511d1e81476eb795b37fb0ad23e8951dbca617",mohanasra,https://api.github.com/repos/pytorch/pytorch/git/commits/85ac272670ce7f6085558e1c56c2f383d8f9c6fa
66084c0bc92d3624738c829af60b49766a905090,"Add recognition for XLA device types.

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/16844

Differential Revision: D13988805

Pulled By: gchanan

fbshipit-source-id: 4e89d6d2cde8bdac41739efa65cc91569a360953",dlibenzi,https://api.github.com/repos/pytorch/pytorch/git/commits/66084c0bc92d3624738c829af60b49766a905090
7ce33c586de4c73a618430fdbf8f52ac1b9cc9f6,"Robust determination of cudnn library and relevant conda packages. (#16859)

Summary:
This PR implements:
1. a fix to issue #12174 - determine the location of cudnn library using `ldconfig`
2. a fix to determine the installed conda packages (in recent versions of conda, the command `conda` is a Bash function that cannot be called within a python script, so using CONDA_EXE environment variable instead)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16859

Differential Revision: D14000399

Pulled By: soumith

fbshipit-source-id: 905658ecacb0ca0587a162fade436de9582d32ab",pearu,https://api.github.com/repos/pytorch/pytorch/git/commits/7ce33c586de4c73a618430fdbf8f52ac1b9cc9f6
a9f1d2e3711476ba4189ea804488e5264a4229a8,"Fix the error in the note about `torch.device` documentation. (#16839)

Summary:
This PR is a simple fix for the mistake in the first note for `torch.device` in the ""tensor attributes"" doc.
![image](https://user-images.githubusercontent.com/8536399/52399611-1becaa00-2b00-11e9-85bf-cac04b29842d.png)

```
>>> # You can substitute the torch.device with a string
>>> torch.randn((2,3), 'cuda:1')
```
Above code will cause error like below:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-53-abdfafb67ab1> in <module>()
----> 1 torch.randn((2,3), 'cuda:1')

TypeError: randn() received an invalid combination of arguments - got (tuple, str), but expected one of:
 * (tuple of ints size, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)
 * (tuple of ints size, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)
```

Simply adding the argument name `device` solves the problem: `torch.randn((2,3), device='cuda:1')`.

However, another concern is that this note seems redundant as **there is already another note covering this usage**:
![image](https://user-images.githubusercontent.com/8536399/52399583-0ecfbb00-2b00-11e9-914f-e95da4edecd1.png)

So maybe it's better to just remove this note?
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16839

Reviewed By: ezyang

Differential Revision: D13989209

Pulled By: gchanan

fbshipit-source-id: ac255d52528da053ebfed18125ee6b857865ccaf",JirenJin,https://api.github.com/repos/pytorch/pytorch/git/commits/a9f1d2e3711476ba4189ea804488e5264a4229a8
e0323a6aea2c8b4c9267bb4d7ed538dcd792d121,"ctc_loss error message bug fix. (#16917)

Summary:
CTCLLoss argument error message is wrong.
Please fix this. (sorry if I made some mistakes.)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16917

Differential Revision: D14019983

Pulled By: ezyang

fbshipit-source-id: 3337a2e86da6f3f7594c73fddb73340494a19ce2",drkw1git,https://api.github.com/repos/pytorch/pytorch/git/commits/e0323a6aea2c8b4c9267bb4d7ed538dcd792d121
aae6b53c5b0839cdec16eee6c5a5606d7ae589ec,"DOC: correct docstring for torch and torch.Tensor package (#16842)

Summary:
This PR is a simple fix for the mistake in the  ""tensor""  and ""torch.Tensor""doc.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16842

Differential Revision: D14020300

Pulled By: ezyang

fbshipit-source-id: 3ab04f1223d6e60f8da578d04d759e385d23acbb",ZhuBaohe,https://api.github.com/repos/pytorch/pytorch/git/commits/aae6b53c5b0839cdec16eee6c5a5606d7ae589ec
73d7ecd18398cf121a9a2ff2faad302d429bef3e,"Add abs for ByteTensor and CharTensor. (#16893)

Summary:
Fixes #15089
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16893

Differential Revision: D14020115

Pulled By: ezyang

fbshipit-source-id: 6f3be6ed28d2d37667159be45959d400bc473451",hameerabbasi,https://api.github.com/repos/pytorch/pytorch/git/commits/73d7ecd18398cf121a9a2ff2faad302d429bef3e
c282afffa79e719e366b869914fc824dbb65684b,"Improve the Sparse matrix multiplication computational speed #16187 (#16905)

Summary:
Instead of converting coo to csr format of the sparse matrix in the original implementation, in my revision I directly use coo format for sparse dense matrix mutliplication.
On my linux machine it is 5 times faster than the original code:

```
(original code)
SIZE: 15000 DENSITY: 0.01 DEVICE: cpu
torch: 0.39403 seconds
np:    0.00496674 seconds
torch/np: 79.3338

----------------------------------------

(my update)
SIZE: 15000 DENSITY: 0.01 DEVICE: cpu
torch: 0.0812583 seconds
np:    0.00501871 seconds
torch/np: 16.1911

```

Further code feedback and running time tests are highly welcomed. I will keep revise my code if needed.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16905

Differential Revision: D14020095

Pulled By: ezyang

fbshipit-source-id: 4ab94075344a55b375f22421e97a690e682baed5",musikisomorphie,https://api.github.com/repos/pytorch/pytorch/git/commits/c282afffa79e719e366b869914fc824dbb65684b
e661dc27ffbdd64c6bed1bf9a7c5a601f91e6386,"Int8GivenTensorFill Operator Schema fix typo (#16204)

Summary:
Hi,
caffe2/operators/quantized/int8_given_tensor_fill_op.cc expects the value array to be named ""values"" but the operator schema describe ""value"" (no s). I guess it is a little typo but it made me losing a bit of time before understanding why I had this error by passing ""value"" instead of ""values"":
```
[F int8_given_tensor_fill_op.h:95] Check failed: output->t.numel() == values_.numel() output size: 3 given size: 0
Aborted (core dumped)
```

Thanks,
EyyÃ¼b Sari
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16204

Differential Revision: D14020476

Pulled By: ezyang

fbshipit-source-id: a8a46bfc44ec125e7925ce4b7c79fdf99c890a50",Eyyub,https://api.github.com/repos/pytorch/pytorch/git/commits/e661dc27ffbdd64c6bed1bf9a7c5a601f91e6386
8b4dea3f56a2c5791c4f4b332c6c6a2311984365,"Added scientific notation on set_printoptions (#16876)

Summary:
This PR fixes #15683
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16876

Differential Revision: D14021703

Pulled By: soumith

fbshipit-source-id: 1f603a7d24e331831d8d389f4a704c6a5b070b0c",xmnlab,https://api.github.com/repos/pytorch/pytorch/git/commits/8b4dea3f56a2c5791c4f4b332c6c6a2311984365
8042edcdb10e75b49a15ed7bf8c807ecde41d12b,"Make pin_memory and default_collate preserve namedtuples (#16440)

Summary:
Open issue: https://github.com/pytorch/pytorch/issues/3281
Corresponding PR (conflict): https://github.com/pytorch/pytorch/pull/4577

Another open issue: https://github.com/pytorch/pytorch/issues/14613
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16440

Differential Revision: D14020901

Pulled By: ezyang

fbshipit-source-id: 4abe817fc43c281a510715d311bad544511995d3",eskjorg,https://api.github.com/repos/pytorch/pytorch/git/commits/8042edcdb10e75b49a15ed7bf8c807ecde41d12b
632df48207f0ff1fb7c34753f2638a90a3ff92fd,"Merge binaries ""convert_image_to_tensor"" and ""caffe2_benchmark"" (#16875)

Summary:
Merge binaries ""convert_image_to_tensor"" and ""caffe2_benchmark"" to remove the overhead of writing to/reading from Tensor file.

*TODO next: TensorProtos is another overhead. No need for de-serialization.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16875

Reviewed By: sf-wind

Differential Revision: D13997726

Pulled By: ZhizhenQin

fbshipit-source-id: 4dec17f0ebb59cf1438b9aba5421db2b41c47a9f",ZhizhenQin,https://api.github.com/repos/pytorch/pytorch/git/commits/632df48207f0ff1fb7c34753f2638a90a3ff92fd
3618b52c748bbecb6ab1e5cee016bdcf9b71a2d0,"Add module and name to func created with _jit_internal.boolean_dispatch (#16922)

Summary:
The use case for making this PR is the following bug :
(with F = torch.nn.functional)
`F.max_pool2d.__module__` is `torch._jit_internal`
`F.max_pool2d.__name__` is `fn`

With this PR you get:
`F.max_pool2d.__module__` is `torch.nn.functional`
`F.max_pool2d.__name__` is `max_pool2d`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16922

Differential Revision: D14020053

Pulled By: driazati

fbshipit-source-id: c109c1f04640f3b2b69bc4790b16fef7714025dd",LaRiffle,https://api.github.com/repos/pytorch/pytorch/git/commits/3618b52c748bbecb6ab1e5cee016bdcf9b71a2d0
a2c322e7357a897402e6bdf700f086eaaed48cfd,"fix silent failure on Windows builds (#16984)

Summary:
Closes #16983

Remove backticks that are being interpreted by the shell. Add -e option to bash script to avoid future such failures
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16984

Reviewed By: yf225

Differential Revision: D14039128

Pulled By: kostmo

fbshipit-source-id: c31a1895377ca86c1b59e79351843cc8c4fd7de3",kostmo,https://api.github.com/repos/pytorch/pytorch/git/commits/a2c322e7357a897402e6bdf700f086eaaed48cfd
a5e7b1d03268cc2afbce4109206dcf6a57b0f4fd,"Use IndexError instead of RuntimeError in ATen CPU kernels

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/17049

Reviewed By: ezyang

Differential Revision: D14064700

Pulled By: fmassa

fbshipit-source-id: 3575db103bba5a7d82f574cbb082beca419151ec",skrah,https://api.github.com/repos/pytorch/pytorch/git/commits/a5e7b1d03268cc2afbce4109206dcf6a57b0f4fd
41dddfd55fc59a0ebae7cda7e759b80011741a78,"Make mkldnn Stream object thread_local and enable mkldnn thread-safe (#17022)

Summary:
This PR fixes following issue: https://github.com/pytorch/pytorch/issues/16828

It is a combination of two things:
1) MKLDNN streams are not thread-safe but are currently shared between different threads. This change makes them thread_local
2) By default MKLDNN primitives can share global memory and can't be invoked from multiple threads. This PR enables the MKLDNN_ENABLE_CONCURRENT_EXEC cmake configuration option that makes them thread-safe.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17022

Differential Revision: D14069052

Pulled By: ezyang

fbshipit-source-id: f8f7fcb86c40f5d751fb35dfccc2f802b6e137c6",bdaskalov,https://api.github.com/repos/pytorch/pytorch/git/commits/41dddfd55fc59a0ebae7cda7e759b80011741a78
65d6f1014a3689bef90cdc264e45de560f64b741,"Add support of count_include_pad and test end to end test for AveragePool (#17034)

Summary:
Add support of count_include_pad end to end test for AveragePool

We can export AveragePool from PyTorch with count_include_pad attribute. However, we don't directly support it in Caffe2's ONNX backend.
We also want to check whether we can pass the end to end test for average pool operator with count_include_pad attribute (pytorch => onnx => caffe2)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17034

Reviewed By: houseroad

Differential Revision: D14060186

Pulled By: dwarakrajagopal

fbshipit-source-id: 10dae532611c71f8c8cfc3fa701cc7c1c1c02695",dwarakrajagopal,https://api.github.com/repos/pytorch/pytorch/git/commits/65d6f1014a3689bef90cdc264e45de560f64b741
016f21235721d9998b1ebcd5c7948e1049233771,"fix behavior of ConcatDataset w/ negative indices (#15756)

Summary:
Currently, when you pass a negative index to a `Dataset` created with `ConcatDataset`, it simply passes that index to the first dataset in the list. So if, for example, we took `concatenated_dataset[-1]`, this will give us the last entry of the *first* dataset, rather than the last entry of the *last* dataset, as we would expect.

This is a simple fix to support the expected behavior for negative indices.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15756

Reviewed By: ezyang

Differential Revision: D14081811

Pulled By: fmassa

fbshipit-source-id: a7783fd3fd9e1a8c00fd076c4978ca39ad5a8a2a",jayleverett,https://api.github.com/repos/pytorch/pytorch/git/commits/016f21235721d9998b1ebcd5c7948e1049233771
ff2053dfa1d9d5e39215a297dfaf11b91f9fbb6e,"add clear functionality to list (#17050)

Summary:
Add clear functionality to list. See #16662

```python
import torch

torch.jit.script
def foo():
    a = [1, 2, 3, 4]
	a.clear()

    return a
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17050

Differential Revision: D14071799

Pulled By: driazati

fbshipit-source-id: 305551c16f7db127c43de0ad5885d9f10678e101",TheCodez,https://api.github.com/repos/pytorch/pytorch/git/commits/ff2053dfa1d9d5e39215a297dfaf11b91f9fbb6e
d61455cf4017c0b9e8a9668b18f086c736b2813e,"Fix some documentation links in torch.tensor (#17109)

Summary:
Currently it's broken https://pytorch.org/docs/stable/tensors.html#torch.Tensor.norm
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17109

Differential Revision: D14093567

Pulled By: ezyang

fbshipit-source-id: b167cde2150ee97ccf5689fcf50ff8157acfce10",kngwyu,https://api.github.com/repos/pytorch/pytorch/git/commits/d61455cf4017c0b9e8a9668b18f086c736b2813e
6c67dcfb059e997a35b337cebfe7fc248ba957b6,"Fix AdaptiveLogSoftmaxWithLoss's constructor (#16694)

Summary:
t-ken1 and I are members of a same team.
I have added test codes about the pull request https://github.com/pytorch/pytorch/pull/16656.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16694

Differential Revision: D14070106

Pulled By: ezyang

fbshipit-source-id: ff784dbf45e96a6bcf9a4b5cb9544a661a8acad2",wbydo,https://api.github.com/repos/pytorch/pytorch/git/commits/6c67dcfb059e997a35b337cebfe7fc248ba957b6
82b269060c6937f3deee1a57e6db60dd41f27a61,"Add support for simpler for-in-list + tests (#16726)

Summary:
This PR add supports for simpler for-in-list loops such as the example below:

```python
torch.ji.python
def sum_list(a):
    # type: (List[int]) -> int
    sum = 0
    for i in a:
        sum += i

    return sum
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16726

Differential Revision: D14070007

Pulled By: ezyang

fbshipit-source-id: b4d971ee647729a6caa3099ceac34ec5c4f143de",Krovatkin,https://api.github.com/repos/pytorch/pytorch/git/commits/82b269060c6937f3deee1a57e6db60dd41f27a61
1cdcdd78af84ee60777db9b8cd0fc9bb7d441975,"Kaiming Initialization (#14718)

Summary:
/cc goldsborough

Working on #14582

The corresponding python implementations are at: [pytorch/torch/nn/init.py](https://github.com/pytorch/pytorch/blob/6302e4001ab54b3ddeca2b608d337fe7077e801c/torch/nn/init.py#L261-L327)

Here is my initial implementation of Kaiming Initialization. I have not been able to figure out how to successfully run tests locally so I haven't added any yet.

A couple questions:
- Are the enums defined in the right place? I copied their names from Python, but do you prefer different naming conventions for C++?
- To run tests locally do I use `python setup.py test`? Can I run just a subset of the tests somehow?
- Should I add my tests at [test/cpp/api/misc.cpp](https://github.com/pytorch/pytorch/blob/master/test/cpp/api/misc.cpp#L47-L54)?
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14718

Differential Revision: D14049159

Pulled By: goldsborough

fbshipit-source-id: 966ac5126875936e69b185b5041f16476ed4cf70",JoshVarty,https://api.github.com/repos/pytorch/pytorch/git/commits/1cdcdd78af84ee60777db9b8cd0fc9bb7d441975
57617ee42920c7b0e75b385719f92990b0c06422,"Replace resize_dim() with set_sizes_and_strides() (#17127)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17127

Replace resize_dim() with set_sizes_and_strides() in   `THTensor_(squeeze1d) in aten/src/TH/generic/THTensor.cpp`

Reviewed By: ezyang

Differential Revision: D14088697

fbshipit-source-id: 518b72f7c0c4fbedf11a29a6ceb9fee8eefd9273",pcpLiu,https://api.github.com/repos/pytorch/pytorch/git/commits/57617ee42920c7b0e75b385719f92990b0c06422
82aa51114677b4d2187e1cc2d36f32158b7dddc0,"move prim::None to prim::Constant (again) (#17186)

Summary:
Trying to land again, make prim::None into a case of prim::Constant. Reverted the previous landing because it broke an important onnx export test.

https://github.com/pytorch/pytorch/pull/16160
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17186

Differential Revision: D14115304

Pulled By: eellison

fbshipit-source-id: 161435fc30460b4e116cdd62c7b2e5b94581dcb7",eellison,https://api.github.com/repos/pytorch/pytorch/git/commits/82aa51114677b4d2187e1cc2d36f32158b7dddc0
29f4f8f048b03808bd1c7b6917b1825997b4abc3,"Avoid unnecessary CPU-to-GPU copy of torch.load with CUDA (#17297)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17297

When `torch.load` needs to load a tensor, no matter which device it will be end up being loaded on, it first creates a CPU storage for it of the necessary size. This storage is allocated but it's not ""set"" yet, hence no data is written to it: it exists in the kernel's memory map, but it's not resident and doesn't take up physical pages. Then, this storage is passed to the `map_location` function (if the parameter is a string, a device or a map, PyTorch builds that function automatically). The default map for CUDA consists effectively in `lambda storage, _: storage.cuda()` (I omitted the code needed to pick the correct device). This creates a GPU storage and copies over the data of the CPU storage. *This step is unnecessary as we're copying uninitialized memory*. (Surprisingly enough, though, it appears the kernel is smart enough that reading from the unpaged CPU memory doesn't cause it to become paged.) Once `map_location` returns a storage residing on the correct target device, `torch.load` resumes reading the file and copying the tensor's content over into the storage. This will overwrite the content that had previously been written to it, which confirms that the above copy was pointless.

A way to avoid this useless copy is to just create and return a new empty storage on the target GPU, instead of ""transforming"" the original one.

This does indeed increase the performance:
```
In [5]: torch.save(torch.rand(100, 100, 100), ""/tmp/tensor"")

In [6]: %timeit torch.load(""/tmp/tensor"", map_location=""cuda"")
1.55 ms Â± 111 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)

In [7]: %timeit torch.load(""/tmp/tensor"", map_location=lambda storage, _: torch.cuda.FloatStorage(storage.size()))
1.03 ms Â± 44 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
```

Credit for this diff is shared with adamlerer and fmassa.

Differential Revision: D14147673

fbshipit-source-id: a58d4bc0d894ca03a008499334fc2cdd4cc91e9f",lerks,https://api.github.com/repos/pytorch/pytorch/git/commits/29f4f8f048b03808bd1c7b6917b1825997b4abc3
37890610b02fae2bd4b4eff3fd3747a3dc0ffaad,"Include vec256 headers in setup.py (#17220)

Summary:
Fix #16650.

Headers such as `ATen/cpu/vml.h` contain `#include <ATen/cpu/vec256/vec256.h>`
for example, but these vec256 headers aren't included, due to commit e4c0bb1.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17220

Differential Revision: D14165695

Pulled By: ezyang

fbshipit-source-id: 27b2aa2a734b3719ca4af0565f79623b64b2620f",tridao,https://api.github.com/repos/pytorch/pytorch/git/commits/37890610b02fae2bd4b4eff3fd3747a3dc0ffaad
94a95a0c7f3c38530a9cbc79932ea89ce092d1c2,"Fixing docstring in CTCLoss (#17307)

Summary:
The argument `zero_infinity` is in the wrong place! :)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17307

Differential Revision: D14154850

Pulled By: ezyang

fbshipit-source-id: 7a9fe537483b23041f21ba1b80375b7f44265538",igormq,https://api.github.com/repos/pytorch/pytorch/git/commits/94a95a0c7f3c38530a9cbc79932ea89ce092d1c2
be4ad3fe30b780f41a7db3185a3a6236291cfeb5,"fix(typo): Change 'integeral' to 'integer'

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/17396

Differential Revision: D14195023

Pulled By: soumith

fbshipit-source-id: 300ab68c24bfbf10768fefac44fad64784463c8f",HRK44,https://api.github.com/repos/pytorch/pytorch/git/commits/be4ad3fe30b780f41a7db3185a3a6236291cfeb5
5903522ad62ea96e4175ff03f025293ac496d7de,"refactor: a bit intricate so I refactor it (#16995)

Summary:
this code is a bit intricate so i refactor it
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16995

Differential Revision: D14050667

Pulled By: ifedan

fbshipit-source-id: 55452339c6518166f3d4bc9898b1fe2f28601dc4",knightXun,https://api.github.com/repos/pytorch/pytorch/git/commits/5903522ad62ea96e4175ff03f025293ac496d7de
72eb70c272d8373cf652855e5ac27755226a1bca,"' ' ==> ' ' (#17498)

Summary:
Fix formatting error for cpp code.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17498

Reviewed By: zou3519

Differential Revision: D14224549

Pulled By: fmassa

fbshipit-source-id: f1721c4a75908ded759aea8c561f2e1d66859eec",liangdzou,https://api.github.com/repos/pytorch/pytorch/git/commits/72eb70c272d8373cf652855e5ac27755226a1bca
cbefd0323b7250d286419cd956e453ce8cef1189,"Fix typo

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/17521

Differential Revision: D14237482

Pulled By: soumith

fbshipit-source-id: 636e0fbe2c667d15fcb649136a65ae64937fa0cb",hysts,https://api.github.com/repos/pytorch/pytorch/git/commits/cbefd0323b7250d286419cd956e453ce8cef1189
456d3e5f5612065627b5a3b2360f589989855be5,"Fix errors in the description for installation on Windows (#17475)

Summary:
+ All quotes for ENV VARS are erroneous;
+ Toolset hasn't be specified;
+ Provide paths for all 3 Visual Studio 2017 products: Community/Professional/Enterprise.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17475

Differential Revision: D14262968

Pulled By: soumith

fbshipit-source-id: c0504e0a6be9c697ead83b06b0c5cf569b5c8625",emankov,https://api.github.com/repos/pytorch/pytorch/git/commits/456d3e5f5612065627b5a3b2360f589989855be5
01977c0a896d4922762f6f56db0f370dad2edf4a,"Change fake tqdm constructor to match real tqdm (#17636)

Summary:
Currently, the fake tqdm implementation requires an input (whereas real tqdm does not).

This caused a problem in torchvision (https://github.com/pytorch/vision/pull/770), and seems likely to cause minor irritations elsewhere.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17636

Differential Revision: D14296530

Pulled By: ezyang

fbshipit-source-id: bc077d898773c93dab34c985a7b30525a43e558a",bryanhe,https://api.github.com/repos/pytorch/pytorch/git/commits/01977c0a896d4922762f6f56db0f370dad2edf4a
7a51c03a30100e89ccabb3e517d633ec6f47db76,"Fixed typo in torch/functional.py w/r/t broadcast_tensors (#17642)

Summary:
In reference to #17574
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17642

Differential Revision: D14297177

Pulled By: ezyang

fbshipit-source-id: 968176ea3b46a0153da0fd9e6b40db314d29e51c",jrichterpowell,https://api.github.com/repos/pytorch/pytorch/git/commits/7a51c03a30100e89ccabb3e517d633ec6f47db76
244d3309804a0fc6fd4e66940026df79a4b75ed9,"Fixed typo in aten/src/ATen/native_parse.py (#17641)

Summary:
Hi, there.
There is a typo in aten/src/ATen/native_parse.py, and I fix it.
`std::aray` -> `std::array`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17641

Differential Revision: D14301981

Pulled By: ezyang

fbshipit-source-id: a37859cdedcbf6c29333b954486dfa086d6c2176",wkcn,https://api.github.com/repos/pytorch/pytorch/git/commits/244d3309804a0fc6fd4e66940026df79a4b75ed9
e3516d0a952d41cb493f9c216290496293faa66f,"omit group conv NHWC test for GPU (#17715)

Summary:
Observed the test `TestGroupConvolution.test_group_convolution` to fail with the following error:

```
Falsifying example: test_group_convolution(self=<caffe2.python.operator_test.group_conv_test.TestGroupConvolution testMethod=test_group_convolution>, stride=3, pad=0, kernel=5, size=8, group=4, input_channels_per_group=7, output_channels_per_group=8, batch_size=2, order='NHWC', engine='', use_bias=False, gc=, dc=[, device_type: 1])

You can reproduce this example by temporarily adding reproduce_failure('3.59.1', b'AAAA') as a decorator on your test case
```
This example generated by hypothesis has `group=2, order='NHWC' and dc=[, device_type: 1])`.
I think this example should be skipped.

I have mimicked the change corresponding to [PR#13554](https://github.com/pytorch/pytorch/pull/13554) to skip this example.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17715

Differential Revision: D14346642

Pulled By: ezyang

fbshipit-source-id: b1f1fef09f625fdb43d31c7213854e61a96381ba",deepali-c,https://api.github.com/repos/pytorch/pytorch/git/commits/e3516d0a952d41cb493f9c216290496293faa66f
b87abdfc120137307467a253640467cb9a035da6,"typo fix

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/17653

Differential Revision: D14302003

Pulled By: ezyang

fbshipit-source-id: 8ad90985a392b07127c7e315d4e74ce77962b573",youkaichao,https://api.github.com/repos/pytorch/pytorch/git/commits/b87abdfc120137307467a253640467cb9a035da6
742568e7eb1ebb3679dcadce90cc89c83df3f162,"PyPy compatibility: let unmodified slots be inherited in the standard way (#17837)

Summary:
This is needed to fix a segfault on PyPy 3.6, see https://bitbucket.org/pypy/pypy/issues/2968/segfault-calling-cpyext_tp_new_tuple and https://github.com/pytorch/pytorch/issues/17835
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17837

Differential Revision: D14399408

Pulled By: soumith

fbshipit-source-id: 75328a30018313d3223dd3e3eef9240a416c049b",rlamy,https://api.github.com/repos/pytorch/pytorch/git/commits/742568e7eb1ebb3679dcadce90cc89c83df3f162
8045b3eb143ed70ab1873e148d266d56e0e1481f,"Registering of kl-divergence for independent distribution (#17681)

Summary:
This address issue https://github.com/pytorch/pytorch/issues/13545 and implements the proposed fix together with a single test.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17681

Differential Revision: D14360161

Pulled By: ezyang

fbshipit-source-id: 427afc88e9054b5b0dc39ebbab1087b990695ea5",SkafteNicki,https://api.github.com/repos/pytorch/pytorch/git/commits/8045b3eb143ed70ab1873e148d266d56e0e1481f
b9e8f56daa7ee61546d5adac8f8f4435253cc911,"Add PyTorch Governance, Contributor Guide, and List of Persons of Interest

Summary: Adding new documents to the PyTorch website to describe how PyTorch is governed, how to contribute to the project, and lists persons of interest.

Reviewed By: orionr

Differential Revision: D14394573

fbshipit-source-id: ad98b807850c51de0b741e3acbbc3c699e97b27f",ericnakagawa,https://api.github.com/repos/pytorch/pytorch/git/commits/b9e8f56daa7ee61546d5adac8f8f4435253cc911
9ecee93a1668e535942d0f0079f3c9465748edfe,"Fix minor grammatical mistakes in torch/nn/modules/loss.py (#17892)

Summary:
Fixes some minor grammatical mistakes in the doc of `loss.py`.

I think in the doc:
>  Note that for some losses, there multiple elements per sample.

the ""are"" is lost between ""there"" and ""multiple"".

This mistake takes place in all the descriptions of parameter `size_average` and there are 17 of them.
It's minor but perfects the doc I think. ðŸ˜
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17892

Differential Revision: D14418177

Pulled By: ezyang

fbshipit-source-id: 412759f2f9b215819463bf8452ab0e0513218cd6",z-yin,https://api.github.com/repos/pytorch/pytorch/git/commits/9ecee93a1668e535942d0f0079f3c9465748edfe
0f7e6f293b9a56d5d064d3f169dc4a0896f11e28,"Make Variable::set_data non-const; cosmetic fixes.

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/17761

Differential Revision: D14406603

Pulled By: ezyang

fbshipit-source-id: bc8bba73352eb4b3e21196b36522e9cec70f6676",danpovey,https://api.github.com/repos/pytorch/pytorch/git/commits/0f7e6f293b9a56d5d064d3f169dc4a0896f11e28
4ad17c9031680dc4d5997bc4999ea4a93d33c8d0,"Misleading documentation for module._load_from_state_dict (#17618)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17618

Base on the code, we only add key to `missing_keys` and `unexpected_keys` if `$strict` is `True`. The documentation is confusing.

This diff also fix one FLAKE8 warning.

Reviewed By: ailzhang

Differential Revision: D14280593

fbshipit-source-id: d368f5596bdf74ff62ee4d28d79120f5af91e0a3",kazhang,https://api.github.com/repos/pytorch/pytorch/git/commits/4ad17c9031680dc4d5997bc4999ea4a93d33c8d0
f7b70a69e5095a59dcec755cace5016bf2ad9c9e,"Fix Windows build (#17917)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17917

D14375995 introduced instantiation of the following templates with `bool` type (more specifically `To` is `int64_t`, `From` is `bool`):
```
template <typename To, typename From>
typename std::enable_if<std::is_integral<From>::value, bool>::type overflows(
    From f) {
  using limit = std::numeric_limits<typename scalar_value_type<To>::type>;
  if (!limit::is_signed && std::numeric_limits<From>::is_signed) {
    // allow for negative numbers to wrap using two's complement arithmetic.
    // For example, with uint8, this allows for `a - b` to be treated as
    // `a + 255 * b`.
    return f > limit::max() ||
        (f < 0 && -static_cast<uint64_t>(f) > limit::max());
  } else {
    return f < limit::lowest() || f > limit::max();
  }
}

template <typename To, typename From>
typename std::enable_if<std::is_floating_point<From>::value, bool>::type
overflows(From f) {
  using limit = std::numeric_limits<typename scalar_value_type<To>::type>;
  if (limit::has_infinity && std::isinf(static_cast<double>(f))) {
    return false;
  }
  if (!limit::has_quiet_NaN && (f != f)) {
    return true;
  }
  return f < limit::lowest() || f > limit::max();
}
```
MSVC gives C4804 warning and because ""treat warnings as errors"" is on it fails to build on Windows. Disabling such warning for those 2 templates.

Reviewed By: mingzhe09088

Differential Revision: D14421157

fbshipit-source-id: e72ba34406628c84da48518b32a46f851819bad1",blackm00n,https://api.github.com/repos/pytorch/pytorch/git/commits/f7b70a69e5095a59dcec755cace5016bf2ad9c9e
f6de833cac63016de7f0f04b260c734f19e46505,"Update docs for `mark_non_differentiable` method (#17891)

Summary:
The current documentation doesn't reflect the real values of tensors during the backward pass.
This issue is mentioned in https://github.com/pytorch/pytorch/issues/12631
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17891

Differential Revision: D14419949

Pulled By: soumith

fbshipit-source-id: 8b495628c3f017bc880f8096682cd176a53974e5",serhii-havrylov,https://api.github.com/repos/pytorch/pytorch/git/commits/f6de833cac63016de7f0f04b260c734f19e46505
ecc5e623a29a825937ab6e8e9c4b4156b5a48626,"fix punctuation

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/17973

Differential Revision: D14438725

Pulled By: zou3519

fbshipit-source-id: 30a5485b508b4ae028057e0b66a8abb2b163d66b",livc,https://api.github.com/repos/pytorch/pytorch/git/commits/ecc5e623a29a825937ab6e8e9c4b4156b5a48626
26a4c2ada6aadace7b0c62658eb9f567ff313951,"Speed up gemm by reordering the for loops (#17730)

Summary:
Optimize the order of the ""for"" loops.

Note: For ""transa = true"" cases, the order of the ""for"" loops has been optimzied in the original code. Therefore, no significant improvement is observed in those case (i.e. ""transa && transb"" and ""transa && !transb"")

mode/opt (i.e. static libary)
//////////////////////////////////////////////////////////////////////////////
transa && transb
after:
loops:  2229     x:     128      y:     128      z:     128      time:  2243ns      =>  acceleration multiplier:  0.90
loops:  124      x:     128      y:     1024     z:     128      time:  40381ns      =>  acceleration multiplier:  0.97
loops:  121      x:     1024     y:     128      z:     128      time:  41651ns      =>  acceleration multiplier:  0.96
loops:  15       x:     1024     y:     1024     z:     128      time:  333771ns       =>  acceleration multiplier:  0.98
loops:  4610     x:     128      y:     128      z:     64       time:  1084ns       =>  acceleration multiplier:  0.95
loops:  252      x:     128      y:     1024     z:     64       time:  19860ns      =>  acceleration multiplier:  0.98
loops:  248      x:     1024     y:     128      z:     64       time:  20232ns      =>  acceleration multiplier:  0.98
loops:  30       x:     1024     y:     1024     z:     64       time:  167338ns      =>  acceleration multiplier:  0.99

before:
loops:  2468     x:     128      y:     128      z:     128      time:  2026ns
loops:  128      x:     128      y:     1024     z:     128      time:  39338ns
loops:  126      x:     1024     y:     128      z:     128      time:  39930ns
loops:  16       x:     1024     y:     1024     z:     128      time:  327549ns
loops:  4840     x:     128      y:     128      z:     64       time:  1033ns
loops:  258      x:     128      y:     1024     z:     64       time:  19441ns
loops:  252      x:     1024     y:     128      z:     64       time:  19854ns
loops:  31       x:     1024     y:     1024     z:     64       time:  166254ns

//////////////////////////////////////////////////////////////////////////////
transa && !transb
after:
loops:  4880     x:     128      y:     128      z:     128      time:  1024ns      =>  acceleration multiplier:  0.98
loops:  638      x:     128      y:     1024     z:     128      time:  7839ns      =>  acceleration multiplier:  1.04
loops:  605      x:     1024     y:     128      z:     128      time:  8276ns      =>  acceleration multiplier:  1.01
loops:  77       x:     1024     y:     1024     z:     128      time:  65713ns      =>  acceleration multiplier:  1.00
loops:  9935     x:     128      y:     128      z:     64       time:  503ns      =>  acceleration multiplier:  1.00
loops:  1252     x:     128      y:     1024     z:     64       time:  3994ns      =>  acceleration multiplier:  1.00
loops:  1183     x:     1024     y:     128      z:     64       time:  4226ns      =>  acceleration multiplier:  0.98
loops:  153      x:     1024     y:     1024     z:     64       time:  32766ns      =>  acceleration multiplier:  0.99

before:
loops:  4985     x:     128      y:     128      z:     128      time:  1003ns
loops:  615      x:     128      y:     1024     z:     128      time:  8140ns
loops:  599      x:     1024     y:     128      z:     128      time:  8357ns
loops:  76       x:     1024     y:     1024     z:     128      time:  65934ns
loops:  9897     x:     128      y:     128      z:     64       time:  505ns
loops:  1248     x:     128      y:     1024     z:     64       time:  4008ns
loops:  1203     x:     1024     y:     128      z:     64       time:  4159ns
loops:  154      x:     1024     y:     1024     z:     64       time:  32499ns

//////////////////////////////////////////////////////////////////////////////
!transa && transb
after:
loops:  3919     x:     128      y:     128      z:     128      time:  1276ns      =>  acceleration multiplier:  2.97
loops:  497      x:     128      y:     1024     z:     128      time:  10069ns      =>  acceleration multiplier:  7.85
loops:  449      x:     1024     y:     128      z:     128      time:  11145ns      =>  acceleration multiplier:  4.77
loops:  57       x:     1024     y:     1024     z:     128      time:  88595ns      =>  acceleration multiplier:  7.12
loops:  7575     x:     128      y:     128      z:     64       time:  660ns      =>  acceleration multiplier:  3.00
loops:  967      x:     128      y:     1024     z:     64       time:  5173ns      =>  acceleration multiplier:  7.66
loops:  877      x:     1024     y:     128      z:     64       time:  5702ns      =>  acceleration multiplier:  4.76
loops:  111      x:     1024     y:     1024     z:     64       time:  45232ns      =>  acceleration multiplier:  7.03

before:
loops:  1320     x:     128      y:     128      z:     128      time:  3789ns
loops:  64       x:     128      y:     1024     z:     128      time:  79061ns
loops:  95       x:     1024     y:     128      z:     128      time:  53107ns
loops:  8        x:     1024     y:     1024     z:     128      time:  631161ns
loops:  2521     x:     128      y:     128      z:     64       time:  1983ns
loops:  127      x:     128      y:     1024     z:     64       time:  39604ns
loops:  185      x:     1024     y:     128      z:     64       time:  27128ns
loops:  16       x:     1024     y:     1024     z:     64       time:  318155ns

//////////////////////////////////////////////////////////////////////////////
!transa && !transb
after:
loops:  3895     x:     128      y:     128      z:     128      time:  1283ns      =>  acceleration multiplier:  1.73
loops:  393      x:     128      y:     1024     z:     128      time:  12746ns      =>  acceleration multiplier:  3.36
loops:  411      x:     1024     y:     128      z:     128      time:  12170ns      =>  acceleration multiplier:  1.93
loops:  46       x:     1024     y:     1024     z:     128      time:  110116ns      =>  acceleration multiplier:  3.17
loops:  7404     x:     128      y:     128      z:     64       time:  675ns      =>  acceleration multiplier:  1.58
loops:  636      x:     128      y:     1024     z:     64       time:  7872ns      =>  acceleration multiplier:  2.70
loops:  724      x:     1024     y:     128      z:     64       time:  6911ns      =>  acceleration multiplier:  1.32
loops:  73       x:     1024     y:     1024     z:     64       time:  68502ns      =>  acceleration multiplier:  2.49

before:
loops:  2253     x:     128      y:     128      z:     128      time:  2219ns
loops:  117      x:     128      y:     1024     z:     128      time:  42788ns
loops:  214      x:     1024     y:     128      z:     128      time:  23465ns
loops:  15       x:     1024     y:     1024     z:     128      time:  349076ns
loops:  4694     x:     128      y:     128      z:     64       time:  1065ns
loops:  236      x:     128      y:     1024     z:     64       time:  21251ns
loops:  549      x:     1024     y:     128      z:     64       time:  9108ns
loops:  30       x:     1024     y:     1024     z:     64       time:  170799ns
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17730

Differential Revision: D14325149

Pulled By: zhangguanheng66

fbshipit-source-id: a7a5a83890fdf99fee6eb87a3a5060b7b6bd862f",zhangguanheng66,https://api.github.com/repos/pytorch/pytorch/git/commits/26a4c2ada6aadace7b0c62658eb9f567ff313951
1e42720a77109f68ca04d44d306e2d3040e10e45,"Fix some typos in distributed.py.

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/17959

Differential Revision: D14437347

Pulled By: soumith

fbshipit-source-id: 4c33571f56e9da687666516a310f91924cddd4d9",elliotwaite,https://api.github.com/repos/pytorch/pytorch/git/commits/1e42720a77109f68ca04d44d306e2d3040e10e45
54ef852d7fb1aff2b43cc70207f5bae227158689,"Fix unclosed files in download.py, test_onnxifi.py, test_trt.py (#18017)

Summary:
According to https://docs.python.org/3/tutorial/inputoutput.html, it is good practice to use the ""with"" keyword when dealing with file objects. If not, you should call f.close() to close the file and immediately free up any system resources used by it.  Thus, I adjust the open file function to ""with open() as f"".
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18017

Differential Revision: D14475112

Pulled By: ezyang

fbshipit-source-id: d1c0821e39cb8a09f86d6d08b437b4a99746416c",ttup7777,https://api.github.com/repos/pytorch/pytorch/git/commits/54ef852d7fb1aff2b43cc70207f5bae227158689
aafbefa4d624f06de94683f6b3741e7759090d2b,"Remove the identical if branch (#18019)

Summary:
elif branch and else branch have the same content.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18019

Differential Revision: D14475107

Pulled By: ezyang

fbshipit-source-id: 5075cc938f57649af7537de1a7c9d76ea976cafc",BARBAPAPA215,https://api.github.com/repos/pytorch/pytorch/git/commits/aafbefa4d624f06de94683f6b3741e7759090d2b
cb2ea1770791ac5a942f51d1cb1268e288abe86b,"Automatic update of fbcode/foxi to 2bcc4064c90e87b9638615c733485f07c47b7558 (#18070)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18070

Previous import was d1f45b1a2b1585d0e9bc65e15e463db344fc3ff6

Included changes:
- **[2bcc406](https://github.com/houseroad/foxi/commit/2bcc406)**: Merge pull request #7 from jackm321/tracing_fixes <Jack Montgomery>
- **[c39033c](https://github.com/houseroad/foxi/commit/c39033c)**: Fixes for tracing events <Jack Montgomery>
- **[50912cf](https://github.com/houseroad/foxi/commit/50912cf)**: Merge pull request #5 from jackm321/add_trace_events <Jack Montgomery>
- **[ba2fdcb](https://github.com/houseroad/foxi/commit/ba2fdcb)**: Merge pull request #5 from jackm321/add_trace_events <Jack Montgomery>
- **[7d42b12](https://github.com/houseroad/foxi/commit/7d42b12)**: address comments <Jack Montgomery>
- **[dcabd8d](https://github.com/houseroad/foxi/commit/dcabd8d)**: Add trace events interface <Jack Montgomery>

Reviewed By: houseroad

Differential Revision: D14483201

fbshipit-source-id: f51ed869c9a89521079df89903abc0ac0a45ac7b",jackm321,https://api.github.com/repos/pytorch/pytorch/git/commits/cb2ea1770791ac5a942f51d1cb1268e288abe86b
2737d2c7dc2b4f56e7f000d24120b601c4d3618f,"delete unnecessary file .gitkeep (#18136)

Summary:
delete unnecessary file .gitkeep in /pytorch/tree/master/torch/csrc/nn
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18136

Differential Revision: D14516584

Pulled By: ezyang

fbshipit-source-id: a7555693cb3df1c5e37fcd3ca9bb379a2258f2d1",zdong1995,https://api.github.com/repos/pytorch/pytorch/git/commits/2737d2c7dc2b4f56e7f000d24120b601c4d3618f
afb2f2424a8509b2fc32f38a5b9ba88b91ec7a63,"Increase line-width of Declarations.yaml (#18050)

Summary:
There are some line breaks in schema_string of Declarations.yaml.
Is this valid yaml? I am reading yaml-spec.
It seems that the â€œ|â€ indicator or single/double quote is required to insert line-break.
https://yaml.org/spec/1.2/spec.html
![image](https://user-images.githubusercontent.com/2469618/54405834-1e53ac80-471b-11e9-9925-be13a109eb46.png)
Could you increase line-width of yaml to avoid newline?
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18050

Differential Revision: D14516694

Pulled By: ezyang

fbshipit-source-id: 1db9f3bf131b54a783d668de973915892603189e",junjihashimoto,https://api.github.com/repos/pytorch/pytorch/git/commits/afb2f2424a8509b2fc32f38a5b9ba88b91ec7a63
195cba500f2c98c10f3d4b8fc3cca6493099b6df,"Fix Caffe2 operator schemas (#15462) (#13229) (#18109)

Summary:
Maratyszcza harouwu yinghai

This is broken since #13065. `c_str()` returns a pointer that isn't permanent.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18109

Differential Revision: D14516622

Pulled By: ezyang

fbshipit-source-id: 7113d92eac4f61479c4c7b323cf78cc8aa00b17e",lutzroeder,https://api.github.com/repos/pytorch/pytorch/git/commits/195cba500f2c98c10f3d4b8fc3cca6493099b6df
8ed2b88bf1ce39ba166e21e262d4f6b97bd96acd,"Corrected type of 'swap' in torch.nn.TripletMarginLoss (#18115)

Summary:
Fix #16428 by correcting type of 'swap' from `float` to `bool`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18115

Differential Revision: D14516615

Pulled By: ezyang

fbshipit-source-id: c61a45d533f3a443edf3c31c1ef3d9742bf46d2b",Bharat123rox,https://api.github.com/repos/pytorch/pytorch/git/commits/8ed2b88bf1ce39ba166e21e262d4f6b97bd96acd
1c76746f61598cc3c3f05057fd8ebc5e771338fc,"SGD: remove unneeded multiply-add initialization operations (#18114)

Summary:
The momentum buffer is initialized to the value of
d_p, but the current code takes the long way to do this:
1. Create a buffer of zeros
2. Multiply the buffer by the momentum coefficient
3. Add d_p to the buffer

All of these can be collapsed into a single step:
1. Create a clone of d_p
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18114

Differential Revision: D14509122

Pulled By: ezyang

fbshipit-source-id: 4a79b896201d5ff20770b7ae790c244ba744edb8",nzmora,https://api.github.com/repos/pytorch/pytorch/git/commits/1c76746f61598cc3c3f05057fd8ebc5e771338fc
abc171bd53ee00caa3ab638b770316583a0c6c5f,"Fix typo in docstring

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/18216

Differential Revision: D14539824

Pulled By: ezyang

fbshipit-source-id: 490b72951a75f3f8b949a2d692d660a3693ee98a",hikjik,https://api.github.com/repos/pytorch/pytorch/git/commits/abc171bd53ee00caa3ab638b770316583a0c6c5f
1c671c56c16b203ececd1ee6f70a0f6fcb9265a5,"Fix contribution_guide docs (#18237)

Summary:
Fixes Typo and a Link in the `docs/source/community/contribution_guide.rst`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18237

Differential Revision: D14566907

Pulled By: ezyang

fbshipit-source-id: 3a75797ab6b27d28dd5566d9b189d80395024eaf",kshitij12345,https://api.github.com/repos/pytorch/pytorch/git/commits/1c671c56c16b203ececd1ee6f70a0f6fcb9265a5
f6df6aed89c00f3baa270998417ce2b8ca5756c9,"Optimize MomentumSGDUpdate maximum block size and make it templated

Summary: Removing the maximum number of blocks limit from the operator and making the nesterov parameter templated to remove branching.

Reviewed By: BIT-silence

Differential Revision: D14567003

fbshipit-source-id: 394c2039ee214adc6ccd2e562e4e9563d307131f",bilgeacun,https://api.github.com/repos/pytorch/pytorch/git/commits/f6df6aed89c00f3baa270998417ce2b8ca5756c9
e5eb871419fc80799a8126fefe147d8fb4a04a2f,"Fix DCHECK to handle dangling else (#18295)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18295

Replace ""if (false)"" with ""while (false)"" which fixes potential dangling else issue as shown in added test case.

Reviewed By: ezyang

Differential Revision: D14569608

fbshipit-source-id: 407052db9182ce27b7a59841e90fa50d3eca262e",ljk53,https://api.github.com/repos/pytorch/pytorch/git/commits/e5eb871419fc80799a8126fefe147d8fb4a04a2f
8bc5b867093f1b44f5168f3e41b1bdf0ba3e05a7,"Added tensor size warning to F.mse_loss() (#18349)

Summary:
To address the issue of broadcasting giving the wrong result in `nn.MSELoss()` as mentioned here https://github.com/pytorch/pytorch/issues/16045 . In particular, the issue often arises when computing the loss between tensors with shapes (n, 1) and (n,)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18349

Differential Revision: D14594176

Pulled By: soumith

fbshipit-source-id: f23ae68a4bf42f3554ad7678a314ba2c7532a6db",mc-robinson,https://api.github.com/repos/pytorch/pytorch/git/commits/8bc5b867093f1b44f5168f3e41b1bdf0ba3e05a7
a4f83fff2b7fb0e7ca7b584648c98165b2576ed4,"Only look for Caffe2 package when shared (#18421)

Summary:
Previously it would look for the Config even if it was not written.

Fixed #18419
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18421

Differential Revision: D14597139

Pulled By: ezyang

fbshipit-source-id: c212cbf5dc91564c12d9d07e507c8285e11c6bdf",xsacha,https://api.github.com/repos/pytorch/pytorch/git/commits/a4f83fff2b7fb0e7ca7b584648c98165b2576ed4
ed8c462dc79461c434cdd69f378743d1258c624b,"Fix caffe2 build with BLAS=OpenBLAS (#18422)

Summary:
g++ complains about failing to find the declaration of cblas_sscal and cblas_dscal BLAS function
let's fix it  :)

fedora 29, gcc 8.3.1, openblas 0.3.5
build with cmake -DBLAS=OpenBLAS ..
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18422

Differential Revision: D14598977

Pulled By: soumith

fbshipit-source-id: bde77bfb359d2ff38226401caeed78c114ef7468",nihui,https://api.github.com/repos/pytorch/pytorch/git/commits/ed8c462dc79461c434cdd69f378743d1258c624b
bbe110f4e1ed28da3fa528071a1985453ef36caa,"Updating onnxtrt submodule to master branch

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/18441

Differential Revision: D14613517

Pulled By: bddppq

fbshipit-source-id: dd20d718db55942df9cce7acd1151d6902bc57ff",kyhchen,https://api.github.com/repos/pytorch/pytorch/git/commits/bbe110f4e1ed28da3fa528071a1985453ef36caa
bdd098c694246f2b27c7e743d4f383739cc361a8,"Fix typo in Github links in elementwise_ops_schema.cc (#18018)

Summary:
s/elementwise_op_schema.cc/elementwise_ops_schema.cc
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18018

Differential Revision: D14612291

Pulled By: soumith

fbshipit-source-id: 09276283b9ff92c039ce530165c62cc8421fb443",isameer,https://api.github.com/repos/pytorch/pytorch/git/commits/bdd098c694246f2b27c7e743d4f383739cc361a8
45ec4920e32d1422342148afedf3254675a7e5d7,"how to use the `ccache` package on Ubuntu (#18495)

Summary:
Added full instructions for how to use the `ccache` package. Thanks.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18495

Differential Revision: D14635351

Pulled By: ezyang

fbshipit-source-id: 158e1052bae580e95f73644252fdbddcc0213128",stas00,https://api.github.com/repos/pytorch/pytorch/git/commits/45ec4920e32d1422342148afedf3254675a7e5d7
defe67caf2b45b18abe7e58bdf56087cbae81470,"Generate sphinx docs with secure content. (#18508)

Summary:
There are a number of pages in the docs that serve insecure content. AFAICT this is the sole source of that.

I wasn't sure if docs get regenerated for old versions as part of the automation, or if those would need to be manually done.

cf. https://github.com/pytorch/pytorch.github.io/pull/177
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18508

Differential Revision: D14645665

Pulled By: zpao

fbshipit-source-id: 003563b06048485d4f539feb1675fc80bab47c1b",zpao,https://api.github.com/repos/pytorch/pytorch/git/commits/defe67caf2b45b18abe7e58bdf56087cbae81470
c3e3c5cc39165470ddab5afb6373399fdbd6598e,"Skip tests if C2/ONNX models cannot be read (#18494)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18494

Today we have some C2 end2end test run requiring reading model data from external filesystem (for example, Gluster and AWS). This could be a source for flaky test when the external filesystems are not reachable during the tests.

In this diff, we add try/catch logic around where we download models and open model files from external system. In case such attempts fails, we will catch the excption and let the unittest skip the current test instead of failure.

I also refactor the code a little bit by removing some duplicated logic on downloading and build the c2 model data. It has been duplicated in two classes and a few functions...

Reviewed By: yinghai

Differential Revision: D14442241

fbshipit-source-id: da8bf56c8d096efa34ca2070de5cd10a18aad70c",nimin98,https://api.github.com/repos/pytorch/pytorch/git/commits/c3e3c5cc39165470ddab5afb6373399fdbd6598e
8635078d9ed47266e6df89ad5ef887b598a76f25,"Adds Cyclical Learning Rate and Momentum (#18001)

Summary:
This implements a cyclical learning rate (CLR) schedule with an optional inverse cyclical momentum. More info about CLR: https://github.com/bckenstler/CLR

This is finishing what #2016 started. Resolves #1909.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18001

Differential Revision: D14451845

Pulled By: sampepose

fbshipit-source-id: 8f682e0c3dee3a73bd2b14cc93fcf5f0e836b8c9",sampepose,https://api.github.com/repos/pytorch/pytorch/git/commits/8635078d9ed47266e6df89ad5ef887b598a76f25
2f174e945384e87843733999abc5f16e54cd974b,"in caching allocator, ignore and clear the error if not ready

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/18584

Differential Revision: D14675041

Pulled By: bddppq

fbshipit-source-id: c1fab797e0d224e0a481a0395a3f9975c4265ff6",jeffdaily,https://api.github.com/repos/pytorch/pytorch/git/commits/2f174e945384e87843733999abc5f16e54cd974b
12abc8a99a5fc60603b3aecf5faa37600ad4fff6,"Target and input sizes mismatch warning in L1 Loss / L1 Smooth Loss (#18565)

Summary:
Addind the same warning message already present in the mse_loss function to the L1 losses when input and target sizes are different.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18565

Differential Revision: D14671415

Pulled By: soumith

fbshipit-source-id: 01f5e1fb1ea119dbb2aecf1d94d0cb462f284982",aurelien-roy,https://api.github.com/repos/pytorch/pytorch/git/commits/12abc8a99a5fc60603b3aecf5faa37600ad4fff6
37529161326a001e8398bc5b4e75791db94c04b0,"Serialization supports pathlib.Path object for the input argument (#18562)

Summary:
This will allow pathlib.Path object to the torch.load as an input argument.
Fixes #16607
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18562

Differential Revision: D14668255

Pulled By: soumith

fbshipit-source-id: 0ae4f7c210918582912f2d1ef2a98f1ab288c540",NySunShine,https://api.github.com/repos/pytorch/pytorch/git/commits/37529161326a001e8398bc5b4e75791db94c04b0
c189eba3e139aca43974e63dba9fb972e88c9d74,"Fixed torch.arange docs (#18604)

Summary:
Kindly let me know if its okay and if any places i need to make a fix. Closes #18534
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18604

Differential Revision: D14680712

Pulled By: soumith

fbshipit-source-id: 030e4a3d8f7839cbe2b8a3ef386323f0d39eb81a",iArunava,https://api.github.com/repos/pytorch/pytorch/git/commits/c189eba3e139aca43974e63dba9fb972e88c9d74
95d3825e48ad8eb6caab2374d831b4e76824dc25,"ReduceLrOnPlateau: best=current -> best=copy(current) (#16364) (#16697)

Summary:
Fixes #16364
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16697

Differential Revision: D14680879

Pulled By: soumith

fbshipit-source-id: c50c22f3eacea4474fb3a04fe85fbf11d5a177c9",sorenrasmussenai,https://api.github.com/repos/pytorch/pytorch/git/commits/95d3825e48ad8eb6caab2374d831b4e76824dc25
96456bfa4cf9394c9c926b143cf724a09901908d,"Update documentation for CTCLoss (#18415)

Summary:
This is meant to resolve #18249, where I pointed out a few things that could improve the CTCLoss docs.

My main goal was to clarify:
- Target sequences are sequences of class indices, excluding the blank index
- Lengths of `target` and `input` are needed for masking unequal length sequences, and do not necessarily = S, which is the length of the longest sequence in the batch.

I thought about Thomas's suggestion to link the distill.pub article, but I'm not sure about it. I think that should be up to y'all to decide.

I have no experience with .rst, so it might not render as expected :)

t-vi ezyang
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18415

Differential Revision: D14691969

Pulled By: soumith

fbshipit-source-id: 381a2d52307174661c58053ae9dfae6e40cbfd46",rlorigro,https://api.github.com/repos/pytorch/pytorch/git/commits/96456bfa4cf9394c9c926b143cf724a09901908d
fba89b2ae1fa294deff2b71274905dd7e27ad466,"fix typo

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/18653

Differential Revision: D14713920

Pulled By: ezyang

fbshipit-source-id: 170295a162dd23916c1dcc9330918d33277cc9ed",MarkPare,https://api.github.com/repos/pytorch/pytorch/git/commits/fba89b2ae1fa294deff2b71274905dd7e27ad466
5ade96fc8461cf40f8bddd9c1a562ebd0c657a0c,"Update cpp_extension.py (#18638)

Summary:
Hi. It seems that when building CPP-extensions with CUDA for Windows, an `extra_cuda_cflags` options are not properly forwarded to `nvcc`.

Use of extra CUDA options is necessary to build, for instance, a InplaceABN (https://github.com/mapillary/inplace_abn), which requires `--expt-extended-lambda` option.

This PR adds one line that correctly appends `extra_cuda_cflags`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18638

Differential Revision: D14704270

Pulled By: ezyang

fbshipit-source-id: e1e330d193d9afd5707a5437a74c0499460d2b90",BloodAxe,https://api.github.com/repos/pytorch/pytorch/git/commits/5ade96fc8461cf40f8bddd9c1a562ebd0c657a0c
8ca9ba17da9ebe73d39082ddd71aae436c663afd,"Fix typo

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/18802

Differential Revision: D14781874

Pulled By: ezyang

fbshipit-source-id: 0f94c40bd84c84558ea3329117580f6c749c019f",Wanwannodao,https://api.github.com/repos/pytorch/pytorch/git/commits/8ca9ba17da9ebe73d39082ddd71aae436c663afd
d108a1abb7b0d12cadf6b106521ceb347b1aab0f,"Add a .ctags.d/ toplevel directory (#18827)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18827
ghimport-source-id: 38f857bc29b2c2c6a71069d00c4c69ed0bef1574

Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#18827 Add a .ctags.d/ toplevel directory**

Exclude build artifacts by default.

Reviewed By: ezyang

Differential Revision: D14765721

fbshipit-source-id: a785dbb2ef1df96af8e23cc65c8db2a6b67b4fce",mxw,https://api.github.com/repos/pytorch/pytorch/git/commits/d108a1abb7b0d12cadf6b106521ceb347b1aab0f
d35c39e73b04d7ab95812be8cbda21023e1b5cc4,"don't attempt to multiply by a sparse matrix (#18737)

Summary:
Tested by running the script in #16562 , and there was no error.

Then:
```
>>> print(mat.grad)
tensor([[1., 2., 3.],
        [1., 2., 3.],
        [1., 2., 3.]])
```

which is correct.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18737

Differential Revision: D14773078

Pulled By: umanwizard

fbshipit-source-id: 8aa36eb6f6aa104263a467d9ac91d61b3bfd05f5",umanwizard,https://api.github.com/repos/pytorch/pytorch/git/commits/d35c39e73b04d7ab95812be8cbda21023e1b5cc4
c1790fa202f30e3aca1d1ecb31f26e0b3bb1e69f,"More numerically stable lerp (#18871)

Summary:
The C++ and CUDA implementations of the lerp are not numerically stable. This is discussed on Wikipedia [here](https://en.wikipedia.org/wiki/Linear_interpolation#Programming_language_support). I checked the GPU SASS output and there's no overhead from using the more precise implementation, from Kepler all the way to Turing. I haven't looked at CPU ASM though.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18871

Differential Revision: D14793438

Pulled By: ezyang

fbshipit-source-id: 2ddc2e026c5285466cae7d1b4101174253100445",mkolod,https://api.github.com/repos/pytorch/pytorch/git/commits/c1790fa202f30e3aca1d1ecb31f26e0b3bb1e69f
b90cbb841d98aeae435b1d02b1ff211e028cdf5b,"Method is supposed to be in-place (#18684)

Summary:
Tracing models which attempts to return this in-place value doesn't turn out well.

I haven't run any tests to confirm the results to be honest, but regardless of the outcome, the operation happens in-place, so it should work as before.

Sample output from traced model attempting to set `max_norm` on `Embedding`:
```
a leaf Variable that requires grad has been used in an in-place operation. (check_inplace at /pytorch/torch/csrc/autograd/VariableTypeUtils.h:49)
frame #0: std::function<std::string ()>::operator()() const + 0x11 (0x7f0ecc5cc021 in /usr/local/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7f0ecc5cb8ea in /usr/local/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #2: <unknown function> + 0x38ab2f (0x7f0ecb55ab2f in /usr/local/lib/python3.7/site-packages/torch/lib/libtorch.so.1)
frame #3: torch::autograd::VariableType::embedding_renorm_(at::Tensor&, at::Tensor const&, double, double) const + 0x76 (0x7f0ecb5b5966 in /usr/local/lib/python3.7/site-packages/torch/lib/libtorch.so.1)
frame #4: <unknown function> + 0x56c958 (0x7f0ecb73c958 in /usr/local/lib/python3.7/site-packages/torch/lib/libtorch.so.1)
frame #5: <unknown function> + 0x672286 (0x7f0ecb842286 in /usr/local/lib/python3.7/site-packages/torch/lib/libtorch.so.1)
frame #6: torch::jit::InterpreterState::run(std::vector<c10::IValue, std::allocator<c10::IValue> >&) + 0x22 (0x7f0ecb83d842 in /usr/local/lib/python3.7/site-packages/torch/lib/libtorch.so.1)
frame #7: <unknown function> + 0x65c6ac (0x7f0ecb82c6ac in /usr/local/lib/python3.7/site-packages/torch/lib/libtorch.so.1)
frame #8: <unknown function> + 0x3c8ab4 (0x7f0f06bc0ab4 in /usr/local/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x3ad2c3 (0x7f0f06ba52c3 in /usr/local/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x11663e (0x7f0f0690e63e in /usr/local/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
frame #39: python_call + 0x11 (0x5563c3c521c1 in uwsgi)
frame #40: uwsgi_request_wsgi + 0x100 (0x5563c3c54410 in uwsgi)
frame #41: wsgi_req_recv + 0xac (0x5563c3becabc in uwsgi)
frame #42: simple_loop_run + 0xc4 (0x5563c3c35be4 in uwsgi)
frame #43: simple_loop + 0x10 (0x5563c3c35a00 in uwsgi)
frame #44: uwsgi_ignition + 0x241 (0x5563c3c3a3a1 in uwsgi)
frame #45: uwsgi_worker_run + 0x275 (0x5563c3c3ec35 in uwsgi)
frame #46: <unknown function> + 0x8f22c (0x5563c3c3f22c in uwsgi)
frame #47: <unknown function> + 0x3c13e (0x5563c3bec13e in uwsgi)
frame #48: __libc_start_main + 0xf1 (0x7f0f138922e1 in /lib/x86_64-linux-gnu/libc.so.6)
frame #49: _start + 0x2a (0x5563c3bec16a in uwsgi)
:
operation failed in interpreter:
op_version_set = 0
def forward(self,
    input_1: Tensor) -> Tensor:
  _0 = torch.norm(self.item_embedding.weight, 2, 1, True)
  _1 = torch.div(self.item_embedding.weight, _0)
  m_weight = torch.t(_1)
  input_2 = torch.contiguous(input_1)
  weight_1 = torch.embedding_renorm_(self.item_embedding.weight, input_2, 1., 2.)
             ~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
  x = torch.embedding(weight_1, input_2, -1, False, False)
  input_3 = torch.div(x, torch.norm(x, 2, 2, True))
  max_batch_size = ops.prim.NumToTensor(torch.size(input_3, 0))
  hx = torch.zeros([2, int(max_batch_size), 70], dtype=6, layout=0, device=torch.device(""cpu""))
  _2 = [self.lstm_layer.weight_ih_l0, self.lstm_layer.weight_hh_l0, self.lstm_layer.weight_ih_l1, self.lstm_layer.weight_hh_l1]
  input_4, _3, _4 = torch.lstm(input_3, [hx, hx], _2, False, 2, 0.10000000000000001, False, False, True)
  input = torch.matmul(input_4, torch.t(self.rnn2item.weight))
  tastevec = torch.div(input, torch.norm(input, 2, 2, True))
  outputs = torch.matmul(tastevec, m_weight)
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18684

Differential Revision: D14782041

Pulled By: ezyang

fbshipit-source-id: 7b2fc19b7d5b6600263644498bb728319a19f39d",NegatioN,https://api.github.com/repos/pytorch/pytorch/git/commits/b90cbb841d98aeae435b1d02b1ff211e028cdf5b
8e1e29124de99c01d08a2e2c02455c72335a971d,"Fix precision issue with expansion that prefers 'probs' over 'logits' (#18614)

Summary:
I have experienced that sometimes both were in `__dict__`, but it chose to copy `probs` which loses precision over `logits`. This is especially important when training (bayesian) neural networks or doing other type of optimization, since the loss is heavily affected.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18614

Differential Revision: D14793486

Pulled By: ezyang

fbshipit-source-id: d4ff5e34fbb4021ea9de9f58af09a7de00d80a63",ahmadsalim,https://api.github.com/repos/pytorch/pytorch/git/commits/8e1e29124de99c01d08a2e2c02455c72335a971d
d6d0fcc92b7ffbf093ed0dfc43a937dfade28280,"Add c10_cuda to libraries in CUDAExtension for Windows (#18982)

Summary:
This change was necessary for me to compile [apex](https://github.com/NVIDIA/apex) on Windows.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18982

Differential Revision: D14819818

Pulled By: soumith

fbshipit-source-id: 37ff9b93a72ab2b7c87f23a61e9f776c71c4c1a8",mooncake4132,https://api.github.com/repos/pytorch/pytorch/git/commits/d6d0fcc92b7ffbf093ed0dfc43a937dfade28280
bcd527190a6aed12c6926624256a448d5461be98,"Quantizer pass to insert quant-dequant nodes into IR (#18446)

Summary:
- Quantizer pass to mutate IR by inserting quant-dequant nodes
before and after nodes which support quantized ops. This information
will be used by jit compiler to substitute with quantized ops

- This currently covers simple model. It will be expanded later
for subgraph pattern matching to cover more complex patterns
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18446

Differential Revision: D14592265

Pulled By: nishantpdce

fbshipit-source-id: c9ba6c12aa96cb9c117826e386721eec83a55ea6",nishantpdce,https://api.github.com/repos/pytorch/pytorch/git/commits/bcd527190a6aed12c6926624256a448d5461be98
31ff0ecd2b400b4863741bcbc41748f2ad01745c,"Fix torch::nn::init::orthogonal_ with CNNs (#18915)

Summary:
Fixes #18518

I changed the C++ API torch::nn::init::orthogonal_ implementation to match the Python implementation.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18915

Differential Revision: D14851833

Pulled By: ezyang

fbshipit-source-id: 45b5e9741582777c203e9ebed564ab3ac1f94baf",Omegastick,https://api.github.com/repos/pytorch/pytorch/git/commits/31ff0ecd2b400b4863741bcbc41748f2ad01745c
a9a29dd63f2f64b1f1703ba2dbe64fd18e8ee528,"Fixes error when too many parameters are passed to fused cuda kernel (#18063)

Summary:
Bug fix for https://github.com/pytorch/pytorch/issues/15043, where a large fusion in JIT with a large number of kernel arguments, which exceeds the limit allowed by nvrtc on a cuda device.
  The fix is to check the number of arguments before a cuda kernel is generated. If the number exceeds the limit, take the runFallBack() path.
  Add a reduced test from the original issue to keep the test time low. The test would fail without this fix.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18063

Differential Revision: D14691401

Pulled By: soumith

fbshipit-source-id: b98829bc89ed7724e91eda82ae3a5a1151af721a",royju,https://api.github.com/repos/pytorch/pytorch/git/commits/a9a29dd63f2f64b1f1703ba2dbe64fd18e8ee528
160d0776d535834eb33eb09d54ab3b4f0a8e1010,"Remove comment (#19148)

Summary:
Remove pointer to nonexistent Note.
It is already removed in ""Remove support for CUDNN 6 (#15851)""
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19148

Differential Revision: D14891514

Pulled By: soumith

fbshipit-source-id: dd33cfefa3a21e18afae5b3992dea085adaabda8",sakaia,https://api.github.com/repos/pytorch/pytorch/git/commits/160d0776d535834eb33eb09d54ab3b4f0a8e1010
c145c34a7b60ecc1def571f761b60d9e7a11ca03,"Basic implementation of QRelu in C10 (#19091)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19091

Implements a basic quantized ReLU (uint8). This is a temporary solution before using the `QTensor` type instead of the tuple.

Reviewed By: dzhulgakov

Differential Revision: D14565413

fbshipit-source-id: 7d53cf5628cf9ec135603d6a1fb7c79cd9383019",z-a-f,https://api.github.com/repos/pytorch/pytorch/git/commits/c145c34a7b60ecc1def571f761b60d9e7a11ca03
20fc7b6ec7cb7b43f66526abf9d5b4fa44ffefb7,"Avoid undefined symbol error when building AdIndexer LTO (#19009)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19009

Move the definition of `MulFunctor<>::Backward()` into a header file.

Reviewed By: BIT-silence

Differential Revision: D14823230

fbshipit-source-id: 1efaec01863fcc02dcbe7e788d376e72f8564501",marksantaniello,https://api.github.com/repos/pytorch/pytorch/git/commits/20fc7b6ec7cb7b43f66526abf9d5b4fa44ffefb7
48859e3ad39abffde828072b391af9e6bbf08d67,"Allow for single-line deletions in clang_tidy.py (#19082)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19082

When you have just one line of deletions, just as with additions, there is no count printed.
Without this fix, we ignore all globs with single-line deletions when selecting which lines were changed.
When all the changes in the file were single-line, this meant no line-filtering at all!

Differential Revision: D14860426

fbshipit-source-id: c60e9d84f9520871fc0c08fa8c772c227d06fa27",efaust,https://api.github.com/repos/pytorch/pytorch/git/commits/48859e3ad39abffde828072b391af9e6bbf08d67
9e3bdb3231d1fd237f51ce949eb31d0e948e6dbe,"Update module.py documentation. (#19347)

Summary:
Added the "">>>"" python interpreter sign(three greater than symbols), so that the edited lines will appear as code, not comments/output, in the documentation. Normally, the interpreter would display ""..."" when expecting a block, but I'm not sure how this would work on the pytorch docs website. It seems that in other code examples the "">>>"" sign is used as well, therefore I used with too.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19347

Differential Revision: D14986154

Pulled By: soumith

fbshipit-source-id: 8f4d07d71ff7777b46c459837f350eb0a1f17e84",AlexMetsai,https://api.github.com/repos/pytorch/pytorch/git/commits/9e3bdb3231d1fd237f51ce949eb31d0e948e6dbe
940caed0d47aa7001308e7f64fd5000d06abd618,"update documentation of PairwiseDistance#19241 (#19412)

Summary:
Fix the documentation of PairwiseDistance [#19241](https://github.com/pytorch/pytorch/issues/19241)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19412

Differential Revision: D14998271

Pulled By: soumith

fbshipit-source-id: bcb2aa46d3b3102c4480f2d24072a5e14b049888",WindChimeRan,https://api.github.com/repos/pytorch/pytorch/git/commits/940caed0d47aa7001308e7f64fd5000d06abd618
0676ba0c5cdc51d882337a46dcac3d60c885d4c4,"Mention packed accessors in tensor basics doc (#19464)

Summary:
This is a continuation of efforts into packed accessor awareness.
A very simple example is added, along with the mention that the template can hold more arguments.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19464

Differential Revision: D15012564

Pulled By: soumith

fbshipit-source-id: a19ed536e016fae519b062d847cc58aef01b1b92",ClementPinard,https://api.github.com/repos/pytorch/pytorch/git/commits/0676ba0c5cdc51d882337a46dcac3d60c885d4c4
30292d994fabf24146fe0dbcfe9621d4f87325e2,"Add an identity module (#19249)

Summary:
This is a simple yet useful addition to the torch.nn modules: an identity module. This is a first draft - please let me know what you think and I will edit my PR.

 There is no identity module - nn.Sequential() can be used, however it is argument sensitive so can't be used interchangably with any other module. This adds nn.Identity(...) which can be swapped with any module because it has dummy arguments. It's also more understandable than seeing an empty Sequential inside a model.

See discussion on #9160. The current solution is to use nn.Sequential(). However this won't work as follows:

```python
batch_norm = nn.BatchNorm2d
if dont_use_batch_norm:
    batch_norm = Identity
```

Then in your network, you have:

```python
nn.Sequential(
    ...
    batch_norm(N, momentum=0.05),
    ...
)
```

If you try to simply set `Identity = nn.Sequential`, this will fail since `nn.Sequential` expects modules as arguments. Of course there are many ways to get around this, including:

- Conditionally adding modules to an existing Sequential module
- Not using Sequential but writing the usual `forward` function with an if statement
- ...

**However, I think that an identity module is the most pythonic strategy,** assuming you want to use nn.Sequential.

Using the very simple class (this isn't the same as the one in my commit):

```python
class Identity(nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__()
    def forward(self, x):
        return x
```

we can get around using nn.Sequential, and `batch_norm(N, momentum=0.05)` will work. There are of course other situations this would be useful.

Thank you.
Best,
Miles
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19249

Differential Revision: D15012969

Pulled By: ezyang

fbshipit-source-id: 9f47e252137a1679e306fd4c169dca832eb82c0c",MilesCranmer,https://api.github.com/repos/pytorch/pytorch/git/commits/30292d994fabf24146fe0dbcfe9621d4f87325e2
557b1b362fba144676edf42a5fa730dd2c22448b,"Fix copied optimizer (#19308)

Summary:
Add the defaults field to the copied object.
Prior to this patch, optimizer.__getattr__ has excluded the defaults
attribute of optimizer source object, required by some LR schedulers. (e.g. CyclicLR with momentum)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19308

Differential Revision: D15012801

Pulled By: soumith

fbshipit-source-id: 95801b269f6f9d78d531d4fed95c973b280cc96f",barrh,https://api.github.com/repos/pytorch/pytorch/git/commits/557b1b362fba144676edf42a5fa730dd2c22448b
88f78c719aaac8f5a72440662711775af54a46a5,"Fix math formatting of PairwiseDistance and CosineSimilarity docs and fix math formatting of CTC loss docs.

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/19534

Differential Revision: D15034011

Pulled By: ezyang

fbshipit-source-id: 60b81c970c919508a57c86fb23edc9f64973117c",zhiqwang,https://api.github.com/repos/pytorch/pytorch/git/commits/88f78c719aaac8f5a72440662711775af54a46a5
f9c4ce781f994ede6d4272c133e4760479c3716b,"Removes variable which is assigned but not used (#19194)

Summary:
n was set as self.in_channels, but not used within the scope of the function.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19194

Differential Revision: D14937764

Pulled By: ezyang

fbshipit-source-id: 55cb599109309503fee897f77d798fd454fcc02d",SebFar,https://api.github.com/repos/pytorch/pytorch/git/commits/f9c4ce781f994ede6d4272c133e4760479c3716b
36084908e4d31d70d1c8022fb979059e6984ecc6,"Fix lr_scheduler's last_epoch value at the time of initialization (BC BREAKING!) (#7889)

Summary:
Hello everyone :) !!

I've found that lr_scheduler was initialized with last_epoch as -1.
This causes that even after the first step (not the one in init but explicit step of scheduler),
learning rate of scheduler's optimizer remains as the previous.
```python
>>> import torch
>>> cc = torch.nn.Conv2d(10,10,3)
>>> myinitial_lr = 0.1
>>> myoptimizer = torch.optim.Adam(cc.parameters(), lr=myinitial_lr)
>>> mylrdecay = 0.5
>>> myscheduler = torch.optim.lr_scheduler.ExponentialLR(myoptimizer,mylrdecay)

>>> myscheduler.get_lr()
[0.2]    # this is because of  get_lr calculates lr by 0.1 * 0.5^-1
>>> myscheduler.optimizer.param_groups[0][""lr""]
0.1    # this is not consistent with get_lr value
>>> myscheduler.last_epoch
-1

>>> myscheduler.step()
>>> myscheduler.get_lr()
[0.1]    # this should be the value right after the init, not after first step
>>> myscheduler.optimizer.param_groups[0][""lr""]
0.1    # since this is after first step, it should have been decayed as 0.05
>>> myscheduler.last_epoch
0

>>> myscheduler.step()
>>> myscheduler.last_epoch
1
>>> myscheduler.get_lr()
[0.05]
>>> myscheduler.optimizer.param_groups[0][""lr""]
0.05
>>> myscheduler.last_epoch
1
```

First problem is, even after the init of lr_scheduler, you get the inconsistent parameter values.

The second problem is, you are stuck with same learning rate in the first 2 epochs if the step function of lr_scheduler is not called in the beginning of the epoch loop.
Of course, you can avoid this by calling lr_scheduler's step in the beginning,
but I don't think this is proper use since, incase of optimizer, step is called in the end of the iteration loop.

I've simply avoided all above issues by setting last_epoch as 0 after the initialization.

This also makes sense when you init with some value of last_epoch which is not -1.
For example, if you want to init with last epoch 10,
lr should not be set with decayed 1 step further. Which is
last_epoch gets +1 in the previous code.
base_lr * self.gamma ** self.last_epoch

Instead, it should be set with step 10 exact value.

I hope this fix find it's way with all your help :)
I'm really looking forward & excited to become a contributor for pytorch!
Pytorch Rocks!!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/7889

Differential Revision: D15012769

Pulled By: ezyang

fbshipit-source-id: 258fc3009ea7b7390a3cf2e8a3682eafb506b08b",bado-lee,https://api.github.com/repos/pytorch/pytorch/git/commits/36084908e4d31d70d1c8022fb979059e6984ecc6
f767c9ac767bf8b2d87119183dea4149bdbacdf1,"Add docs and test guaranteeing indices from torch.nonzero ordered C-style (#19539)

Summary:
See #17556.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19539

Differential Revision: D15030151

Pulled By: ezyang

fbshipit-source-id: d46ee56a66d89b0113f86e3f8693dc1680d0adb9",jhultman,https://api.github.com/repos/pytorch/pytorch/git/commits/f767c9ac767bf8b2d87119183dea4149bdbacdf1
c855e04d5f2e5892bf44624c1671b50128c51036,"Caffe2 shouldn't fail if CUDA peer access is already enabled

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/19586

Differential Revision: D15061544

Pulled By: dzhulgakov

fbshipit-source-id: 6a5f9f4fe45259d689671f58ad5206cdaf15c5bd",deadeyegoodwin,https://api.github.com/repos/pytorch/pytorch/git/commits/c855e04d5f2e5892bf44624c1671b50128c51036
6c7135decba2e9090fd663f6d520311a10c84ed8,"fix typo: pytoch -> pytorch

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/19719

Differential Revision: D15080095

Pulled By: ezyang

fbshipit-source-id: b731a0fde87d25c63c1e3d4b9a9c2244e5ad84af",seungwonpark,https://api.github.com/repos/pytorch/pytorch/git/commits/6c7135decba2e9090fd663f6d520311a10c84ed8
af06d6342c17f2196f599b7e1fa515b5f09c4759,"Add SGDR(Stochastic Gradient Descent with Warm Restarts) scheduler (#17226)

Summary:
Because of merge error with master in #15042, open a new PR for ezyang.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17226

Differential Revision: D14418145

Pulled By: mrshenli

fbshipit-source-id: 099ba225b28e6aba71760b81b2153ad1c40fbaae",Kirayue,https://api.github.com/repos/pytorch/pytorch/git/commits/af06d6342c17f2196f599b7e1fa515b5f09c4759
29d8711ef02eb52a6e4ad8693337ec467ee548fb,"Fix compilation on Windows 10 (CUDA 10.0, Visual Studio 2017) (#19615)

Summary:
I want to use libtorch in a C++/CUDA project but as soon as I  include `<torch/torch.h>`, "".cu"" files fail to compile:

`torch/csrc/jit/script/tree.h(64): error C3520: 'args': parameter pack must be expanded in this context`

This PR makes it build on my machine (don't know if it breaks anything though).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19615

Differential Revision: D15063712

Pulled By: ezyang

fbshipit-source-id: 7561e705f8f5b42b8e6a23430710b36508fee1ee",dthul,https://api.github.com/repos/pytorch/pytorch/git/commits/29d8711ef02eb52a6e4ad8693337ec467ee548fb
3f7e3d5857fa443bd6ed2187e9f0914c13ebc1db,"Add the ability to observe intermediate tensors in an onnxifi op

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/19966

Reviewed By: yinghai

Differential Revision: D15096086

fbshipit-source-id: 8e6a26c46898f99d411dd5841f086946884b2457",tracelogfb,https://api.github.com/repos/pytorch/pytorch/git/commits/3f7e3d5857fa443bd6ed2187e9f0914c13ebc1db
6f7a315a710001529f77185f2150506760246dfe,"Allow onnx export for maxpool with dilations (#18721)

Summary:
Now, MaxPool operator supports the 'dilations' attribute with this commit:
https://github.com/onnx/onnx/commit/b22041c3f16cf8bcca9ed93982d6ffdf6ebf3746
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18721

Reviewed By: zrphercule

Differential Revision: D15152400

Pulled By: houseroad

fbshipit-source-id: e8f5ab35c5c2c3a540a22f7cf7bb453d892d0400",karljang,https://api.github.com/repos/pytorch/pytorch/git/commits/6f7a315a710001529f77185f2150506760246dfe
71d23ebc1333e49dfc30f079f7dbf7ef764ed3c1,"#20143 TripletMarginLoss example isn't clear (#20145)

Summary:
Pull Request for TripletMarginLoss example isn't clear #20143
SsnL
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20145

Differential Revision: D15217491

Pulled By: ezyang

fbshipit-source-id: 46e30496e9f0dd830a2eec42c5775209a773cc48",PetroSokirniy,https://api.github.com/repos/pytorch/pytorch/git/commits/71d23ebc1333e49dfc30f079f7dbf7ef764ed3c1
57948414acf87c846bf88cf5f194282b8f47c423,"Fix small typo T_mul->T_mult

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/20148

Differential Revision: D15217485

Pulled By: ezyang

fbshipit-source-id: cb183cdc2eb3e42c685ef024742a18745923d283",rtqichen,https://api.github.com/repos/pytorch/pytorch/git/commits/57948414acf87c846bf88cf5f194282b8f47c423
a8387b77798dbfca70debf3e34492032a3ba40a3,"Delete TensorImpl::GetDevice() (#20025)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20025

Delete TensorImpl::GetDevice() and clean all its call sites.

Reviewed By: ezyang

Differential Revision: D15170917

fbshipit-source-id: b6862b74aa036198544f79d18a8c0f995cb0ca7b",yanboliang,https://api.github.com/repos/pytorch/pytorch/git/commits/a8387b77798dbfca70debf3e34492032a3ba40a3
f7a78688207fd56fb865393d6f644ec6d86cc0c9,"add process_group in convert_sync_batchnorm (#19240)

Summary:
In line 508.
convert_sync_batchnorm is called recursively to convert the bn to syncbn, thus the process_group also should be passed in the function.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19240

Differential Revision: D15240318

Pulled By: ezyang

fbshipit-source-id: 0fc9e856392824814991e5e9e8f9513d57f311af",zhangliliang,https://api.github.com/repos/pytorch/pytorch/git/commits/f7a78688207fd56fb865393d6f644ec6d86cc0c9
cf55670bddf6bb5f6199df2c543c8fb4ca21b08e,"Add proper __repr__ to LogSoftMax (#20018)

Summary:
Fixes #19961
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20018

Differential Revision: D15218171

Pulled By: ezyang

fbshipit-source-id: 36bdf44d3b7a6df6a6ec5275a74741d4b057d3b4",SdgJlbl,https://api.github.com/repos/pytorch/pytorch/git/commits/cf55670bddf6bb5f6199df2c543c8fb4ca21b08e
21a3895c7dea0acf916eef9cee68aba11d295f5d,"Extract repeated scripts into files (#19674)

Summary:
The current pytorch config.yml is causing some backend performance
problems on CircleCI, due to the size of the file when all of the YAML
anchors have been expanded. You can view the ""processed"" config as our
internal system deal with it by running `circleci config process`.

    circleci config process .circleci/config.yml | wc -c

    Before: 2833769 bytes
    After:   558252 bytes (~80% less)

Add create a new job, `setup`, that has 2 functions:
- Assert that config.yml is up to date
- Put the .circleci/scripts directory into a workspace, so that
downstream jobs can easily access it.

The `setup` job becomes the parent of all jobs in the workflow. This
allows us to fail fast if config is invalid. It might be a good place to
add other, quick, lint checks to help fail the build faster.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19674

Differential Revision: D15252864

Pulled By: pjh5

fbshipit-source-id: 0778c7b8f95e7f3f33ac92fbb8862377fc9fb0ac",glenjamin,https://api.github.com/repos/pytorch/pytorch/git/commits/21a3895c7dea0acf916eef9cee68aba11d295f5d
2aea5b63350edac33f6455ef13636685f4d6e201,"Fixed Softmax doc to specify dimension to prevent warning in 1.1.0. (#20310)

Summary:
See Issue #20301

Specifying dim in docstring example to prevent UserWarning.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20310

Differential Revision: D15277734

Pulled By: ezyang

fbshipit-source-id: 2e8b748dbe743675a5a538ccbe97713aad02e8ac",ZaydH,https://api.github.com/repos/pytorch/pytorch/git/commits/2aea5b63350edac33f6455ef13636685f4d6e201
2019f6cd51c94a85bc38140031528bf5552a06b0,"Add unit test to ensure no gradients sync when calling ddp.module(input) (#20282)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20282

Add unit test to ensure no gradients sync when calling ddp.module(input), e.g not invoking prepare_for_backward

PyText is depending on DDP for data parallel distributed training. To support accumulate gradients locally before gradients sync, we are calling orig_model.forward instead of ddp_model.forward. Add a unit test to avoid changes break the assumption.

Reviewed By: pietern, mrshenli

Differential Revision: D15263155

fbshipit-source-id: 7734e174f507690fb23ea6c52dffff4a93f9b151",chenyangyu1988,https://api.github.com/repos/pytorch/pytorch/git/commits/2019f6cd51c94a85bc38140031528bf5552a06b0
ea5c9c926712a909a5e4c99cf1978fb1c13f7764,"Update installing.rst (#20354)

Summary:
Delete useless `cd`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20354

Differential Revision: D15296154

Pulled By: soumith

fbshipit-source-id: 2042b56c91b33e302b0ed9c77f29b9b64079fa98",standbyme,https://api.github.com/repos/pytorch/pytorch/git/commits/ea5c9c926712a909a5e4c99cf1978fb1c13f7764
35fed93b1ef05175143f883c6f89f06c6dd9429b,"Adding Poisson NLL loss to libtorch (#19316)

Summary:
This PR add Poisson NLL loss to aten and substitute the python implementation with a call to the c++.

Fixes #19186.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19316

Differential Revision: D15012957

Pulled By: ezyang

fbshipit-source-id: 0a3f56e8307969c2f9cc321b5357a496c3d1784e",interesaaat,https://api.github.com/repos/pytorch/pytorch/git/commits/35fed93b1ef05175143f883c6f89f06c6dd9429b
5f7ef09f5777d35eb1f08e6c675eb2ead41d6165,"math module support: gcd, copysign, erf, erfc, expm1, fabs, gamma, lgamma (#19707)

Summary:
eellison driazati Refer to issue #19026
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19707

Differential Revision: D15302632

Pulled By: eellison

fbshipit-source-id: 68ff13b478b93cc33703ef3276b5fa727c8ff31a",eugenekoran,https://api.github.com/repos/pytorch/pytorch/git/commits/5f7ef09f5777d35eb1f08e6c675eb2ead41d6165
6e82b1c77d36386ba738af3287693105b4bbafe2,"Split nn.MultiHeadAttention into Module + functional (#20415)

Summary:
Moving functions from torch/nn/modules/activation.py to torch/nn/functional.py. For functions not implemented (_get_input_buffer and _set_input_buffer), a TODO is added.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20415

Differential Revision: D15318078

Pulled By: jamarshon

fbshipit-source-id: 5ca698e2913821442cf8609cc61ac8190496a3c6",jamarshon,https://api.github.com/repos/pytorch/pytorch/git/commits/6e82b1c77d36386ba738af3287693105b4bbafe2
5f14ef8cc167e8d4b12aa8a65b09e84c1b963a7a,"Split out gpu/cpu targets based on gpu_library_targets (#20633)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20633

Merge the c2_gpu and is_amd_build logic in targets files.

Reviewed By: dzhulgakov

Differential Revision: D15176621

fbshipit-source-id: 9185b394ffcb305fd8d94dc7c7c92780bf10a511",lertoz,https://api.github.com/repos/pytorch/pytorch/git/commits/5f14ef8cc167e8d4b12aa8a65b09e84c1b963a7a
c0a2a3b22b54b32d1bb1c8dd58cf3c4714001950,"Add a new method SummaryWriter.flush() (#20607)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20607

Add a new method SummaryWriter.flush()  that iterates through all of the FileWriters and flushes them

Reviewed By: orionr

Differential Revision: D15380124

fbshipit-source-id: 1975f3f61c5ae3754552bfdb23f2cd78f687d19f",natalialunova,https://api.github.com/repos/pytorch/pytorch/git/commits/c0a2a3b22b54b32d1bb1c8dd58cf3c4714001950
c062175803b967674528646076222d9880057ad9,"Remove unused var (ws_) and use vars in undefined case for compile (#20667)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20667

Compilation errors:
```
xplat/caffe2/caffe2/utils/signal_handler.h:31:10: error: private field 'SIGINT_action_' is not used [-Werror,-Wunused-private-field]
  Action SIGINT_action_;
         ^
xplat/caffe2/caffe2/utils/signal_handler.h:32:10: error: private field 'SIGHUP_action_' is not used [-Werror,-Wunused-private-field]
  Action SIGHUP_action_;
         ^
xplat/caffe2/caffe2/utils/signal_handler.h:33:17: error: private field 'my_sigint_count_' is not used [-Werror,-Wunused-private-field]
  unsigned long my_sigint_count_;
                ^
xplat/caffe2/caffe2/utils/signal_handler.h:34:17: error: private field 'my_sighup_count_' is not used [-Werror,-Wunused-private-field]
  unsigned long my_sighup_count_;
                ^
4 errors generated.

xplat/caffe2/caffe2/share/fb/stylizer/median_blur_ops.cc:593:14: error: private field 'ws_' is not used [-Werror,-Wunused-private-field]
  Workspace* ws_;
             ^
1 error generated.
```

Reviewed By: bwasti

Differential Revision: D15402928

fbshipit-source-id: 5b98499850aa659fd37ab8e7f2e75166787b8129",dxyang,https://api.github.com/repos/pytorch/pytorch/git/commits/c062175803b967674528646076222d9880057ad9
5d8879cf6d9cf59a5691c1111f0f791acf600c67,"Auto-convert GPU arrays that support the __cuda_array_interface__ protocol (#20584)

Summary:
This PR implements auto-conversion of GPU arrays that support the `__cuda_array_interface__` protocol (fixes #15601).

If an object exposes the `__cuda_array_interface__` attribute, `touch.as_tensor()` and `touch.tensor()` will use the exposed device memory.

#### Zero-copy
When using `touch.as_tensor(...,device=D)` where `D` is the same device as the one used in `__cuda_array_interface__`.

#### Implicit copy
When using `touch.as_tensor(...,device=D)` where `D` is the CPU or another non-CUDA device.

#### Explicit copy
When using `torch.tensor()`.

#### Exception
When using `touch.as_tensor(...,device=D)` where `D` is a CUDA device not used in `__cuda_array_interface__`.

#### Lifetime
`torch.as_tensor(obj)` tensor grabs a reference to `obj` so that the lifetime of `obj` exceeds the tensor
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20584

Differential Revision: D15435610

Pulled By: ezyang

fbshipit-source-id: c423776ba2f2c073b902e0a0ce272d54e9005286",madsbk,https://api.github.com/repos/pytorch/pytorch/git/commits/5d8879cf6d9cf59a5691c1111f0f791acf600c67
99b3f5cd705018c96b5559daa89aa71e66fc7e8a,"Fixes error with custom scalars, fixes #20579 (#20580)

Summary:
When adding custom scalars like this
```python
from torch.utils.tensorboard import SummaryWriter

with SummaryWriter() as writer:
    writer.add_custom_scalars({'Stuff': {
        'Losses': ['MultiLine', ['loss/(one|two)']],
        'Metrics': ['MultiLine', ['metric/(three|four)']],
    }})
```
This error is raised:
```
TypeError: Parameter to MergeFrom() must be instance of same class: expected tensorboard.SummaryMetadata.PluginData got list.
```

Removing the square brackets around `SummaryMetadata.PluginData(plugin_name='custom_scalars')` should be enough to fix it.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20580

Differential Revision: D15469700

Pulled By: orionr

fbshipit-source-id: 7ce58034bc2a74ab149fee6419319db68d8abafe",baldassarreFe,https://api.github.com/repos/pytorch/pytorch/git/commits/99b3f5cd705018c96b5559daa89aa71e66fc7e8a
5f83c5d8349fb0c3645146efe7ad4b4f20fca457,"Fix build error with MSVC (#20853)

Summary:
Close #20642

Possibly broken by #19816
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20853

Differential Revision: D15474620

Pulled By: jerryzh168

fbshipit-source-id: 99b52d92a93bac7cab52537f1ebdbd286d4b2cfe",Jonas1312,https://api.github.com/repos/pytorch/pytorch/git/commits/5f83c5d8349fb0c3645146efe7ad4b4f20fca457
93d5503f34ab54c915b640d3bc45d41e6afe0dd2,"bug fix 19374 - fix for upsample export

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/20116

Differential Revision: D15256899

Pulled By: houseroad

fbshipit-source-id: cf0dfd679d528fbb77f483e23071f4a96fb27091",pk-g,https://api.github.com/repos/pytorch/pytorch/git/commits/93d5503f34ab54c915b640d3bc45d41e6afe0dd2
ec45baf4dd79ca72bf8089bbd21168f932909000,"tensor_illustration with correct numbers and better fonts for README file (#20751)

Summary:
Fix of README tensor image for issue #20641
Numbers are fixed, symbols made more readable.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20751

Differential Revision: D15495706

Pulled By: ezyang

fbshipit-source-id: b6013574d16253ec681fc57143efe3d53952fbe9",PgLoLo,https://api.github.com/repos/pytorch/pytorch/git/commits/ec45baf4dd79ca72bf8089bbd21168f932909000
a640c815368fad86de680136c07174ae266aa595,"Add llvm8 installation step. (#20879)

Summary:
Add ability to build docker container with llvm8.

ezyang
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20879

Differential Revision: D15497037

Pulled By: rdzhabarov

fbshipit-source-id: d673d1ddd4156c95516e61223b397c2f9bce1214",rdzhabarov,https://api.github.com/repos/pytorch/pytorch/git/commits/a640c815368fad86de680136c07174ae266aa595
b5a5e296aa2f3fad6b2f99f87644237837bf3c82,"Support 3D mesh/point cloud (#20413)

Summary:
I started adding support for the new **[mesh/point cloud](https://github.com/tensorflow/graphics/blob/master/tensorflow_graphics/g3doc/tensorboard.md)** data type introduced to TensorBoard recently.

I created the functions to add the data, created the appropriate summaries.
This new data type however requires a **Merged** summary containing the data for the vertices, colors and faces.

I got stuck at this stage. Maybe someone can help. lanpa?

I converted the example code by Google to PyTorch:
```python
import numpy as np
import trimesh

import torch
from torch.utils.tensorboard import SummaryWriter

sample_mesh = 'https://storage.googleapis.com/tensorflow-graphics/tensorboard/test_data/ShortDance07_a175_00001.ply'
log_dir = 'runs/torch'
batch_size = 1

# Camera and scene configuration.
config_dict = {
    'camera': {'cls': 'PerspectiveCamera', 'fov': 75},
    'lights': [
        {
            'cls': 'AmbientLight',
            'color': '#ffffff',
            'intensity': 0.75,
        }, {
            'cls': 'DirectionalLight',
            'color': '#ffffff',
            'intensity': 0.75,
            'position': [0, -1, 2],
        }],
    'material': {
        'cls': 'MeshStandardMaterial',
        'roughness': 1,
        'metalness': 0
    }
}

# Read all sample PLY files.
mesh = trimesh.load_remote(sample_mesh)
vertices = np.array(mesh.vertices)
# Currently only supports RGB colors.
colors = np.array(mesh.visual.vertex_colors[:, :3])
faces = np.array(mesh.faces)

# Add batch dimension, so our data will be of shape BxNxC.
vertices = np.expand_dims(vertices, 0)
colors = np.expand_dims(colors, 0)
faces = np.expand_dims(faces, 0)

# Create data placeholders of the same shape as data itself.
vertices_tensor = torch.as_tensor(vertices)
faces_tensor = torch.as_tensor(faces)
colors_tensor = torch.as_tensor(colors)

writer = SummaryWriter(log_dir)

writer.add_mesh('mesh_color_tensor', vertices=vertices_tensor, faces=faces_tensor,
                colors=colors_tensor, config_dict=config_dict)

writer.close()
```

I tried adding only the vertex summary, hence the others are supposed to be optional.
I got the following error from TensorBoard and it also didn't display the points:
```
Traceback (most recent call last):
  File ""/home/dawars/workspace/pytorch/venv/lib/python3.6/site-packages/werkzeug/serving.py"", line 302, in run_wsgi
    execute(self.server.app)
  File ""/home/dawars/workspace/pytorch/venv/lib/python3.6/site-packages/werkzeug/serving.py"", line 290, in execute
    application_iter = app(environ, start_response)
  File ""/home/dawars/workspace/pytorch/venv/lib/python3.6/site-packages/tensorboard/backend/application.py"", line 309, in __call__
    return self.data_applications[clean_path](environ, start_response)
  File ""/home/dawars/workspace/pytorch/venv/lib/python3.6/site-packages/werkzeug/wrappers/base_request.py"", line 235, in application
    resp = f(*args[:-2] + (request,))
  File ""/home/dawars/workspace/pytorch/venv/lib/python3.6/site-packages/tensorboard/plugins/mesh/mesh_plugin.py"", line 252, in _serve_mesh_metadata
    tensor_events = self._collect_tensor_events(request)
  File ""/home/dawars/workspace/pytorch/venv/lib/python3.6/site-packages/tensorboard/plugins/mesh/mesh_plugin.py"", line 188, in _collect_tensor_events
    tensors = self._multiplexer.Tensors(run, instance_tag)
  File ""/home/dawars/workspace/pytorch/venv/lib/python3.6/site-packages/tensorboard/backend/event_processing/plugin_event_multiplexer.py"", line 400, in Tensors
    return accumulator.Tensors(tag)
  File ""/home/dawars/workspace/pytorch/venv/lib/python3.6/site-packages/tensorboard/backend/event_processing/plugin_event_accumulator.py"", line 437, in Tensors
    return self.tensors_by_tag[tag].Items(_TENSOR_RESERVOIR_KEY)
KeyError: 'mesh_color_tensor_COLOR'
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20413

Differential Revision: D15500737

Pulled By: orionr

fbshipit-source-id: 426e8b966037d08c065bce5198fd485fd80a2b67",Dawars,https://api.github.com/repos/pytorch/pytorch/git/commits/b5a5e296aa2f3fad6b2f99f87644237837bf3c82
2fb665a9dfc37fe0aa35b08d48c6285a269c14ba,"Add warning about memory overhead when using multiple tiny tensors (#20801)

Summary:
added note in docs regarding #19408
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20801

Differential Revision: D15503351

Pulled By: mrshenli

fbshipit-source-id: 7ab371a7992233fb867aadd4bb6b74fccd232c33",iamhatesz,https://api.github.com/repos/pytorch/pytorch/git/commits/2fb665a9dfc37fe0aa35b08d48c6285a269c14ba
5e69e76aba4d75664920832a1e584f14350f160d,"Remove padding_mode from torch.nn.functional.conv{1,2,3}d's docstr (#20891)

Summary:
Fixes #20694
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20891

Differential Revision: D15510790

Pulled By: soumith

fbshipit-source-id: aa3630693c7446bf18a390cb49c4df9bc9c59eea",yifuwang,https://api.github.com/repos/pytorch/pytorch/git/commits/5e69e76aba4d75664920832a1e584f14350f160d
c611630b9df4a25007dd299676b2bb88bf692784,"Fix subscripts in RNN documentation

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/20949

Differential Revision: D15510760

Pulled By: soumith

fbshipit-source-id: 51e9dbea7d8c8194e46e12311e397deff32dbe2f",link2xt,https://api.github.com/repos/pytorch/pytorch/git/commits/c611630b9df4a25007dd299676b2bb88bf692784
8dbdd00f871f07d1fac9a6d79f1d818206860f0c,"tweak tqdm to have download speed in kB/MB/etc (#20908)

Summary:
This changes the progress bars in `_download_url_to_file` from saying things like `49773343.40it/s` to `47.5MB/s`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20908

Differential Revision: D15511223

Pulled By: soumith

fbshipit-source-id: 2422eb5fb486f9ef4bd69c556c4ed1775b8b2860",dougalsutherland,https://api.github.com/repos/pytorch/pytorch/git/commits/8dbdd00f871f07d1fac9a6d79f1d818206860f0c
ebc8d7170e352a0f403ed8e175094fde28ef5563,"fix the bug for mkldnn clone (#20943)

Summary:
This PR is to solve the bug for clone a MKLDNN tensor, please see the issue https://github.com/pytorch/pytorch/issues/20895.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20943

Differential Revision: D15511516

Pulled By: mrshenli

fbshipit-source-id: 05b41d6c7eaf8703521f4c768b8f26ec8501dc5e",XiaobingSuper,https://api.github.com/repos/pytorch/pytorch/git/commits/ebc8d7170e352a0f403ed8e175094fde28ef5563
0556141339d54cfb3cb9570d7132a995a114417d,"fix small typo muliprocessing -> multiprocessing (#20998)

Summary:
Minor typo fix in docstring.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20998

Differential Revision: D15514698

Pulled By: soumith

fbshipit-source-id: a9ceb557251ff5868e810331195243b6a8717851",jpgard,https://api.github.com/repos/pytorch/pytorch/git/commits/0556141339d54cfb3cb9570d7132a995a114417d
52d27890dc4bea72f58d7b861201ea5ab5986c29,"Improve error message for missing attribute (#20779)

Summary:
Fixes #20495 .

Now for
```python
        class A(torch.jit.ScriptModule):
            def __init__(self):
                super(A, self).__init__()

            torch.jit.script_method
            def forward(self, x):
                return x + self.whatisgoingon

        class B(A):
            def __init__(self):
                super(B, self).__init__()
            torch.jit.script_method
            def bar(self, x):
                return x * x

        A()
```
it does
```
RuntimeError:
attribute 'whatisgoingon' does not exist:
torch.jit.script_method
def forward(self, x):
    return x + self.whatisgoingon
               ~~~~~~~~~~~~~~~~~~ <--- HERE

```

I added a test in `test_jit.py` as well.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20779

Differential Revision: D15441138

Pulled By: Chillee

fbshipit-source-id: 88f458c36b5e32a1ffc467b27bbc28a3c5c07321",Chillee,https://api.github.com/repos/pytorch/pytorch/git/commits/52d27890dc4bea72f58d7b861201ea5ab5986c29
ffdce79078438c8722eccc87d9d68396559b97e4,"Deprecate variadic inputs of checkpoint_sequential (#21006)

Summary:
I've reported inconsistency between `checkpoint_sequential` and `nn.Sequential` at https://github.com/pytorch/pytorch/issues/19260. Both should provide the same input signature but they don't. I think the consistency is important and I agree with apaszke that `nn.Sequential`'s semantics should be kept instead of `checkpoint_sequential`.

I hope `checkpoint_sequential` raises `TypeError` on variadic arguments since PyTorch 1.2.0. But for now, it's okay just to warn as `DeprecationWarning`. I've talked about this approach with soumith.

Please review this pull request. Any comment will be my pleasure.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21006

Differential Revision: D15530801

Pulled By: soumith

fbshipit-source-id: 0ceb2cc6a17dcc547d0d00ebaf9df8603be53183",sublee,https://api.github.com/repos/pytorch/pytorch/git/commits/ffdce79078438c8722eccc87d9d68396559b97e4
0ffd20c268340c0d7fe0b388a6960ff2b271808a,"Fix empty tensor for unique_dim (#19000)

Summary:
Fixes: #18408

cc: zasdfgbnm
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19000

Reviewed By: ezyang

Differential Revision: D15470136

Pulled By: VitalyFedyunin

fbshipit-source-id: daf71566b4dbdc91927d164f813b5ee8645af1a2",codexetreme,https://api.github.com/repos/pytorch/pytorch/git/commits/0ffd20c268340c0d7fe0b388a6960ff2b271808a
6ea9044d3c9881a9f6311d3042c9739834ce3538,"add 'all' builtin (#20521)

Summary:
[jit] add 'all' builtin
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20521

Differential Revision: D15527657

Pulled By: driazati

fbshipit-source-id: eaa3c1c560810581150646858339369e4305fdf2",michaelarg,https://api.github.com/repos/pytorch/pytorch/git/commits/6ea9044d3c9881a9f6311d3042c9739834ce3538
052bab70697bf6cfe18570374ff26fce93dace94,"Move legacy TH functions(sinh,cosh) to TensorIterator + Vec256 (#21115)

Summary:
This is a follow up on Jame's PR: https://github.com/pytorch/pytorch/pull/19041. The idea is to replace the legacy `sinh` / `cosh` ops that are being dispatched to TH with the operations defined in `Vec256` for better performance.

benchmark(from Jame's script):

```python
import torch, time
ops = ['sinh', 'cosh']
x = torch.rand(1024, 1024)
NITER = 10000

print('op', 'time per iter (ms)', 'gops/s', 'GB/s', sep='\t')
for op in ops:
    s = time.time()
    for i in range(NITER):
        getattr(x, op)()
    elapsed_sec = ((time.time() - s) / NITER)
    print(op, elapsed_sec * 1000, (1024*1024/elapsed_sec)/1e9, (1024*1024*4*2) / elapsed_sec / 1e9, sep='\t')
```
code on master:

```
op	time per iter (ms)	gops/s	GB/s
sinh	3.37614369392395	0.3105839369002935	2.484671495202348
cosh	3.480502033233643	0.3012714803748572	2.4101718429988574
```
after change (on Macbook pro 2018):

```
op	time per iter (ms)	gops/s	GB/s
sinh	0.8956503868103027	1.1707425301677301	9.365940241341841
cosh	0.9392147302627564	1.1164390487217428	8.931512389773943
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21115

Reviewed By: ljk53

Differential Revision: D15574580

Pulled By: xta0

fbshipit-source-id: 392546a0df11ed4f0945f2bc84bf5dea2750b60e",xta0,https://api.github.com/repos/pytorch/pytorch/git/commits/052bab70697bf6cfe18570374ff26fce93dace94
e098878d75188f95ae20063ee0d4552e6294be7a,"Cuda persistent softmax (#20827)

Summary:
Adds persistent cuda kernels that speed up SoftMax applied over the fast dimension, i.e. torch.nn.Softmax(dim=-1) and torch.nn.LogSoftmax(dim=-1). When the size is <= 1024, this code is 2-10x faster than the current code, speedup is higher for smaller sizes. This code works for half, float and double tensors with 1024 or fewer elements in the fast dimension. Numerical accuracy is on par with the current code, i.e. relative error is ~1e-8 for float tensors and ~1e-17 for double tensors. Relative error was computed against the CPU code.

The attached image shows kernel time in us for torch.nn.Softmax(dim=-1) applied to a half precision tensor of shape [16384,n], n is plotted along the horizontal axis. Similar uplifts can be seen for the backward pass and for LogSoftmax.

![image](https://user-images.githubusercontent.com/41591019/58212822-b63ebb00-7cb5-11e9-910d-1fc7d8585d58.png)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20827

Differential Revision: D15582509

Pulled By: ezyang

fbshipit-source-id: 65805db37487cebbc4ceefb1a1bd486d24745f80",thorjohnsen,https://api.github.com/repos/pytorch/pytorch/git/commits/e098878d75188f95ae20063ee0d4552e6294be7a
5bc7c1f83dd3ea740ba1d2af286c464862b90743,"fix contribution and governance links (#21243)

Summary:
Updated web links on contribution_guide and governance documentation
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21243

Differential Revision: D15591065

Pulled By: soumith

fbshipit-source-id: fdcfc518605a08a2ac35a10c146122d7d0a3f609",sovvo,https://api.github.com/repos/pytorch/pytorch/git/commits/5bc7c1f83dd3ea740ba1d2af286c464862b90743
2e59a0a646883f6b20f2e0d0af9c795e629bc115,"add contiguous function type hint for tensor (#21285)

Summary:
Fixes #21261
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21285

Differential Revision: D15604270

Pulled By: soumith

fbshipit-source-id: c1c02348e338477a507052de0a1065cf42a99387",shihongzhi,https://api.github.com/repos/pytorch/pytorch/git/commits/2e59a0a646883f6b20f2e0d0af9c795e629bc115
770089c2b8ef8693151a2b5911f1138abbe78ecb,"math module support: isnan, asinh, atanh, cosh, sinh, and tanh (#19337)

Summary:
driazati and eellison Please review This PR is for #19026 .  Specifically, isnan, asinh, atanh, cosh, sinh, and tanh
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19337

Differential Revision: D15580932

Pulled By: driazati

fbshipit-source-id: 38513fa59088e038264f9f6f0d6374a13a165589",lhendre,https://api.github.com/repos/pytorch/pytorch/git/commits/770089c2b8ef8693151a2b5911f1138abbe78ecb
457c0f164ec26540bf73cf9b7b5cf76d12e8dfdb,"insert missing #pragma once in VariableTypeUtils.h (#21134)

Summary:
insert missing #pragma once keyword to prevent redefinition error
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21134

Differential Revision: D15607673

Pulled By: li-roy

fbshipit-source-id: 0000fa18e3c55e5d36a64b171d6e85eb4bc211a1",lsrock1,https://api.github.com/repos/pytorch/pytorch/git/commits/457c0f164ec26540bf73cf9b7b5cf76d12e8dfdb
38d68ad80330b5fc712471ab397e54594a15ec12,"Update randomness.rst (#21337)

Summary:
Following [this question on the forums](https://discuss.pytorch.org/t/reproducibility-and-performance/46504), I propose the following doc change. It clarifies that 'performance reduction' concerns the processing speed (and not the training accuracy).

Related website commit: https://github.com/pytorch/pytorch.github.io/pull/211
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21337

Differential Revision: D15622151

Pulled By: soumith

fbshipit-source-id: f0edeb20049f2ee715c400e7c57abb966864d621",BramVanroy,https://api.github.com/repos/pytorch/pytorch/git/commits/38d68ad80330b5fc712471ab397e54594a15ec12
42b2f56124f4959ce500b90e5c7529dd2c621650,"Fixing race condition at Module::forward method (#21398)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21398

Module::forward method calls find_method() function potentially in multiple threads.
Internally it calls find_offset() method and reads dict_ object.
If the correspondent name is not in a dictionary thread call insert() method and modifies dict_ object.
At the same time when first thread modifies dict_ object another thread can enter forward()->find_method()->find_offset() path
and access dict_ object for reading while it have been modified -> crash.
Moved mutex protection up to protect both calls find_offset() and insert().
Consider to use C++ 17 shared_mutex locking object instead of recursive_mutex object.

Reviewed By: bddppq

Differential Revision: D15638942

fbshipit-source-id: ca6a453448302a0b3666c87724755fa4e9ce242f",putivsky,https://api.github.com/repos/pytorch/pytorch/git/commits/42b2f56124f4959ce500b90e5c7529dd2c621650
22ddddfb809249d0e9bf06a80349b927e3984e54,"Continuation of Port max_unpool1d, max_unpool2d and max_unpool3d to ATen (#19465)

Summary:
This PR is a continuation of #15310, which itself is a continuation of #14845, #14941, & #15293. It should be synced up with the pytorch/master branch as of yesterday.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19465

Differential Revision: D15632268

Pulled By: ezyang

fbshipit-source-id: 8e337e8dc17ac31439935ccb530a7caf77f960e6",scopatz,https://api.github.com/repos/pytorch/pytorch/git/commits/22ddddfb809249d0e9bf06a80349b927e3984e54
d50dca4075119c3a8bef2ba90abfc8d51368261b,"fix two typos: ""a the"" => ""the""

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/20437

Differential Revision: D15321243

Pulled By: zou3519

fbshipit-source-id: 6e1690132769b8ef2fd679cb5898c378efac2112",zenogantner,https://api.github.com/repos/pytorch/pytorch/git/commits/d50dca4075119c3a8bef2ba90abfc8d51368261b
ae18f8e7617af601c63e460976caa57ab5215090,"Fix latex formular error about *normal (#21000)

Summary:
issue:
 https://github.com/pytorch/pytorch/issues/20903
the latex abort norm should be `\mathcal{N}(\text{mean}, \text{std}^2)`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21000

Differential Revision: D15695335

Pulled By: ezyang

fbshipit-source-id: 34dcca0acb20c297f876287e081cd44d11a3e516",selaselah,https://api.github.com/repos/pytorch/pytorch/git/commits/ae18f8e7617af601c63e460976caa57ab5215090
26bcadcc61631051abdd2d573a91c2467e9a288b,"Gumbel-Softmax Arxiv Docs Link Fix (#21376)

Summary:
Links separated #20297
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21376

Differential Revision: D15696413

Pulled By: ezyang

fbshipit-source-id: 513bd430e41c109aa2d0fbaa9a242acb2a12059b",KabirKwatra,https://api.github.com/repos/pytorch/pytorch/git/commits/26bcadcc61631051abdd2d573a91c2467e9a288b
87d10d49f476cdf6db0eda0f2dea57adf9ece057,"Bilinear Upsampling increased throughput (#19306)

Summary:
changed `UpsampleBilinearKernel` s.t. the throughput increased 40~50%.

I tested locally with my local test code -- **not pytorch's provided test code** -- because I am having a build problem ( which I made an issue about [here](https://github.com/pytorch/pytorch/issues/19184)). I tested with various tensor sizes and across all the sizes, it should a significant increase in throughput.

1. added `__restrict__`
2. instead of launch as many threads as there are output elements, I launched only `output_height * output_width` may threads and had each thread iterate through the channel and batch dimension.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19306

Differential Revision: D15701840

Pulled By: ezyang

fbshipit-source-id: 53c54d4f4e4a28b58ecc7d7ae6b864cbfc760e27",ThisIsIsaac,https://api.github.com/repos/pytorch/pytorch/git/commits/87d10d49f476cdf6db0eda0f2dea57adf9ece057
8ae7b1c486b24e19bb6db4d3f21680603e65efae,"Update functional.py doc (#21510)

Summary:
- Fixes a typo.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21510

Differential Revision: D15731277

Pulled By: ezyang

fbshipit-source-id: c3f8e110f5c61e797b857477b495168ea8d63cd5",cjlovering,https://api.github.com/repos/pytorch/pytorch/git/commits/8ae7b1c486b24e19bb6db4d3f21680603e65efae
e447a733a108d1b364b0faaaf0d5d23a171fc914,"Update module.py

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/21570

Differential Revision: D15732665

Pulled By: ezyang

fbshipit-source-id: caa12a8619ad1396540f787b5c849d29cc5b03bd",ypwhs,https://api.github.com/repos/pytorch/pytorch/git/commits/e447a733a108d1b364b0faaaf0d5d23a171fc914
9deab0cf0ef1847e2095c2fa5768beb300e22548,"Documentation for locking discipline in engine.cpp/.h (#21548)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21548

Added documentation as titled.

Reviewed By: ezyang

Differential Revision: D15723146

fbshipit-source-id: fab4a35c62f07256673318c0874701f7628b2f7a",malvika2147,https://api.github.com/repos/pytorch/pytorch/git/commits/9deab0cf0ef1847e2095c2fa5768beb300e22548
2378c120e6dacb8c42d2efae37cbcedf5fe28e99,"Implements divmod function (#20979)

Summary:
This PR refer to issue #18627
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20979

Differential Revision: D15743929

Pulled By: wanchaol

fbshipit-source-id: 967fc3fd519501e427176e10b112c8be1390540b",pandeykartikey,https://api.github.com/repos/pytorch/pytorch/git/commits/2378c120e6dacb8c42d2efae37cbcedf5fe28e99
fb9fbc009c9de3f4eb5fe49fc91fe73cbae51e0d,"Fix momentum bug in CyclicLR (#20401)

Summary:
Resolves issue https://github.com/pytorch/pytorch/issues/19003

The author of this issue also asked that `cycle_momentum` default to `False` if the optimizer does not have a momentum parameter, but I'm not sure what the best way to do this would be. Silently changing the value based on the optimizer may confuse the user in some cases (say the user explicitly set `cycle_momentum=True` but doesn't know that the Adam optimizer doesn't use momentum).

Maybe printing a warning when switching this argument's value would suffice?
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20401

Differential Revision: D15765463

Pulled By: ezyang

fbshipit-source-id: 88ddabd9e960c46f3471f37ea46013e6b4137eaf",emerali,https://api.github.com/repos/pytorch/pytorch/git/commits/fb9fbc009c9de3f4eb5fe49fc91fe73cbae51e0d
f59581218f760b873e11d76c6f229ca558e0c11f,"Fix spelling errors (#21665)

Summary:
alloctor -> allocator
excutable -> executable
excution -> execution
foward -> forward
initiaize -> initialize
paralell -> parallel
preprocesor -> preprocessor
tranpose -> transpose
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21665

Differential Revision: D15806155

Pulled By: soumith

fbshipit-source-id: d92b21ec8650a2b32f05faf9af0b7d2b073e992c",chosungmann,https://api.github.com/repos/pytorch/pytorch/git/commits/f59581218f760b873e11d76c6f229ca558e0c11f
deb2140c6e1e4e7be928c4a66a92b5ccfbad1210,"Throwing errors for min and max reductions in empty CUDA tensors (#19612)

Summary:
Related to https://github.com/pytorch/pytorch/issues/17750.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19612

Differential Revision: D15813649

Pulled By: gchanan

fbshipit-source-id: aa3dc34dd1e6d8bb24fa4c18891204108759bb35",LeviViana,https://api.github.com/repos/pytorch/pytorch/git/commits/deb2140c6e1e4e7be928c4a66a92b5ccfbad1210
b403b10ff98a6bc1a238e7ba4eee6393b6b89048,"Fix #11752: fix numerical issue in log_softmax (#21672)

Summary:
https://github.com/pytorch/pytorch/issues/11866 has corrected this issue in function `host_softmax` (aten/src/ATen/native/SoftMax.cpp). But I tried the example proposed in https://github.com/pytorch/pytorch/issues/11752. `log_softmax` is still not working for big logits.

I have looked into the source code, found that example had called `vec_host_softmax_lastdim`, not `host_softmax`.

This code fixes the issue in `_vec_log_softmax_lastdim` and has a test for `log_softmax`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21672

Differential Revision: D15856327

Pulled By: VitalyFedyunin

fbshipit-source-id: 7a1fd3c0a03d366c99eb873e235361e4fcfa7567",wolegechu,https://api.github.com/repos/pytorch/pytorch/git/commits/b403b10ff98a6bc1a238e7ba4eee6393b6b89048
34536e207ae48564fb559b7801d69b3ada4af6be,"Fix: convert Onnx DynamicSlice operator with 4 inputs to caffe2 faâ€¦ (#20846)

Summary:
I reported an issue [https://github.com/pytorch/pytorch/issues/20743](url)
and make this pull request for it
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20846

Reviewed By: zrphercule

Differential Revision: D15569135

Pulled By: houseroad

fbshipit-source-id: 96a2c818ef666a7d79b96decfa347d7154b34d5c",hexiaoting,https://api.github.com/repos/pytorch/pytorch/git/commits/34536e207ae48564fb559b7801d69b3ada4af6be
b2197ef2b0b384f863d5d830d61cfc08c994b2ec,"Adding support for JIT Fusion on Windows for CUDA (#21861)

Summary:
This pull request adds the necessary Windows DLL code to be able to support JIT fusion for CUDA. CPU JIT Fusion isn't supported. This also adds all the non-CPU JIT tests back in on Windows.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21861

Differential Revision: D15940939

Pulled By: soumith

fbshipit-source-id: e11f6af1ac258fcfd3a077e6e2f2e6fa38be4ef1",gavrielstate,https://api.github.com/repos/pytorch/pytorch/git/commits/b2197ef2b0b384f863d5d830d61cfc08c994b2ec
f9b3989206c1b265cf04f87ebf239c93c47af3c0,"handle slice with negative indices and indices exceeding tensor dimenâ€¦ (#21811)

Summary:
handle slice with negative indices and indices exceeding tensor dimensions.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21811

Reviewed By: zrphercule

Differential Revision: D15944243

Pulled By: houseroad

fbshipit-source-id: f7d987e9d8d704ade9d489599df14afbf1333428",liqunfu,https://api.github.com/repos/pytorch/pytorch/git/commits/f9b3989206c1b265cf04f87ebf239c93c47af3c0
6edaa11e5a3170c756af932cbedc7abb26924f22,"fix broken link

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/22064

Differential Revision: D15951107

Pulled By: mrshenli

fbshipit-source-id: 0b8f97bd2bbac26855cd2889e1fc619770974ee2",byronhe,https://api.github.com/repos/pytorch/pytorch/git/commits/6edaa11e5a3170c756af932cbedc7abb26924f22
0ac28c89661e53de90a99195e2c5e81c8145a714,"Quick fix for #18215, the CPU case (#21910)

Summary:
The bug is that when target_length == 0, there is no preceding BLANK state and the original implementation will lead to out of bound pointer access.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21910

Differential Revision: D15960239

Pulled By: ezyang

fbshipit-source-id: 7bbbecb7bf91842735c14265612c7e5049c4d9b3",zh217,https://api.github.com/repos/pytorch/pytorch/git/commits/0ac28c89661e53de90a99195e2c5e81c8145a714
3b700a43d57065a60d24b0355c673f6aaec5e3b8,"Add missing whitespace in error message (#21904)

Summary:
The current error message displays as:
     `RuntimeError: index koccurs twice in output`
A whitespace is missing between the index and 'occurs'
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21904

Differential Revision: D15878941

Pulled By: colesbury

fbshipit-source-id: 163dda1829bf4956978cd01fd0e751673580722d",cdancette,https://api.github.com/repos/pytorch/pytorch/git/commits/3b700a43d57065a60d24b0355c673f6aaec5e3b8
f176950a67adc44421a52f2fb8354e5a3cf26728,"Use lower case for strong wolfe option. (#22092)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22092
ghimport-source-id: ccc53ed2f1e16865237334a4dde4d162e21762e5

Test Plan: Imported from OSS

Differential Revision: D15955996

Pulled By: vincentqb

fbshipit-source-id: 8ffbea3b9ef8ff7021d42524fa46112da8a3438e",vincentqb,https://api.github.com/repos/pytorch/pytorch/git/commits/f176950a67adc44421a52f2fb8354e5a3cf26728
b52621c870562bbac229770678045a0a513c966b,"Revise error message for invalid Reduction (#22160)

Summary:
Say the user inputs reduction=False. Of course, we can't add a bool and a string, so the ValueError itself will error -which is more confusing to the user. Instead, we should use string formatting. I would use `f""{reduction} is not...""` but unsure whether we are ok with using f"""" strings.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22160

Differential Revision: D15981826

Pulled By: soumith

fbshipit-source-id: 279f34bb64a72578c36bdbabe2da83d2fa4b93d8",kiddyboots216,https://api.github.com/repos/pytorch/pytorch/git/commits/b52621c870562bbac229770678045a0a513c966b
d8de69d621f0fe0183049aefc648033983b162e0,"Adds symbolic op for logsumexp

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/22306

Differential Revision: D16046027

Pulled By: houseroad

fbshipit-source-id: 7319fd58321220941250c5b8eff024914798e392",iiSeymour,https://api.github.com/repos/pytorch/pytorch/git/commits/d8de69d621f0fe0183049aefc648033983b162e0
3cba9e8aaa81009a724bcc8c2545c11e070b00ac,"Error Message Paraphrasing (#22369)

Summary:
Saying `I` in an err msg is too subjective to be used in a framework.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22369

Differential Revision: D16067712

Pulled By: soumith

fbshipit-source-id: 2a390646bd5b15674c99f65e3c460a7272f508b6",andrewnaguib,https://api.github.com/repos/pytorch/pytorch/git/commits/3cba9e8aaa81009a724bcc8c2545c11e070b00ac
a4b2f3e21316461598fbebfdeb97259d2bbf29fa,"Implement AdamW optimizer (#21250)

Summary:
# What is this?
This is an implementation of the AdamW optimizer as implemented in [the fastai library](https://github.com/fastai/fastai/blob/803894051bef32304ceea0c8ea5e04db64ff26b8/fastai/callback.py) and as initially introduced in the paper [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101). It decouples the weight decay regularization step from the optimization step during training.

There have already been several abortive attempts to push this into pytorch in some form or fashion: https://github.com/pytorch/pytorch/pull/17468, https://github.com/pytorch/pytorch/pull/10866, https://github.com/pytorch/pytorch/pull/3740, https://github.com/pytorch/pytorch/pull/4429. Hopefully this one goes through.
# Why is this important?
Via a simple reparameterization, it can be shown that L2 regularization has a weight decay effect in the case of SGD optimization. Because of this, L2 regularization became synonymous with the concept of weight decay. However, it can be shown that the equivalence of L2 regularization and weight decay breaks down for more complex adaptive optimization schemes. It was shown in the paper [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101) that this is the reason why models trained with SGD achieve better generalization than those trained with Adam. Weight decay is a very effective regularizer. L2 regularization, in and of itself, is much less effective. By explicitly decaying the weights, we can achieve state-of-the-art results while also taking advantage of the quick convergence properties that adaptive optimization schemes have.
# How was this tested?
There were test cases added to `test_optim.py` and I also ran a [little experiment](https://gist.github.com/mjacar/0c9809b96513daff84fe3d9938f08638) to validate that this implementation is equivalent to the fastai implementation.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21250

Differential Revision: D16060339

Pulled By: vincentqb

fbshipit-source-id: ded7cc9cfd3fde81f655b9ffb3e3d6b3543a4709",mjacar,https://api.github.com/repos/pytorch/pytorch/git/commits/a4b2f3e21316461598fbebfdeb97259d2bbf29fa
2dd71b18c4bfc07bc07cafbd14e83e23add9e88f,"Fix PoolWindow crash from thread_local (#22405)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/19394

See https://developercommunity.visualstudio.com/content/problem/124121/thread-local-variables-fail-to-be-initialized-when.html for context.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22405

Differential Revision: D16090822

Pulled By: ezyang

fbshipit-source-id: 9fdd2c272fa7723fb62b906336d2e2620411b12b",dkotfis-oculus,https://api.github.com/repos/pytorch/pytorch/git/commits/2dd71b18c4bfc07bc07cafbd14e83e23add9e88f
d684112ec94a0efbdfe0e5caec3f577edc926726,"Output sequence probability with CTC beam search, optional multiple output sequences (#21927)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21927

Add `OUTPUT_PROB` output to CTCBeamSearchDecoderOp to return a probability for each sequence.

Add argument to output top-k instead of top-1 decoded sequences.

Reviewed By: SuperIRabbit

Differential Revision: D15797371

fbshipit-source-id: 737ca5cc4f90a0bcc3660ac9f58519a175977b69",warut-vijit,https://api.github.com/repos/pytorch/pytorch/git/commits/d684112ec94a0efbdfe0e5caec3f577edc926726
3507eaf3ea9eb768f721220236e1e49f4160fc26,"Add clone() implementation for QTensor (#22510)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22510

Added a new function to implement clone operation on quantized tensors. Also added a test case which can be tested as shown in test plan.

This change is required to be able to call torch.jit.trace on quantized models.
Clone implementation calls copy_ on QTensor internally.

Differential Revision: D16059576

fbshipit-source-id: 226918cd475521b664ed72ee336a3da8212ddcdc",supriyar,https://api.github.com/repos/pytorch/pytorch/git/commits/3507eaf3ea9eb768f721220236e1e49f4160fc26
43d36415b9800c500272d1ba175c72a2d09bdb7c,"torch.utils.data.Dataloader: documentation about RNG state consumption (#22540)

Summary:
the outcome from the pytorch forum issue: https://discuss.pytorch.org/t/dataloader-problem-problem-arises-when-shuffle-true/45631

The discussion is here: https://github.com/pytorch/pytorch/pull/20749
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22540

Differential Revision: D16131777

Pulled By: ezyang

fbshipit-source-id: 566deda1b44dc7fae54250e9b508d120851a2848",InnovArul,https://api.github.com/repos/pytorch/pytorch/git/commits/43d36415b9800c500272d1ba175c72a2d09bdb7c
a48cf8f52ddf8f23d307c2bd086f80a101dc6092,"Fixed RNNImplBase reset and flat_weights methods to handle bidirectional flag correctly (#22493)

Summary:
Fixing https://github.com/pytorch/pytorch/issues/19545:
Changed torch/csrc/api/src/nn/modules/rnn.cpp to be consistent with torch/nn/modules/rnn.py
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22493

Differential Revision: D16111433

Pulled By: pbelevich

fbshipit-source-id: edfa41e8a9889d64918998dc7c46b8763fdf5765",pbelevich,https://api.github.com/repos/pytorch/pytorch/git/commits/a48cf8f52ddf8f23d307c2bd086f80a101dc6092
456d27dff012805b815ae5a3c0a706e8b4f61770,"Update module.h (Fix a grammatical error of the comment in line 233) (#22548)

Summary:
Fix a grammatical error of the comment in line 233.
change from "" Returns an `OrderedDict` of he submodules of this `Module`""
to "" Returns an `OrderedDict` of the submodules of this `Module`""
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22548

Differential Revision: D16134534

Pulled By: zou3519

fbshipit-source-id: 33b1dd0fbc3a24bef99b6e0192566e2839292842",joker-xii,https://api.github.com/repos/pytorch/pytorch/git/commits/456d27dff012805b815ae5a3c0a706e8b4f61770
5395db22a498769295a3cbfd58824aa4140b49ea,"Typo fixed in documentation

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/22600

Differential Revision: D16156989

Pulled By: mrshenli

fbshipit-source-id: e491b083d872eaceb829028dadbab2e28ecfc785",shoaibahmed,https://api.github.com/repos/pytorch/pytorch/git/commits/5395db22a498769295a3cbfd58824aa4140b49ea
92e8d04098d98a29f1798167a563ffc048382658,"Sync worker requirement mismatches

Summary:
Syncing worker requirement mismatches to improve remote build time.

Created actions:
MEDIUM: 488
LARGE: 29
XXLARGE: 2

Updated actions:
From MEDIUM to LARGE: 227
From XLARGE to MEDIUM: 1
From XLARGE to LARGE: 1
From XLARGE to XXLARGE: 1
From LARGE to MEDIUM: 2
From LARGE to XLARGE: 2

Differential Revision: D16161669

fbshipit-source-id: 67a4e0d883ca3f1ca3185a8285903c0961537757",Skory,https://api.github.com/repos/pytorch/pytorch/git/commits/92e8d04098d98a29f1798167a563ffc048382658
7fcfed19e7c4805405f3bec311fc056803ca7afb,"Fix interpreter lines for files with python2-only syntax.

Reviewed By: lisroach

Differential Revision: D15362271

fbshipit-source-id: 48fab12ab6e55a8537b19b4623d2545ca9950ec5",thatch,https://api.github.com/repos/pytorch/pytorch/git/commits/7fcfed19e7c4805405f3bec311fc056803ca7afb
e2216ada658b4282c2bcde967bdf3d34ead93f39,"Properly formats errors rising up from C++ extension compilation (#22445)

Summary:
Here's a C++ extension with a missing semicolon:
```python
torch.utils.cpp_extension.load_inline('test', 'int main() { return 0 }')
```
which currently generates this error
```
RuntimeError: Error building extension 'test_v6': b'[1/2] c++ -MMD -MF main.o.d -
DTORCH_EXTENSION_NAME=test_v6 -DTORCH_API_INCLUDE_EXTENSION_H -isystem
/opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-
packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-
packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC
-isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++11 -c
/tmp/torch_extensions/test/main.cpp -o main.o\nFAILED: main.o \nc++ -MMD -MF main.o.d -
DTORCH_EXTENSION_NAME=test_v6 -DTORCH_API_INCLUDE_EXTENSION_H -isystem
/opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-
packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-
packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC
 -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++11 -c
/tmp/torch_extensions/test/main.cpp -o main.o\n/tmp/torch_extensions/test/main.cpp: In
function \xe2\x80\x98int main()\xe2\x80\x99:\n/tmp/torch_extensions/test/main.cpp:2:23:
error: expected \xe2\x80\x98;\xe2\x80\x99 before \xe2\x80\x98}\xe2\x80\x99 token\n int
main() { return 0 }\n                       ^\nninja: build stopped: subcommand failed.\n'
```

After this PR, the error is
```
RuntimeError: Error building extension 'test': [1/2] c++ -MMD -MF main.o.d -
DTORCH_EXTENSION_NAME=test -DTORCH_API_INCLUDE_EXTENSION_H -isystem
/opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-
packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-
packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC
 -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++11 -c
/tmp/torch_extensions/test/main.cpp -o main.o
FAILED: main.o
c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=test -
DTORCH_API_INCLUDE_EXTENSION_H -isystem /opt/conda/lib/python3.7/site-
packages/torch/include -isystem /opt/conda/lib/python3.7/site-
packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-
packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC
 -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++11 -c
/tmp/torch_extensions/test/main.cpp -o main.o
/tmp/torch_extensions/test/main.cpp: In function â€˜int main()â€™:
/tmp/torch_extensions/test/main.cpp:2:23: error: expected â€˜;â€™ before â€˜}â€™ token
 int main() { return 0 }
                       ^
ninja: build stopped: subcommand failed.
```
which is a lot easier to read.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22445

Differential Revision: D16094205

Pulled By: ezyang

fbshipit-source-id: 21043344aac260dc3e4e04d6a42898507bb840e4",andyljones,https://api.github.com/repos/pytorch/pytorch/git/commits/e2216ada658b4282c2bcde967bdf3d34ead93f39
6eb3969ac7f6ea1065b75ba15b0e7805573bf29c,"keep reuqires_grad unchanged after converting bn to syncbn (#22569)

Summary:
After converting BN layers to SyncBN layers, the function will set all `requires_grad = True` regardless of the original requires_grad states. I think it is a bug and have fixed it in this PR.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22569

Differential Revision: D16151647

Pulled By: zou3519

fbshipit-source-id: e2ad1886c94d8882485e7fb8be51ad76469ecc67",tianzhi0549,https://api.github.com/repos/pytorch/pytorch/git/commits/6eb3969ac7f6ea1065b75ba15b0e7805573bf29c
89d6e8804267d467e39529d710e2da39982905fa,"Add environment variables used in CONTRIBUTING example (#22736)

Summary:
Some other environment variables can be added to speed things up for development.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22736

Differential Revision: D16200904

Pulled By: soumith

fbshipit-source-id: 797ef91a863a244a6c96e0adf64d9f9b4c9a9582",kianasun,https://api.github.com/repos/pytorch/pytorch/git/commits/89d6e8804267d467e39529d710e2da39982905fa
8399197df67f90d7e5cb52d735eb55c803664b35,"Set up CI with Azure Pipelines (#22839)

Summary:
Introduce Azure Pipelines for the linting checks.  This is meant to be equivalent to the existing Travis linting phase.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22839

Differential Revision: D16260376

Pulled By: ezyang

fbshipit-source-id: 1e535c3096358be67a0dad4cd920a92082b2d18e",ethomson,https://api.github.com/repos/pytorch/pytorch/git/commits/8399197df67f90d7e5cb52d735eb55c803664b35
c6fe864db3e17830bf12957a64e6fd579ddeffad,"Add key_padding_mask kwarg to Transformer (#22588)

Summary:
Motivation:
The forward method of MultiheadAttention has a kwarg a key_padding_mask. This mask is of shape (N,S) where N is batch and S is sequence length. This mask is applied prior to attention softmax where True values in the mask are set to float('-inf'). This allows you to mask position j from attention for all position i in input sequence. It's typically used to mask padded inputs. So for a sample in a batch we will be able to make sure no encoder outputs depend on padding inputs. Currently the Transformer, TransformerEncoder, and TransformerEncoderLayer do not have this kwarg, and only have options for a (S,S), (T,T), and (S,T) masks which are applied equally across the batch for source input, target output, and target-source memory respectively. These masks can't be used for padding and are instead used for things like subsequent masking in language modeling, by masking the attention of position i to position j.

This diff exposes the key_padding_mask to Transformer, TransformerEncoder, and TransformerEncoderLayer forward methods which is ultimately passed to MultiheadAttention forward.

Open question: should we also allow a key_padding_mask for the decoder layer? As padding is usually at the end of each sentence in a batch and sentences are usually decoding from left to right, usually people deal with padding on decoded outputs by just masking those outputs at the loss layer. There might be some scenarios where it's needed though I don't think it would be common. People can also still just subclass and override the layers. We could also pass the input key_padding_mask to the memory <> decoder attention layer. Not sure if that's necessary though because the output of position i from each attention encoder layer won't depend on any masked positions in the input (even if position i is a masked position itself) so there's not really any point in masking position i again.
Adds the key_padding_mask kwarg to Transformer, TransformerEncoder, and TransformerEncoderLayer forward methods.
The standard TransformerEncoderLayer uses a MultiheadAttention layer as self_attn. MultiheadAttention forward method has a key_padding_mask kwarg that allows for masking of values such as padding per sequence in a batch, in contrast to the attn_mask kwarg which is usually of shape (S,S) and applied equally across the batch.

MultiheadAttention calls functional.multi_head_attention_forward, which has the same key_padding_mask kwarg of shape (N,S). Masked (True) values are set to float('-inf').
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22588

Test Plan:
buck test mode/dev caffe2/test:nn -- 'test_transformerencoderlayer \(test_nn\.TestNN\)'
buck test mode/dev caffe2/test:nn -- 'test_Transformer_cell \(test_nn\.TestNN\)'
buck test mode/dev caffe2/test:nn -- 'test_transformer_args_check \(test_nn\.TestNN\)'

Differential Revision: D16112263

Pulled By: lucasgadams

fbshipit-source-id: dc4147dd1f89b55a4c94e8c701f16f0ffdc1d1a2",lucasgadams,https://api.github.com/repos/pytorch/pytorch/git/commits/c6fe864db3e17830bf12957a64e6fd579ddeffad
12ac9171dbf8d19cd2750f6190bd79f4a70d6013,"fix error message

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/22982

Differential Revision: D16356464

Pulled By: soumith

fbshipit-source-id: 3ddd5de4cf5c000dcf5b2faed39283dc715cba25",orena1,https://api.github.com/repos/pytorch/pytorch/git/commits/12ac9171dbf8d19cd2750f6190bd79f4a70d6013
2ac9abf7591894525dfc7e76507f332df632ef19,"Fix memory leak in Adam, Adagrad, RMSProp (#23125)

Summary:
As reported in LaurentMazare/tch-rs#76, the memory grows when weight_decay is present when using Adam. It applies the same fix in https://github.com/pytorch/pytorch/issues/23007 to Adam, Adagrad and RMSProp.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23125

Differential Revision: D16402421

Pulled By: soumith

fbshipit-source-id: 59eb4bd81b8bd9e1a5f7c068ed841f70a4c38a80",jerry73204,https://api.github.com/repos/pytorch/pytorch/git/commits/2ac9abf7591894525dfc7e76507f332df632ef19
1c0309a9a924e34803bf7e8975f7ce88fb845131,"make OMP_NUM_THREADS default in launch.py (#22501)

Summary:
per https://github.com/pytorch/pytorch/issues/22260, default number of open mp threads are spawned to be the same of number of cores available, for multi processing data parallel cases, too many threads may be spawned and could overload the CPU, resulting in performance regression.

so set OMP_NUM_THREADS = number of CPU processors/number of processes in default to neither overload or waste CPU threads
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22501

Test Plan:
1. without and with this change, example codes result in same result
      python ~/local/fbsource-fbcode/fbcode/caffe2/torch/distributed/launch.py --nproc_per_node=2 pytorch/examples/yanlizhao/distributed_launch_example.py

  Setting OMP_NUM_THREADS environment variable for each process to be: 24, which
  is max(1, num_cpus / num_processes), you can further tune the variable for optimal performance in your application if needed.
  final loss =  tensor(0.5211, device='cuda:0', grad_fn=<MseLossBackward>)

Differential Revision: D16092225

Pulled By: zhaojuanmao

fbshipit-source-id: b792a4c27a7ffae40e4a59e96669209c6a85e27f",zhaojuanmao,https://api.github.com/repos/pytorch/pytorch/git/commits/1c0309a9a924e34803bf7e8975f7ce88fb845131
7ed9622fdfe34df4e41f7cfb00148a4b4c0cd7e5,"Read number of workspaces from argument in recurrent_network_op (#23272)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23272

We see significant performance improvements by limiting concurrency
at caffe2 level on mobile. This diff enables setting the number of caffe2
workspaces used during rnn inference.

Reviewed By: akyrola

Differential Revision: D16448611

fbshipit-source-id: 28abaddb4ea60bacb084ceb28cb7a4d1e67ccc17",jiatongzhou,https://api.github.com/repos/pytorch/pytorch/git/commits/7ed9622fdfe34df4e41f7cfb00148a4b4c0cd7e5
a936a90391f03280104def1df7a824224fc47bad,"caffe2/caffe2/fb/operators/cc_amrc: drop SIMD OpenMP vectorization

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/23235

Reviewed By: ajtulloch

Differential Revision: D16384612

Pulled By: luciang

fbshipit-source-id: a4c8257c6d3e151ba99167a152ad824b0dde7671",luciang,https://api.github.com/repos/pytorch/pytorch/git/commits/a936a90391f03280104def1df7a824224fc47bad
c23ba35009370734e2840189825a802f1752c129,"Skip QNNpack tests on ppc64le (where support is not enabled) (#23343)

Summary:
Proposed PR for
https://github.com/pytorch/pytorch/issues/23342

Disables execution of QNNpack tests if IS_PPC.
Basically this parallels the same skipping of tests for IS_WINDOWS as well, which is already present.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23343

Differential Revision: D16469218

Pulled By: soumith

fbshipit-source-id: 80b651d00e5d413e359cf418f79e20d74cd9c8e1",dncliss,https://api.github.com/repos/pytorch/pytorch/git/commits/c23ba35009370734e2840189825a802f1752c129
0842624d5001a48f34c1fbed2bf90e3262c7793f,"Fix upload issue with linux libtorch nightlies (#23368)

Summary:
This is a small fix on top of gh-23348, which fixed the libtorch
nightly build timeouts.

For the latest nighly build (25 July), see
https://circleci.com/workflow-run/33d0a24a-b77c-4a8f-9ecd-5646146ce684
The only failures are these uploads, which is because `aws s3 cp`
can only deal with one file at a time. The only way to make it do
multiple files at once is:
```
aws s3 cp . ""$s3_dir"" --exclude ""*""  --include ""libtorch-*.zip"" --recursive --acl public-read
```
which is much more verbose. executing one `cp` per file should be fine,
and this is also what's done in `binary_macos_upload.sh`

Closes gh-23039
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23368

Differential Revision: D16488853

Pulled By: soumith

fbshipit-source-id: 6dc04b4de2f6cd2de5ae9ad57a6e980f56896498",rgommers,https://api.github.com/repos/pytorch/pytorch/git/commits/0842624d5001a48f34c1fbed2bf90e3262c7793f
0c79753c0d384534225715ee3074d3469e433060,"Improve documentation for torch.enable_grad , torch.no_grad and torch.set_grad_enabled (#23310)

Summary:
Modified documentation for ` torch.enable_grad` , ` torch.no_grad` and `torch.set_grad_enabled`.

Fixes https://github.com/pytorch/pytorch/issues/19189
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23310

Differential Revision: D16489626

Pulled By: soumith

fbshipit-source-id: f0926e4f51ffd97521e67bee3a16ad954458247a",prasunanand,https://api.github.com/repos/pytorch/pytorch/git/commits/0c79753c0d384534225715ee3074d3469e433060
d6d7a5f0750a66bc8ecc0b3ab7ecfc6392ab9316,"only scatter in forward if multi-device per process (#22384)

Summary:
Scatter is unnecessary if only using one device, and it breaks on some custom data structures like namedtuple, so would like to avoid :)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22384

Differential Revision: D16428208

Pulled By: soumith

fbshipit-source-id: eaa3876b2b95c1006ccaaacdb62f54c5280e730c",astooke,https://api.github.com/repos/pytorch/pytorch/git/commits/d6d7a5f0750a66bc8ecc0b3ab7ecfc6392ab9316
19858f7cc67a084e0e0e27e8a89aeb5df721d102,"Sync worker requirement mismatches

Summary:
Syncing worker requirement mismatches to improve remote build time.

Created actions:
MEDIUM: 981
LARGE: 56

Updated actions:
From MEDIUM to LARGE: 10
From LARGE to MEDIUM: 3
From LARGE to XLARGE: 1

Differential Revision: D16532427

fbshipit-source-id: c58bf59e6c571627b3994f8cdfa79758fb85892b",alisdair04,https://api.github.com/repos/pytorch/pytorch/git/commits/19858f7cc67a084e0e0e27e8a89aeb5df721d102
9219a37c12e27991427c6d0054e9f99cbef65b79,"avoid Include the same header file twice (#23418)

Summary:
avoid Include the same header file twice
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23418

Differential Revision: D16546422

Pulled By: colesbury

fbshipit-source-id: 5cd868cce73d9199ced9b6f2f6f57bf42e5a5d5b",dongfangduoshou123,https://api.github.com/repos/pytorch/pytorch/git/commits/9219a37c12e27991427c6d0054e9f99cbef65b79
50ce9e09dae1775f8a504b403b4738b6970e1789,"Fix typos in .circleci/README.md (#23588)

Summary:
Fix typos in .circleci/README.md
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23588

Differential Revision: D16581934

Pulled By: ezyang

fbshipit-source-id: 39bf07e8d9d80493e15ecba7e846097ef44a6851",ilhamfp,https://api.github.com/repos/pytorch/pytorch/git/commits/50ce9e09dae1775f8a504b403b4738b6970e1789
fed5ca192c98619947cca2fb9491ac9624b787d2,"Adam/AdamW implementation minor fix (#22628)

Summary:
I have noticed a small discrepancy between theory and the implementation of AdamW and in general Adam. The epsilon in the denominator of the following Adam update should not be scaled by the bias correction [(Algorithm 2, L9-12)](https://arxiv.org/pdf/1711.05101.pdf). Only the running average of the gradient (_m_) and squared gradients (_v_) should be scaled by their corresponding bias corrections.

![adam_update](https://user-images.githubusercontent.com/13050245/60894105-11117f00-a230-11e9-9ba0-adad2ae2e0ae.png)

In the current implementation, the epsilon is scaled by the square root of `bias_correction2`.  I have plotted this ratio as a function of step given `beta2 = 0.999` and `eps = 1e-8`. In the early steps of optimization, this ratio slightly deviates from theory (denoted by the horizontal red line).

![plot](https://user-images.githubusercontent.com/13050245/60893952-cabc2000-a22f-11e9-8dc2-6353ad5d674d.png)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22628

Differential Revision: D16589914

Pulled By: vincentqb

fbshipit-source-id: 8791eb338236faea9457c0845ccfdba700e5f1e7",farhadrgh,https://api.github.com/repos/pytorch/pytorch/git/commits/fed5ca192c98619947cca2fb9491ac9624b787d2
4e6e11c139e4d4bcf948853dbdbf165284a0b3c8,"added opset10 ORT tests (#22993)

Summary:
Added a number of opset10 tests from Caffe2 to ORT
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22993

Differential Revision: D16467954

Pulled By: bddppq

fbshipit-source-id: 0b92694c7c0213bdf8e77e6f8e07e6bc8a85170a",neginraoof,https://api.github.com/repos/pytorch/pytorch/git/commits/4e6e11c139e4d4bcf948853dbdbf165284a0b3c8
445440a6a94988cbf1b8914e8117113cba25c5ff,"Add Virtual Memory and CPU percentage computation to AIBench (#23590)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23590

This diff adds CPU% and Virtual Memory computation by default to AIBench when doing mobile remote run

Reviewed By: llyfacebook

Differential Revision: D16469619

fbshipit-source-id: 670f3549c830a36bc456a57f2ea668f9f82dd15a",geof90,https://api.github.com/repos/pytorch/pytorch/git/commits/445440a6a94988cbf1b8914e8117113cba25c5ff
0015b188beb1a7f2ecebc4058ba4d70ec660db1b,"Fix typos

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/23770

Differential Revision: D16646852

Pulled By: ezyang

fbshipit-source-id: 826b041c0b528ae6e0b320d49d8141057c1f9bf3",galv,https://api.github.com/repos/pytorch/pytorch/git/commits/0015b188beb1a7f2ecebc4058ba4d70ec660db1b
52be1448e875645ee8ba639a1fa930e96ecff081,"Docs: Delete placeholder to use top-level file (#23869)

Summary:
Replaces and closes https://github.com/pytorch/pytorch/issues/23864.

When opening a pull request, GitHub shows you this:

![image](https://user-images.githubusercontent.com/1324225/62534181-30142880-b851-11e9-9b39-32d0ed6ff26c.png)

Or this:

![image](https://user-images.githubusercontent.com/1324225/62534569-24753180-b852-11e9-8242-8905ddda1f6f.png)

However, that links to https://github.com/pytorch/pytorch/blob/master/.github/CONTRIBUTING.md which looks like:

![image](https://user-images.githubusercontent.com/1324225/62534607-3656d480-b852-11e9-8c8c-37f54e8ca774.png)

As the commit message shows, that was a placeholder. There's already a real `CONTRIBUTING.md` document, so *delete the placeholder*.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23869

Differential Revision: D16667966

Pulled By: ezyang

fbshipit-source-id: c4135ebbb75de803ef227e4608e16da1a2e83a0c",hugovk,https://api.github.com/repos/pytorch/pytorch/git/commits/52be1448e875645ee8ba639a1fa930e96ecff081
10b1254edd9bcb4d3ab6a47f2bab31edef6485d0,"fix crash on torch.Tensor.repeat() for 0 repeats (#23766)

Summary:
This PR fixes https://github.com/pytorch/pytorch/issues/23603
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23766

Differential Revision: D16644866

Pulled By: soumith

fbshipit-source-id: ee7d368afdfe874133d0bd90f4d03a191ee22b13",oja,https://api.github.com/repos/pytorch/pytorch/git/commits/10b1254edd9bcb4d3ab6a47f2bab31edef6485d0
4ccb707161356bcb9caa1f0727084384e9d7bbbe,"Removing deprecated warning message from torch.h (#24002)

Summary:
discussed [here](https://github.com/pytorch/vision/issues/1173)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24002

Differential Revision: D16710635

Pulled By: yf225

fbshipit-source-id: 95117dd601061691d4cfd0d644777825aeaeaf8c",ShahriarSS,https://api.github.com/repos/pytorch/pytorch/git/commits/4ccb707161356bcb9caa1f0727084384e9d7bbbe
87508f401cabbc3f4d0aa64b604415e62131ed62,"Delete unnecessary file split_types.py

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/23754

Differential Revision: D16732834

Pulled By: ezyang

fbshipit-source-id: 087c573ecde8cd05dd7a28f47939a257e1cc25f3",wswday,https://api.github.com/repos/pytorch/pytorch/git/commits/87508f401cabbc3f4d0aa64b604415e62131ed62
d27fb41167f275c65fd90345ae1d2b856152f0e9,"tensor_numpy: add missing include header (#24042)

Summary:
This patch fixes the following error:
```
In file included from /path/to/lib/python3.6/site-packages/numpy/core/include/numpy/arrayobject.h:4:0,
                 from ../torch/csrc/utils/numpy_stub.h:19,
                 from ../torch/csrc/utils/tensor_numpy.cpp:2:
../torch/csrc/utils/tensor_numpy.cpp: In function 'bool torch::utils::is_numpy_scalar(PyObject*)':
../torch/csrc/utils/tensor_numpy.cpp:223:11: error: 'PyInt_Check' was not declared in this scope
   return (PyArray_IsIntegerScalar(obj) ||
           ^
../torch/csrc/utils/tensor_numpy.cpp:225:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24042

Differential Revision: D16732545

Pulled By: ezyang

fbshipit-source-id: 8d73d228b88b4a95daedcd7a4ef81c268830792e",nehaljwani,https://api.github.com/repos/pytorch/pytorch/git/commits/d27fb41167f275c65fd90345ae1d2b856152f0e9
0002448b43acbe48c2699ad71149ed163e5e6b25,"Enhance Tensor indexSelect performance (#23055)

Summary:
This is try to reduce the overhead on the index_select on CPU path at DLRM (https://github.com/facebookresearch/dlrm). To make src as contiguous can make it go into the parallelied path in Tensor indexSelect function
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23055

Differential Revision: D16603913

Pulled By: ezyang

fbshipit-source-id: baaa02f184a8e70f1193e5d96ada195a46d140b9",JianpingChen066,https://api.github.com/repos/pytorch/pytorch/git/commits/0002448b43acbe48c2699ad71149ed163e5e6b25
6be24be9ff7c7100e08971ca2031d4b720a139ac,"OpenCV 4 compatibility fix for caffe2/video (#24143)

Summary:
Trying to fix https://github.com/pytorch/pytorch/issues/24073 as in https://github.com/pytorch/pytorch/issues/9966.  Make caffe2 compile with OpenCV 4.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24143

Differential Revision: D16753624

Pulled By: ezyang

fbshipit-source-id: 524eac10a9285e0c0a803a8566917aa95aa0662c",leomao,https://api.github.com/repos/pytorch/pytorch/git/commits/6be24be9ff7c7100e08971ca2031d4b720a139ac
a5f697619cf8179ba1297c31ef9d49c48b4ff0f2,"Add interfaces in lr_scheduler.pyi (#23934)

Summary:
Some interfaces of schedulers defined in lr_scheduler.py are missing in lr_scheduler.pyi.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23934

Differential Revision: D16726622

Pulled By: ezyang

fbshipit-source-id: 45fd2d28fbb658c71f6fcd33b8997d6ee8e2b17d",tczhangzhi,https://api.github.com/repos/pytorch/pytorch/git/commits/a5f697619cf8179ba1297c31ef9d49c48b4ff0f2
1daac9c0a29525e1257c786e1d34c962e9faf283,"Update tensorboard.rst (#22026)

Summary:
**Patch Description**:
Update the docs to reflect one no longer needs to install tensorboard nightly, as Tensorboard 1.14.0 was [released last week](https://github.com/tensorflow/tensorboard/releases/tag/1.14.0).

**Testing**:
Haven't actually tested pytorch with tensorboard 1.14 yet. I'll update this PR once I have.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22026

Differential Revision: D16772136

Pulled By: orionr

fbshipit-source-id: 2e1e17300f304f50026837abbbc6ffb25704aac0",stephenroller,https://api.github.com/repos/pytorch/pytorch/git/commits/1daac9c0a29525e1257c786e1d34c962e9faf283
bd054e7cef00da543b80844aeb7ae0ac78c91675,"reduce memory usage for centered rmsprop (#24170)

Summary:
Reduce gpu memory usage by using in-place operation
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24170

Differential Revision: D16784495

Pulled By: vincentqb

fbshipit-source-id: 03820cdc9a3952b95b9af0f87d3a9bb0f21e9b4d",meijieru,https://api.github.com/repos/pytorch/pytorch/git/commits/bd054e7cef00da543b80844aeb7ae0ac78c91675
1c5e48bbd03f520559e6b8cd65d3b638f306fa0a,"Observer returns original tensor for post training quantization (#24196)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24196

Observer returns output with no changes for post training quant. This unifies observer semantics for QAT and PTQ.
ghstack-source-id: 88140887

Differential Revision: D16768277

fbshipit-source-id: fae7c94e3dc0eeda363e9982b3865a15113e11bd",raghuramank100,https://api.github.com/repos/pytorch/pytorch/git/commits/1c5e48bbd03f520559e6b8cd65d3b638f306fa0a
517b3c4cd20dfa44ca84a81633e82dceb6eb6204,"Fix validation of dynamic axes names (#23974)

Summary:
Existing code adds two enumerators to the set instead of forming their union.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23974

Differential Revision: D16732762

Pulled By: ezyang

fbshipit-source-id: 787737b7cf4b97ca4e2597e2da4a6ade863ce85c",MaximKalininMS,https://api.github.com/repos/pytorch/pytorch/git/commits/517b3c4cd20dfa44ca84a81633e82dceb6eb6204
32ed676b4672f0ffbec0fdcebdf5506e6021b0e5,"Make aten_to_numpy_dtype in tensor_numpy.h public. (#23943)

Summary:
The corresponding numpy_dtype_to_aten is public already so this
should be fine. Tests still pass.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23943

Differential Revision: D16690742

Pulled By: soumith

fbshipit-source-id: 81431a3316509cff8a9122e10e8f6a362bbcc9c0",heiner,https://api.github.com/repos/pytorch/pytorch/git/commits/32ed676b4672f0ffbec0fdcebdf5506e6021b0e5
87217cfd2a16171ffed5fa6ead66afc9878534f4,"Add align_corners option to grid_sample and affine_grid, change default to False (#23923)

Summary:
Resolves: https://github.com/pytorch/pytorch/issues/20785

Adds the `align_corners` option to `grid_sample` and `affine_grid`, paralleling the option that was added to `interpolate` in version 0.4.0.

In short, setting `align_corners` to `False` allows these functions to be resolution agnostic.
This ensures, for example, that a grid generated from a neural net trained to warp 1024x1024 images will also work to warp the same image upsampled/downsampled to other resolutions like 512x512 or 2048x2048 without producing scaling/stretching artifacts.

Refer to the documentation and https://github.com/pytorch/pytorch/issues/20785 for more details.

**Important**: BC-Breaking Change because of new default
The old functionality can still be achieved by setting `align_corners=True`, but the default is now set to `align_corners=False`, since this is the more correct setting, and since this matches the default setting of `interpolate`.

The vectorized 2D cpu version of `grid_sampler` is refactored a bit. I donâ€™t suspect that this refactor would affect the runtime much, since it is mostly done in inlined functions, but I may be wrong, and this has to be verified by profiling.

~The tests are not yet updated to reflect the new default. New tests should probably also be added to test both settings of `align_corners`.~ _Tests are now updated._
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23923

Differential Revision: D16887357

Pulled By: ailzhang

fbshipit-source-id: ea09aad7853ef16536e719a898db8ba31595daa5",bnehoran,https://api.github.com/repos/pytorch/pytorch/git/commits/87217cfd2a16171ffed5fa6ead66afc9878534f4
c0a796d95dfdf9c03cc23f1290c02f6b1c894867,"Update docs for softmax in onnx supported operators (#24832)

Summary:
Update the softmax in onnx supported operators from `softmax (only dim=-1 supported)` to `softmax`, as all cases of dim options are supported in:
[https://github.com/pytorch/pytorch/issues/18482](https://github.com/pytorch/pytorch/pull/18482): ONNX Export All Cases of Softmax
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24832

Differential Revision: D16896538

Pulled By: bddppq

fbshipit-source-id: 284039ffa42f09b0043e95cfe9f17e1afde53814",hackeritchy,https://api.github.com/repos/pytorch/pytorch/git/commits/c0a796d95dfdf9c03cc23f1290c02f6b1c894867
d33623f7c12b16fd49f74594e0e89ceecb817be9,"Make SobolEngine use random seed if not specified (#24884)

Summary:
Addresses https://github.com/pytorch/pytorch/issues/24881. Makes behavior consistent with the rest of the random functions.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24884

Test Plan: Unit tests

Reviewed By: sdsingh

Differential Revision: D16912036

Pulled By: Balandat

fbshipit-source-id: eff00cca989926a5d9e20d8846a8674f7cd270cb",Balandat,https://api.github.com/repos/pytorch/pytorch/git/commits/d33623f7c12b16fd49f74594e0e89ceecb817be9
012526dd6bf677fd029e9c27ecb49e401b210d2f,"Fix Typing Error for Padding with asymmetric signatures (#24895)

Summary:
This PR resolves https://github.com/pytorch/pytorch/issues/24806
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24895

Differential Revision: D16925208

Pulled By: ezyang

fbshipit-source-id: f4a374ca86e2e99faa30ca4b41c681e9976fe2de",Microsheep,https://api.github.com/repos/pytorch/pytorch/git/commits/012526dd6bf677fd029e9c27ecb49e401b210d2f
b9a51881782953ccbc60a4528e4a21a4cf8241a3,"Fixed Error in Transformer Example (#24837)

Summary:
In the examples for creating an instance of the Transformer module, src and tgt parameters (from forward) were added which are not present in the __init__ .
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24837

Differential Revision: D16938065

Pulled By: zhangguanheng66

fbshipit-source-id: 7b2d2180d95ddb65930ad83c87c926e35f2bf521",johnolafenwa,https://api.github.com/repos/pytorch/pytorch/git/commits/b9a51881782953ccbc60a4528e4a21a4cf8241a3
632aeb034dd57c0586fc3e979e65c362b936396a,"Fix log_prob() in torch.distributions.Uniform, HalfCauchy and Gamma (#23017)

Summary:
This fixes https://github.com/pytorch/pytorch/issues/22970. Specifically, `torch.distributions.uniform.Uniform.log_prob()` now works even if `value` is passed as a python float.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23017

Differential Revision: D16383258

Pulled By: vincentqb

fbshipit-source-id: 26943c33431d6da6f47e0897d6eda1c5f5541d28",vbsinha,https://api.github.com/repos/pytorch/pytorch/git/commits/632aeb034dd57c0586fc3e979e65c362b936396a
d62bca9792215591fe2e41a43387126cf41eca26,"jni-java wrapper for pytorchScript api (#25084)

Summary:
TLDR; initial commit of android java-jni wrapper of pytorchscript c++ api

The main idea is to provide java interface for android developers to use pytorchscript modules.
java API tries to repeat semantic of c++ and python pytorchscript API

org.pytorch.Module (wrapper of torch::jit::script::Module)
 - static Module load(String path)
 - IValue forward(IValue... inputs)
 - IValue runMethod(String methodName, IValue... inputs)

org.pytorch.Tensor (semantic of at::Tensor)
 - newFloatTensor(long[] dims, float[] data)
 - newFloatTensor(long[] dims, FloatBuffer data)

 - newIntTensor(long[] dims, int[] data)
 - newIntTensor(long[] dims, IntBuffer data)

 - newByteTensor(long[] dims, byte[] data)
 - newByteTensor(long[] dims, ByteBuffer data)

org.pytorch.IValue (semantic of at::IValue)
 - static factory methods to create pytorchscript supported types

Examples of usage api could be found in PytorchInstrumentedTests.java:

Module module = Module.load(path);
IValue input = IValue.tensor(Tensor.newByteTensor(new long[]{1}, Tensor.allocateByteBuffer(1)));
IValue output = module.forward(input);
Tensor outputTensor = output.getTensor();

ThreadSafety:
Api is not thread safe, all synchronization must be done on caller side.

Mutability:
org.pytorch.Tensor buffer is DirectBuffer with native byte order, can be created with static factory methods specifing DirectBuffer.
At the moment org.pytorch.Tensor does not hold at::Tensor on jni side, it has: long[] dimensions, type, DirectByteBuffer blobData

Input tensors are mutable (can be modified and used for the next inference),
Uses values from buffer on the momment of Module#forward or Module#runMethod calls.
Buffers of input tensors is used directly by input at::Tensor

Output is copied from output at::Tensor and is immutable.

Dependencies:
Jni level is implemented with usage of fbjni library, that was developed in Facebook,
and was already used and opensourced in several opensource projects,
added to the repo as submodule from personal account to be able to switch submodule
when fbjni will be opensourced separately.

ghstack-source-id: b39c848359a70d717f2830a15265e4aa122279c0
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25084
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25105

Reviewed By: dreiss

Differential Revision: D16988107

Pulled By: IvanKobzarev

fbshipit-source-id: 41ca7c9869f8370b8504c2ef8a96047cc16516d4",IvanKobzarev,https://api.github.com/repos/pytorch/pytorch/git/commits/d62bca9792215591fe2e41a43387126cf41eca26
2ec23804e22096d549d244ee39b24153a67476f1,"dictPop: dereference dict.find() iterator before calling dict.erase() (#25056)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25056

For some combinations of key and entry ordering (and only on an OSX
build) dict.pop() would return a value other than the popped one,
failing test_pop in test_jit.py. Caused by erase() mutating the
iterator returned from find(), fixed by dereferencing it first.

Test Plan: Imported from OSS

Differential Revision: D16975020

Pulled By: bhosmer

fbshipit-source-id: ce84e9aed6b90010121c0ef5d6c9ed8d2d1356b8",bhosmer,https://api.github.com/repos/pytorch/pytorch/git/commits/2ec23804e22096d549d244ee39b24153a67476f1
2c220763421af817ad0d523806f60f47e2ed4f58,"Moving sign function to ATen (#22861)

Summary:
This PR linked to https://github.com/pytorch/pytorch/issues/22806 moving sign function to ATen.

sign(x) supports bool,  and vectorized operation on CPU.
sign(NaN) is defined to return 0.
sign(bool) is a no-op, the resulting tensor will holds the same values than the input one.

- [x] CPU Backend
- [x] CUDA Backend
- [x] Bring support for bool dtype
- [x] Bring support for Half dtype
- [x] Add test for NaN
- [x] Add test for bool dtype
- [x] Delete legacy implementation in THTensorMoreMath.cpp

Performances:
```python
timeit -s 'import torch; x = torch.randn((1000, 1000))' -n 1000 'torch.sign(x)'
timeit -s 'import torch; x = torch.randn((1000, 1000), device=""cuda"")' -n 1000 'torch.sign(x); torch.cuda.synchronize()'
```

| device |  before  | after |
| :-------------: | :-------------: | :-----: |
| CPU    | 1.24 msec | 33.9 usec |
| GPU    | 680 usec | 7.13 usec  |
| CPU (1 thread) | 0.82 msec | 0.73 msec |
| GPU (1 thread) | 16.1 used | 15.9 usec |
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22861

Differential Revision: D16503452

Pulled By: VitalyFedyunin

fbshipit-source-id: a87ce7fff139642ef4ed791f15873074ad0d53af",mfuntowicz,https://api.github.com/repos/pytorch/pytorch/git/commits/2c220763421af817ad0d523806f60f47e2ed4f58
a74d702e576ff35653d22eee18962ad2359553a0,"Return a message instead of void from rpc udf (#25283)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25283

Return a message instead of void from rpc udf
This is to help thrift style rpc where there is no need for explicit send for a response.
We need to figure out how to solve the non-blocking callback case but don't want to block the thrift backed rpc agent implementation till then.
ghstack-source-id: 89130305

Differential Revision: D16825072

fbshipit-source-id: 75cb1c9aa5a10363b1c6b12cd21c50d7047d2268",satgera,https://api.github.com/repos/pytorch/pytorch/git/commits/a74d702e576ff35653d22eee18962ad2359553a0
687aa781dfafca0e182560806cc0437007911493,"Fix typo

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/25238

Differential Revision: D17076308

Pulled By: mrshenli

fbshipit-source-id: 2827150be1d15af63088db21051ab0e3476992e6",Mirwaisse,https://api.github.com/repos/pytorch/pytorch/git/commits/687aa781dfafca0e182560806cc0437007911493
d7cce323034c2bc50106870b5600eda019328ebc,"note location

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/25311

Differential Revision: D17093302

Pulled By: soumith

fbshipit-source-id: 14510351cf3f1568cfc415488eb0ba05a8af6cf8",vainaijr,https://api.github.com/repos/pytorch/pytorch/git/commits/d7cce323034c2bc50106870b5600eda019328ebc
590619ab8c2d237d4e0b55c8cc3552932afe7da5,"Support all_reduce a list of same-device tensors #21640 (#24949)

Summary:
addresses https://github.com/pytorch/pytorch/issues/21640 for CPU tensors and the Gloo backend.

Questions:
- ~~currently takes `AllreduceOptions`, since all of the options are the same. Would it be better to make a new `AllreduceCoalescedOptions` class?~~
- ~~I decided to inherit from `ProcessGroupGloo::AsyncWork` instead of `AsyncAllreduceWork` to shorten the inheritance chain a bit and for consistency with existing classes. However, this means that the two `getFunction` methods are copy-pasted. Would inheriting from `AsyncAllreduceWork` be preferable?~~
- ~~should the work class be named `AsyncCoalescedAllreduceWork` or `AsyncAllreduceCoalescedWork`?~~

thank you!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24949

Differential Revision: D17055580

Pulled By: mrshenli

fbshipit-source-id: e63b5fcaec6021053ea960776a09ee8cf11d1ec2",jfc4050,https://api.github.com/repos/pytorch/pytorch/git/commits/590619ab8c2d237d4e0b55c8cc3552932afe7da5
0cc92de447962ea9fdcaacb2f4101190f1d5cd8c,"Extend nn.Transformer to support BERT (gelu) (#24181)

Summary:
To use transformer for BERT, we need `gelu` activation. https://github.com/pytorch/pytorch/issues/24177
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24181

Differential Revision: D16790327

Pulled By: zhangguanheng66

fbshipit-source-id: b4eed21ad1a4d753bb090fa7fd78886714a6d761",Meteorix,https://api.github.com/repos/pytorch/pytorch/git/commits/0cc92de447962ea9fdcaacb2f4101190f1d5cd8c
05bf74a8904c6b3b054f33a79898d784848a5544,"Compare shapes of outputs and grad_outputs in autograd.grad (#25349)

Summary:
PR to compare shapes of `outputs` and `grad_outputs` in `torch.autograd.grad()`.

> grad_outputs should be a sequence of length matching output containing the pre-computed gradients w.r.t. each of the outputs.

resolve https://github.com/pytorch/pytorch/issues/17893
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25349

Differential Revision: D17119931

Pulled By: CamiWilliams

fbshipit-source-id: 86c9089e240ca0cea5f4ea8ec7bcff95f9d8cf53",CamiWilliams,https://api.github.com/repos/pytorch/pytorch/git/commits/05bf74a8904c6b3b054f33a79898d784848a5544
4fb28e5df913e742d60b120e0e31c4d6b4a2d595,"Fixes #25454

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/25456

Differential Revision: D17132144

Pulled By: ezyang

fbshipit-source-id: 68d11cbbb80f783959110b626594373ee41981d7",yaroslavvb,https://api.github.com/repos/pytorch/pytorch/git/commits/4fb28e5df913e742d60b120e0e31c4d6b4a2d595
861194e3f8fe59b6877f62722fa1cd28b69ba45b,"Fix windows build error when TBB enabled and Windows SDK installed (#25398)

Summary:
Fixed https://github.com/pytorch/pytorch/issues/25320
See the issue for more infomation.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25398

Differential Revision: D17131116

Pulled By: ezyang

fbshipit-source-id: cc3ebfe746abb33e24b4c884b08d9e57a1ea3476",cloudhan,https://api.github.com/repos/pytorch/pytorch/git/commits/861194e3f8fe59b6877f62722fa1cd28b69ba45b
885da48d2253c33200ff8dcbe817b311f92cc581,"remove protobuf usage from mobile build (#25493)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25493

remove protobuf usage from mobile build

Test Plan:
buck build //caffe2:torch

buck build -c 'protobuf.use_v3=true' -c 'project.ignore=true' fbsource//fbandroid/mode/dev_clang_asan //xplat/experimental/pytorch/predictor:predictor

Reviewed By: ljk53

Differential Revision: D17116846

fbshipit-source-id: d75e5f48e7eae960c0b5c7b8ef7f3359eb6ca4ec",linbinyu,https://api.github.com/repos/pytorch/pytorch/git/commits/885da48d2253c33200ff8dcbe817b311f92cc581
849c32f8e97af9f95e57b59d0db0aca6feced2d7,"Cpu-strided-complex support for binary-ops (#25534)

Summary:
In-tree changes to pytorch to support complex numbers are being submitted here.
Out-of-tree support for complex numbers is here: [pytorch-cpu-strided-complex extension](https://gitlab.com/pytorch-complex/pytorch-cpu-strided-complex)

Note: These changes do not support AVX/SSE operations on complex tensors.
Changes so far:

- [x]  Added complex support of torch.empty.
- [x]  Added complex support of CopyKernels
- [x]  Added complex support of BinaryOp kernels

Once these changes are applied the rest of the kernels are pretty easy.

ezyang
I have fixed the issues in the original [PR: 25373](https://github.com/pytorch/pytorch/pull/25373).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25534

Differential Revision: D17188390

Pulled By: ezyang

fbshipit-source-id: ade9fb00b2caa89b0f66a4de70a662b62db13a8c",dylanbespalko,https://api.github.com/repos/pytorch/pytorch/git/commits/849c32f8e97af9f95e57b59d0db0aca6feced2d7
0cc8ac75c91da7641c0b81a3e329bd780228ee96,"Alphabetize Package Reference section in Docs

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/25666

Differential Revision: D17190766

Pulled By: soumith

fbshipit-source-id: 836305062b0195b2f11be069447e05008c128d21",jlin27,https://api.github.com/repos/pytorch/pytorch/git/commits/0cc8ac75c91da7641c0b81a3e329bd780228ee96
38e4766349338174c126a65bc8c241c8f37a7785,"Add CosineAnnealingWarmRestarts to optim documentation (#25421)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/20028.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25421

Differential Revision: D17221542

Pulled By: soumith

fbshipit-source-id: 9c83c9ad6bf34ba59713c61485e4ef4b782a2792",TortillasAlfred,https://api.github.com/repos/pytorch/pytorch/git/commits/38e4766349338174c126a65bc8c241c8f37a7785
d47ced49ad8a6abd6479794ab3208223b9b86d73,"Adds a -m flag to pytorch.distributed.launch (#24910)

Summary:
Adds a '-m' flag to torch.distributed.launch that allows users to launch python modules using launch instead of specifying the full file path.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24910

Differential Revision: D17221653

Pulled By: pietern

fbshipit-source-id: 5c6453ed266fd121103b11caab303e3f9404227d",Phirefly9,https://api.github.com/repos/pytorch/pytorch/git/commits/d47ced49ad8a6abd6479794ab3208223b9b86d73
511d1875c5c00e5186abf7f092d0ae611dbc569d,"add torch.nn.Identity to __init__.pyi.in (#25777)

Summary:
I fixed https://github.com/pytorch/pytorch/issues/25694. Check it, please.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25777

Differential Revision: D17228224

Pulled By: ezyang

fbshipit-source-id: a8d36240892bcb7e669b8dce38419ff3fc9e9afd",eduidl,https://api.github.com/repos/pytorch/pytorch/git/commits/511d1875c5c00e5186abf7f092d0ae611dbc569d
a41ff31702ce0e0275e25147f61a4ccf01aad075,"Correctly gate __CUDA_ARCH__ with defined() (#25729)

Summary:
Undefined preprocessor macros being evaluated cause
errors on some compilers/configs. There is an ungated define in caffe2
which is inconsistent with the rest of the file and should be
fixed anyway because it's causing issues in ovrsource.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25729

Test Plan: contbuilds

Differential Revision: D17211552

Pulled By: akrieger

fbshipit-source-id: 499b123894b255f37ff68079c4ba3650b1599a5c",akrieger,https://api.github.com/repos/pytorch/pytorch/git/commits/a41ff31702ce0e0275e25147f61a4ccf01aad075
ec3793362f5f6226ae73cc28602f4a1299dbbded,"Documentation change of torch.where (#25554)

Summary:
Change the doc of torch.where. The parameters are x and y instead of input and other
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25554

Differential Revision: D17227193

Pulled By: soumith

fbshipit-source-id: 96d8a6f60ae8e788648247320ae715d0058de2b4",fdarmon,https://api.github.com/repos/pytorch/pytorch/git/commits/ec3793362f5f6226ae73cc28602f4a1299dbbded
825f4714f92818975c5a900bba45297299a4fa8e,"Fork QNNPACK into aten/src/ATen/native/quantized/cpu/qnnpack (#25500)

Summary:
The motivation for this move, and our long-term commitment to maintaining and integrating this code into ATen is described in the issue below:

https://github.com/pytorch/pytorch/issues/25621
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25500

Test Plan:
QNNPack unit tests, as follows:
OSS:
x86:
mkdir build; cd build; cmake ..; make all -j16 && make test
All 26 unit tests pass, both when built with ADD_DEFINITIONS(-DPYTORCH_QNNPACK_RUNTIME_QUANTIZATION=0) and ADD_DEFINITIONS(-DPYTORCH_QNNPACK_RUNTIME_QUANTIZATION=1)
ARM:
Make sure you have an android device available to adb either through one world or directly connected.
To compile and push do
$> adb shell mkdir /data/qnnpack && ./scripts/build-android-arm64.sh && adb push ./build/android/arm64-v8a/*-test /data/qnnpack
To execute tests, first $> adb shell to login into the device, then run all the tests by
$> for t in $(ls /data/qnnpack); do /data/qnnpack/$t; done
Repeat the exact same process with ADD_DEFINITIONS(-DPYTORCH_QNNPACK_RUNTIME_QUANTIZATION=0), and ADD_DEFINITIONS(-DPYTORCH_QNNPACK_RUNTIME_QUANTIZATION=1)
Repeat the exact same process with ./scripts/build-android-armv7.sh for AARCH32.

Reviewed By: ljk53

Differential Revision: D17194732

Pulled By: AshkanAliabadi

fbshipit-source-id: 9e627338ebd63aa917a36b717618c0643ccf40c8",AshkanAliabadi,https://api.github.com/repos/pytorch/pytorch/git/commits/825f4714f92818975c5a900bba45297299a4fa8e
4fac61a886c94456a2a9c49fe6e0a6eec502455d,"Fix typing on nn.Parameter (#25586)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/25399

As per https://github.com/pytorch/pytorch/issues/25580 I'm pushing this to test my changes on the CI
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25586

Differential Revision: D17259178

Pulled By: ezyang

fbshipit-source-id: d48cdd602bfda60c213f79a4f124df54a68ca698",theo-m,https://api.github.com/repos/pytorch/pytorch/git/commits/4fac61a886c94456a2a9c49fe6e0a6eec502455d
d1496183f59b59928ac7003503591560716f21c5,"Fix cuDnn build error with CC3.0 platform(#25820) (#25825)

Summary:
__ldg is only available for CC3.5 and above, add default implementation for CC3.0 platform.

This PR along with jcooky's PR of https://github.com/jcooky/pytorch/commit/ecdf4564d44835b3b2ffd18e286ad7e549231a14. make the pytorch master HEAD build and runs properly for CC3.0 platform(such as Retina MacBook Pro of Late 2013).

I test the mnist example from pytorch/examples with the wheel built, the test accuracy ends with 99% after 10 Epochs with GT 750M CC3.0 platform. CC3.0 platform decrease training time into about 1/5 of its cpu counterpart.

```
(pytorch) SamuelFdeMBP:mnist sfeng$ pip list | grep torch
pytorch-sphinx-theme          0.0.24               /Users/sfeng/GH/pytorch_110/docs/src/pytorch-sphinx-theme
torch                         1.3.0a0+a332583
torchvision                   0.5.0a0+0bd7080
(pytorch) SamuelFdeMBP:mnist sfeng$ date && time python main.py && date
æ—¥  9  8 07:17:38 CST 2019
/Users/sfeng/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/cuda/__init__.py:132: UserWarning:
    Found GPU0 GeForce GT 750M which is of cuda capability 3.0.
    PyTorch no longer supports this GPU because it is too old.
    The minimum cuda capability that we support is 3.5.

  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))
Train Epoch: 1 [0/60000 (0%)]	Loss: 2.300039
......
Train Epoch: 10 [59520/60000 (99%)]	Loss: 0.007440

Test set: Average loss: 0.0322, Accuracy: 9895/10000 (99%)

real	2m39.962s
user	4m13.625s
sys	0m9.672s
æ—¥  9  8 07:20:18 CST 2019

(pytorch) SamuelFdeMBP:mnist sfeng$ date && time python main.py --no-cuda && date
æ—¥  9  8 07:20:40 CST 2019
Train Epoch: 1 [0/60000 (0%)]	Loss: 2.300039
Train Epoch: 1 [640/60000 (1%)]	Loss: 2.213470
Train Epoch: 1 [1280/60000 (2%)]	Loss: 2.170460
......
Train Epoch: 10 [58880/60000 (98%)]	Loss: 0.005681
Train Epoch: 10 [59520/60000 (99%)]	Loss: 0.007686

Test set: Average loss: 0.0319, Accuracy: 9894/10000 (99%)

real	12m6.604s
user	75m53.129s
sys	3m41.744s
æ—¥  9  8 07:32:47 CST 2019
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25825

Differential Revision: D17252176

Pulled By: soumith

fbshipit-source-id: 70bf84ae6380be86b56344b161a52fb06a53a1b2",XiagenFeng,https://api.github.com/repos/pytorch/pytorch/git/commits/d1496183f59b59928ac7003503591560716f21c5
378881e903999342c84ad6f20224f712148e0359,"Enable log_softmax and CrossEntropyLoss for bfloat16 (#24457)

Summary:
Enabled torch.nn.functional.log_softmax and torch.nn.CrossEntropyLoss for bfloat16 data type.
In order to do that, following dependency have to be enabled.
- RNE (round to nearest even)
- AccumulateType
- bfloat16 arithmetic operator overload

Also, we implement std::numeric_limits fully support for bfloat16 data type

background for dependency:
- RNE vs truncate
From torch.nn.CrossEntropyLoss test. input_size=(128, 1000)
RNE result:
float    output:  tensor(7.3981, dtype=torch.float32, grad_fn=<NllLossBackward>)
bfloat16 output:  tensor(7.3125, dtype=torch.bfloat16, grad_fn=<NllLossBackward>)
truncate result:
float    output:  tensor(7.3981, dtype=torch.float32, grad_fn=<NllLossBackward>)
bfloat16 output:  tensor(5.8750, dtype=torch.bfloat16, grad_fn=<NllLossBackward>)

- scalar_t vs AccumulateType (AccumulateType of bfloat16 is float)
AccumulateType is essential to keep accuracy, especially for reduction related operation.
we have verified it with both local case and real topology. It turns out that bfloat16 type accumulator would cause huge relative error when elements number is large, even more than 50%.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24457

Differential Revision: D17113018

Pulled By: ezyang

fbshipit-source-id: 8d61297ca118f9b5c6730a01efcf3a3704d2f206",hongzhen1,https://api.github.com/repos/pytorch/pytorch/git/commits/378881e903999342c84ad6f20224f712148e0359
1777eb2ed92554ef5ff009bf38f03d52ec089889,"fix typo: toDense --> to_dense #25706 (#25832)

Summary:
Only fixes a minor typo in [torch.sparse.FloatTensor docs](https://pytorch.org/docs/stable/sparse.html#torch.sparse.FloatTensor.toDense).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25832

Differential Revision: D17276700

Pulled By: soumith

fbshipit-source-id: cf3d550d5756b000a4e864170ecd4b31826b40f8",Nikronic,https://api.github.com/repos/pytorch/pytorch/git/commits/1777eb2ed92554ef5ff009bf38f03d52ec089889
67281deec03c7cea4d4fca41fd768202c5610276,"Fix missing str to int conversion in the commit f71ddd42 (#25861)

Summary:
Came up in internal testing w/ python 2.7
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25861

Differential Revision: D17261070

Pulled By: soumith

fbshipit-source-id: 412fe5e53ef4d8f2564d77dd17b480bb58cc391e",mshivama,https://api.github.com/repos/pytorch/pytorch/git/commits/67281deec03c7cea4d4fca41fd768202c5610276
76ee02f10d2c936606bebf7f364a56f34a612823,"Rename packed tensor accessor (#25654)

Summary:
Closes https://github.com/pytorch/pytorch/issues/19268

This does the renaming suggested by ezyang in https://github.com/pytorch/pytorch/issues/19268#issuecomment-490478887 except that the templated version of `packed_accessor` is also renamed to `generic_packed_accessor`.

Additionally, all of the users I could find in `ATen/native/cuda` are updated without changing their index types.

The corresponding tutorial update is in pytorch/tutorials#644
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25654

Differential Revision: D17259208

Pulled By: ezyang

fbshipit-source-id: 172a46f623d544ca16f7ed5077b6e4f57a3d1f21",peterbell10,https://api.github.com/repos/pytorch/pytorch/git/commits/76ee02f10d2c936606bebf7f364a56f34a612823
ec8e75ea92ae2b5ea73b4aeb3ec7cb39e9f95db9,"Fix int32 overflow in SummaryOps.cu getBin #25747 (#25748)

Summary:
Fixes issue https://github.com/pytorch/pytorch/issues/25747 by upcasting to int64 before multiplication. Should be good enough for all reasonable nbins
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25748

Differential Revision: D17269111

Pulled By: ezyang

fbshipit-source-id: 484be39080571203264a1bb9898ecf23d1aeafab",kaczorrrro,https://api.github.com/repos/pytorch/pytorch/git/commits/ec8e75ea92ae2b5ea73b4aeb3ec7cb39e9f95db9
b9bf91feb8e866e030aacfaa9ad4fa9e0a5c84df,"Add torch.backends.mkldnn.enabled flag (#25459)

Summary:
This PR is about add torch.backends.mkldnn.enabled flag said in https://github.com/pytorch/pytorch/issues/25186 which can be used disable mkldnn at runtime step as torch.backends.cudnn.enabled.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25459

Differential Revision: D17258926

Pulled By: ezyang

fbshipit-source-id: e179ad364cc608fdaa7d0f37e2e762ceb5eda598",jiayisunx,https://api.github.com/repos/pytorch/pytorch/git/commits/b9bf91feb8e866e030aacfaa9ad4fa9e0a5c84df
076eaf4ccf9e8f669a0e62ba6495a126e16aff86,"Exposing Fused8BitRowwiseQuantizedToFloat in PyTorch (#26080)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26080

Will be used in c2 ctr_mbl_feed model to PyTorch conversion

Test Plan: Unit test

Reviewed By: yinghai

Differential Revision: D17337604

fbshipit-source-id: a90d9f5dc38301608d1562c6f2418e7f4616e753",qizzzh,https://api.github.com/repos/pytorch/pytorch/git/commits/076eaf4ccf9e8f669a0e62ba6495a126e16aff86
327e94f51bcbbe78046f0fbfcdb7c9a240487589,"Add __s390x__ compiler define for s390 builds. (#26233)

Summary:
pytorch builds fail on 390 architecture because
in simd.h the ifdef macros default to an x86 asm instruction.
This patchs adds an ifdef __s390x__ to be able to build on s390.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26233

Differential Revision: D17392714

Pulled By: soumith

fbshipit-source-id: 037672bfea64fc5e52da2390d93b973534137c12",abalib,https://api.github.com/repos/pytorch/pytorch/git/commits/327e94f51bcbbe78046f0fbfcdb7c9a240487589
4a947b607c915f2381590d7a9b807aea69bde27c,"Clarified ambiguous docstring in NegativeBinomial

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/25923

Differential Revision: D17392848

Pulled By: soumith

fbshipit-source-id: 2833e72fe449c74dfd8273a7b1eb46c05c63d999",kyleengel,https://api.github.com/repos/pytorch/pytorch/git/commits/4a947b607c915f2381590d7a9b807aea69bde27c
31960e887228d25cae66757fe97813676c96d3d6,"Add missing argument for failing function call (#26311)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26311

We are currently unable to deploy models due to D16955662 changing function signature of ```quantized_lstm(``` but the function call here (https://fburl.com/diffusion/e4wrmx83) not passing the newly added ```use_dynamic``` param.

Here is the details of the error: P111215482

```
E0916 12:36:16.423516 1149877 ExceptionTracer.cpp:214] exception stack complete
terminate called after throwing an instance of 'torch::jit::script::ErrorReport'
  what():
Arguments for call are not valid.
The following operator variants are available:

  aten::quantized_lstm(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, *, int? dtype=None) -> (Tensor, Tensor, Tensor):
  Keyword argument use_dynamic unknown.
```

This diff fixes that.

Test Plan:
Running quantization tests after.

```buck test mode/dev caffe2/test:jit -- 'test_quantization_modules \(test_jit\.TestScript\)'```

https://our.intern.facebook.com/intern/testinfra/testrun/5910974518872494

Also, currently building a package (language_technology.translation.jedi.scripts:35c3643) and testing this (f138747078).

f138771702

Reviewed By: jhcross

Differential Revision: D17404451

fbshipit-source-id: 390d2ce1ecbdd63a07a8f16c80e4c3ac25ab0a99",akinh,https://api.github.com/repos/pytorch/pytorch/git/commits/31960e887228d25cae66757fe97813676c96d3d6
e5d9a5e5bead14bbdd7e39d62171efd6e0d39d14,"Fix typo in docs.

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/26263

Differential Revision: D17397190

Pulled By: ezyang

fbshipit-source-id: 62e3c4c3021c728a3314262528579676d605a81e",mkuchnik,https://api.github.com/repos/pytorch/pytorch/git/commits/e5d9a5e5bead14bbdd7e39d62171efd6e0d39d14
b8ae4d0f1cdf1bed61ddfaeea68a7b69251dcea0,"Resolve #25605 cyclic reference in _LRScheduler (#25776)

Summary:
Cyclic reference was introduced in a previous version due to runtime overwriting of the bound method `optimizer.step`. This is now avoided by keeping a weak reference to the optimizer instance.

Credit: https://stackoverflow.com/questions/26157952/why-set-a-bound-method-to-python-object-create-a-circular-reference
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25776

Differential Revision: D17420770

Pulled By: ezyang

fbshipit-source-id: 546ec94cf725ebfddb310b24e6a2e146ddecd1f6",huzecong,https://api.github.com/repos/pytorch/pytorch/git/commits/b8ae4d0f1cdf1bed61ddfaeea68a7b69251dcea0
872ca919a9bf7dc32e47795c3758e03277ccf864,"Distance module (#26424)

Summary:
Adds `Distance` module parity.
https://github.com/pytorch/pytorch/issues/25883
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26424

Differential Revision: D17487314

Pulled By: yf225

fbshipit-source-id: c7d124cb4afb08a4733e7212af0bb276bf32d172",jon-tow,https://api.github.com/repos/pytorch/pytorch/git/commits/872ca919a9bf7dc32e47795c3758e03277ccf864
916eee182c9dc8d335501f6672842c6d29f0af58,"Fix for Conv shape check prints overflowed ints (#25827)

Summary:
Fix for issue https://github.com/pytorch/pytorch/issues/19947
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25827

Differential Revision: D17508653

Pulled By: soumith

fbshipit-source-id: 1afec60b9b39de5f2d0be44a170650aa4c1879cf",spidyDev,https://api.github.com/repos/pytorch/pytorch/git/commits/916eee182c9dc8d335501f6672842c6d29f0af58
557246b77d71fe91e79957bb00263b6b51568442,"Fixing the calling parameters of write_gif function of the moviepy.

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/21218

Differential Revision: D17509260

Pulled By: ezyang

fbshipit-source-id: 51e392cbcc20ade4c38c4edb75919f9bb314a830",danielmatte,https://api.github.com/repos/pytorch/pytorch/git/commits/557246b77d71fe91e79957bb00263b6b51568442
fbc3c14830bb2ccc16ce35450419ca1f188d42a8,"adding OpProfile proto into ProfDAGProtos to support storing operation cost (#26677)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26677

This diff adds OpProfile proto into ProfDAGProtos to support storing operation cost. During performance estimation idx, net_name, type, and exec_time will be stored in this proto.

Test Plan:
```
buck test caffe2/caffe2/fb/net_transforms/tests/:stats_collector_test
buck test caffe2/caffe2/fb/net_transforms/tests/:perf_estimator_test
buck run caffe2/caffe2/fb/distribute/snntest/cogwheel/:cogwheel_snntest_offline_training_simple_online_training
```

Reviewed By: heslami

Differential Revision: D17533791

fbshipit-source-id: a339c8eadcac891aa631daaf64522b69876b5045",wenqicaofb,https://api.github.com/repos/pytorch/pytorch/git/commits/fbc3c14830bb2ccc16ce35450419ca1f188d42a8
5fc52482cf7984f10185bc35d6bc06924dbc13bd,"torch.load default encoding change to 'utf-8' (#26421)

Summary:
Default encoding when using torch.load to 'utf-8'

This commit provides changes for cases where user tries to torch.load
a pickled module with non-ASCII characters in the docstring as
discussed in https://github.com/pytorch/pytorch/issues/21743. The default encoding was changed from 'ascii'
to 'utf-8'. Documentation for `torch.load` was updated and two tests
(loading py2 unicode module with unicode in it; error throwing when
user explicitly sets wrong encoding) were written.

~~This commit provides changes for better error handling in cases
where user tries to `torch.load` a pickled module with non-ASCII
characters in the docstring as discussed in https://github.com/pytorch/pytorch/issues/21743.~~

Ping ezyang
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26421

Differential Revision: D17581633

Pulled By: yf225

fbshipit-source-id: f8e77dcf7907092771149aad8ede6cfb73c21620",nmilosev,https://api.github.com/repos/pytorch/pytorch/git/commits/5fc52482cf7984f10185bc35d6bc06924dbc13bd
93836015234cc7a7ed95ea183a2d539e0e91feb9,"fix to operate on cuda kernel with clang and libc++ (#25553)

Summary:
We find a bug about `std::tuple` with nvcc.

In C++11, `std::tuple` constructor is constexpr in libstdc++, but is not constexpr in libc++.

https://github.com/pytorch/pytorch/blob/c36b77fcdad3d54227cf0fd51693eb57035002c0/aten/src/ATen/native/cuda/Loops.cuh#L109-L111

The lines have occurred crashes in CUDA with a message `scan failed with synchronize`. It is a error message of cuda initialization.

The purpose of this PR is fixed for loop in nvcc and libc++ by not using `std::tuple`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25553

Differential Revision: D17582118

Pulled By: yf225

fbshipit-source-id: d6f62ed46c2415b48eb49f8a051cf3c0e7cb23ce",jcooky,https://api.github.com/repos/pytorch/pytorch/git/commits/93836015234cc7a7ed95ea183a2d539e0e91feb9
90ffab6e3713a40f47f4c9d3d3b16d56e83f97c4,"enable double backward for non-cudnn LSTM and GRU (#26660)

Summary:
An attempt to enable double backward for non-cudnn LSTM and GRU (see https://github.com/pytorch/pytorch/issues/25315, https://github.com/pytorch/pytorch/issues/20449). RNN works already because it does not rely on fused kernels.
This does not implement double backward function itself, because that is pretty hard to spell out. Instead, it implements backward using differentiable operations, so that double backward can be done automatically.
The good: seems to work, no effect on performance on the usual case without double backward. because fused lstm backward is used.
The bad: Performance of backward and, especially, double backward, is pretty bad. Scripting would still be a preferred way if we want a performant solution. Performance and/or memory use can be slightly improved if in-place variants can be used for sigmoid_backward and tanh_backward to avoid cat in the end, but I'm not yet sure it's possible, and in any case it is only slight improvement.
The ugly: I could not figure out a way to reuse workspace that contains the sum of the gates with the applied sigmoid and tanh operations, so that's probably another perf and memory hit.
cc soumith, albanD. If you think this approach is viable, I can extend to GRU and RNN.
Thanks to mcarilli whose approach to double backward in weight norm I copied.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26660

Test Plan: added tests to check gradgrad for GRU and LSTM with cudnn disabled.

Differential Revision: D17581489

Pulled By: ngimel

fbshipit-source-id: efd204289e9a0e94d94896a0b3bff5cf6246cafa",ngimel,https://api.github.com/repos/pytorch/pytorch/git/commits/90ffab6e3713a40f47f4c9d3d3b16d56e83f97c4
8f359a48a6843702e61eeefc8294f73c1a13cac1,"Fix building with PARALLEL_BACKEND=NATIVE_TBB (#26742)

Summary:
Fixing https://github.com/pytorch/pytorch/issues/26721
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26742

Test Plan:
```
export USE_OPENMP=0
export USE_TBB=1
export BLAS=MKL
export MKL_THREADING=TBB
export MKLDNN_THREADING=TBB
export PARALLEL_BACKEND=NATIVE_TBB
export USE_CUDA=0
python setup.py build
```

Reviewed By: dskhudia

Differential Revision: D17586233

Pulled By: ilia-cher

fbshipit-source-id: 8e8befa6aa776b8c2b27bb4b79a3bff33dbcba7e",abonnet,https://api.github.com/repos/pytorch/pytorch/git/commits/8f359a48a6843702e61eeefc8294f73c1a13cac1
d63d7ab99772a37bee97673c3e709cacdc46b977,"Expose PiecewiseLinearTransform to PyTorch

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/26903

Test Plan: Unit Test

Reviewed By: bddppq

Differential Revision: D17585637

fbshipit-source-id: fe669aaf3301d7efb5c28ec0097945d55a71773d",simran2905,https://api.github.com/repos/pytorch/pytorch/git/commits/d63d7ab99772a37bee97673c3e709cacdc46b977
23260f3e7db42b01efb6b9846603c49b8c2b4cbf,"Add logging in constant propagation pass

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/26653

Test Plan: Imported from OSS

Reviewed By: Krovatkin

Differential Revision: D17621895

Pulled By: bzinodev

fbshipit-source-id: eda7df423a995590fd50052424891b6d04277882",bzinodev,https://api.github.com/repos/pytorch/pytorch/git/commits/23260f3e7db42b01efb6b9846603c49b8c2b4cbf
3a18e2e7686d01e4222b5f3fafdbe0b6c9154797,"support re-creating/destroying process groups when some trainers recover after failures (#26912)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26912

group name is used as prefix in the c10d store and without a consistent name process group cannot be initialized.

When process group doesn't have an explicit name (only WORLD (default) process group can have an explicit name), we use global _group_counter to generate the name. We need to reset the counter on destruction to allow consistent value to be generated when we re-create process groups after some trainers recover from failure.

Test Plan: existing tests passed

Reviewed By: mrshenli

Differential Revision: D17594268

fbshipit-source-id: 17f4d2746584dadaa5d468085d871ff3e95a1c84",mehta-vikas,https://api.github.com/repos/pytorch/pytorch/git/commits/3a18e2e7686d01e4222b5f3fafdbe0b6c9154797
ad58045af9454ba4917070d9da3284f442a02312,"Remove LOG(INFO) from math_cpu.cc (#27001)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27001

This unconditional log line spams the logs enough that it's a drag on cpu and will eventually fill up logs.

Test Plan: Allow unit test and automated testing to give feedback.

Reviewed By: jspark1105

Differential Revision: D17638140

fbshipit-source-id: 4e8a44bda31327ba7e797f7579a9e3bf866eef7e",elliottneilclark,https://api.github.com/repos/pytorch/pytorch/git/commits/ad58045af9454ba4917070d9da3284f442a02312
1afe3fc01eb194a3e7ce58240462de2121646233,"Fixed seek offset size to 64bit. (#27047)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/26998
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27047

Differential Revision: D17666050

Pulled By: ezyang

fbshipit-source-id: f02ebd5320ae25f8949be20d0744fe3cd3e2fee9",yn4bit,https://api.github.com/repos/pytorch/pytorch/git/commits/1afe3fc01eb194a3e7ce58240462de2121646233
46539eee0363e25ce5eb408c85cefd808cd6f878,"Ensure that DDP wrapped module has parameters that require gradients (#25858)

Summary:
â€¦ent - see https://github.com/pytorch/pytorch/issues/25552

**TEST PLAN**
```
python test/run_test.py -f distributed
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25858

Differential Revision: D17687542

Pulled By: danthe3rd

fbshipit-source-id: 11bfe4142e72bb21382b30379fe10e60418c7ec9",danthe3rd,https://api.github.com/repos/pytorch/pytorch/git/commits/46539eee0363e25ce5eb408c85cefd808cd6f878
bb51980766182b42f631e3f156233d0fdcc881bc,"make default string arguments in schemas human readable (#27088)

Summary:
[jit] String default args get printed as ascii values https://github.com/pytorch/pytorch/issues/25804
https://github.com/pytorch/pytorch/issues/25804
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27088

Differential Revision: D17689732

Pulled By: Krovatkin

fbshipit-source-id: f385b2fe44c5a2387bfcb6484edf7faa92bc8edf",free25zer,https://api.github.com/repos/pytorch/pytorch/git/commits/bb51980766182b42f631e3f156233d0fdcc881bc
0c4bc2753951fb0f98da98355a1cc78bb1e5361a,"Mention magma-cuda101 package in install instructions (#27325)

Summary:
There is a magma package for the newest CUDA verson (10.1), mention it here lest someone try to mistakenly use the version for CUDA 10.0.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27325

Differential Revision: D17749535

Pulled By: soumith

fbshipit-source-id: 2d34a7af1218e6157935bfd5e03f4d2c0f00f200",ngoldbaum,https://api.github.com/repos/pytorch/pytorch/git/commits/0c4bc2753951fb0f98da98355a1cc78bb1e5361a
f5df46ce397335c43cb15457e3828a1a66cbbfa9,"Set MINIZ_NO_TIME to avoid computing localtime on each pickle/unpickle (#27268)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27268

For small pickle/unpickle, we spend a disproportionate amount of time in
time functions - roughly 23% in __tzset() for unpickle case.

We're currently not using the .m_time currently, though we can add this feature
back if it's ever needed.

An alternative would be to -DMINIZ_NO_TIME in compiler_flags, but we would
need to also consistently # define MINIZ_NO_TIME in any .cpp including this .h,
since this # define modifies the struct length in an unfortunate manner.

Test Plan:
buck test mode/dev-nosan caffe2/test/...
Run benchmark:
 buck-out/opt/gen/caffe2/torch/fb/distributed/thriftRpcBackend/test/ThriftRpcAgentBench

Differential Revision: D17724198

fbshipit-source-id: b44a0217b1d9f8ce6c0f24297f59045c7cadf4b1",jjlilley,https://api.github.com/repos/pytorch/pytorch/git/commits/f5df46ce397335c43cb15457e3828a1a66cbbfa9
42e7eb0426190e07339f03d4e6afb61b7ff5ae9c,"Minor readability fixes to C++ documentation (#27338)

Summary:
Changed `yieldings` to `yielding`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27338

Differential Revision: D17758406

Pulled By: yf225

fbshipit-source-id: 1633834a6ad80449c061ebc330ac24f3e42f5506",sribkain,https://api.github.com/repos/pytorch/pytorch/git/commits/42e7eb0426190e07339f03d4e6afb61b7ff5ae9c
c389156fc4872ad4400046bf146abb6b9ec9e075,"move new_zeros to core from THP (#26511)

Summary:
Fix for issue https://github.com/pytorch/pytorch/issues/25831

ezyang can you please have a look?
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26511

Differential Revision: D17763037

Pulled By: ezyang

fbshipit-source-id: 3596c01c4ab421e7785d6055cc813806f840a5c7",v0dro,https://api.github.com/repos/pytorch/pytorch/git/commits/c389156fc4872ad4400046bf146abb6b9ec9e075
9f9c6c09992a7654640b44461911277d784803e4,"From docs of scatter_add_() removed erroneous comment on uniqueness of indices. (#27132)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/27080
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27132

Differential Revision: D17765307

Pulled By: soumith

fbshipit-source-id: b0892ff442f3b49f8e3cdf029e2a08b51fa88f28",pimdh,https://api.github.com/repos/pytorch/pytorch/git/commits/9f9c6c09992a7654640b44461911277d784803e4
16ece1c9daefe58ba56db63d9c930a21411998d9,"Fixed typos and grammatical errors (#27465)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/27443
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27465

Differential Revision: D17810732

Pulled By: pietern

fbshipit-source-id: b8a62dd086a4f4a61c9aa6acfa495cf822995604",iam-abbas,https://api.github.com/repos/pytorch/pytorch/git/commits/16ece1c9daefe58ba56db63d9c930a21411998d9
b96f49885f3c51d4ad0a2f02478aae77cf274f1c,"caffe2 python ideep conv_op test_int8_convolution skip for python 3

Summary: This test was failing in 3.7,  turns out it was ommitted by test director in 3.6 so I added a skip for both versions

Test Plan: unittests is skipped in 3.7 and 3.6 all other tests pass.

Reviewed By: tomdz

Differential Revision: D17820967

fbshipit-source-id: 571f0ec7fe1b0cb50ead4e0d18c00151a701f36a",fried,https://api.github.com/repos/pytorch/pytorch/git/commits/b96f49885f3c51d4ad0a2f02478aae77cf274f1c
28a1806cbc781b3051ddea0c50abdda4b03af138,"C++ API: torch::nn::Softmax (#27446)

Summary:
Add torch::nn::Softmax module support for the C++ API

Related Issue: https://github.com/pytorch/pytorch/issues/25883

Reviewer: yf225
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27446

Differential Revision: D17839546

Pulled By: yf225

fbshipit-source-id: 7c7fb55111b261614de7c3a75fa1019fbde93c67",nuka137,https://api.github.com/repos/pytorch/pytorch/git/commits/28a1806cbc781b3051ddea0c50abdda4b03af138
3246fddfd63472521786099fd4681ba1223bf1b6,"Implement C++ API torch::nn::MultiMarginLoss. (#27424)

Summary:
Hi yf225 , here is the C++ frontend API MultiMarginLoss implementation and tests https://github.com/pytorch/pytorch/issues/27198. Could you review it and tell me if it is okay?

I am not entirely sure I used `c10::optional` correctly, but `options.weight()` resulted in a compilation error, so I went with `options.weight().value()` instead of `value_or()` to follow the logic in `torch.nn._WeightedLoss.register_buffer` (where one can pass a `None` value).

Oh, and are the tests supposed to be skipped or did I do something wrong? I ran `pytest test/test_cpp_api_parity.py -k Loss -v` , and the `L1Loss` test passed but the others were skipped...

Thank you for the review in any case!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27424

Differential Revision: D17839963

Pulled By: yf225

fbshipit-source-id: f4b6012590cf22d56d42751c214df80cce717cb8",CarMiranda,https://api.github.com/repos/pytorch/pytorch/git/commits/3246fddfd63472521786099fd4681ba1223bf1b6
b6fea4f77f4a19eef02b808469ca073b149a8d6f,"Removes floating_dtype decorator from test_torch and test_cuda (#27599)

Summary:
Per title. Also makes a few test_torch tests generic.

This PR removes ~half the floating_dtype decorators. Follow-up will remove the rest.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27599

Differential Revision: D17840056

Pulled By: mruberry

fbshipit-source-id: 428bb5498c452083e3608325e0b548b1d75baf2d",t-kuha,https://api.github.com/repos/pytorch/pytorch/git/commits/b6fea4f77f4a19eef02b808469ca073b149a8d6f
f7d7c4b72fd936181416f55b4f15c05de7076110,"Fix a bug of C++ L-BFGS optimizer (#27606)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/27605: The C++ L-BFGS Optimizer will not work properly if there are one or more registered tensors with no grad in the model:
```
terminate called after throwing an instance of 'c10::Error'
  what():  There were no tensor arguments to this function (e.g., you passed an empty list of Tensors), but no fallback function is registered for schema aten::view.  This usually means that this function requires a non-empty list of Tensors.  Available functions are [CUDATensorId, QuantizedCPUTensorId, VariableTensorId, CPUTensorId, MkldnnCPUTensorId] (lookup_ at /pytorch/aten/src/ATen/core/dispatch/DispatchTable.h:245)
```

Add some `if (!parameter.grad().defined()) {...}` in the ` torch/csrc/api/src/optim/lbfgs.cpp`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27606

Differential Revision: D17866550

Pulled By: yf225

fbshipit-source-id: bcaf0bf75b93c57304856b03d8984c1617ebbfef",auroraustc,https://api.github.com/repos/pytorch/pytorch/git/commits/f7d7c4b72fd936181416f55b4f15c05de7076110
1c2cb6d523a1865914a36ec3c2e805684c41e729,"Edits to ReadMe file (#27808)

Summary:
Grammar edits to the Readme file to make it read better in English
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27808

Differential Revision: D17901414

Pulled By: soumith

fbshipit-source-id: 02e67289dafaf9280cb1c3bb2f37087cd134cc23",jonmoon74,https://api.github.com/repos/pytorch/pytorch/git/commits/1c2cb6d523a1865914a36ec3c2e805684c41e729
19df7e7e84c1328a0661c267835e1c922a6d90f7,"Fix typo

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/27831

Differential Revision: D17904698

Pulled By: soumith

fbshipit-source-id: 3923dd36bc29f0f6e814d299afd8eef224035ccd",foreshadow,https://api.github.com/repos/pytorch/pytorch/git/commits/19df7e7e84c1328a0661c267835e1c922a6d90f7
cc5c34a0d0bb353a700dbc41875c1a3c7906462b,"Add nn::functional::normalize() to C++ Frontend (#27280)

Summary:
Addresses https://github.com/pytorch/pytorch/issues/27048

PR Summary:

Files Added:

_torch/csrc/api/include/torch/nn/options/normalization.h
torch/csrc/api/include/torch/nn/functional/normalization.h_

Files Modified:

_test/cpp/api/functional.cpp
torch/csrc/api/include/torch/nn/functional.h_

 ---

yf225 : I couldn't find a C++ equivalent of gradcheck(), is there such a function or is it sufficient to call .backward() in the test body? I don't think any solutions are checked for the Python tests.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27280

Differential Revision: D17902109

Pulled By: yf225

fbshipit-source-id: 1bce1a88103d0f1848633fec90fde95ea8f3d1ed",gtamba,https://api.github.com/repos/pytorch/pytorch/git/commits/cc5c34a0d0bb353a700dbc41875c1a3c7906462b
498ca083a6595569497540c77ea0c7de074c2a32,"adding IterableDataset to dataset.pyi (#27966)

Summary:
this shall fix https://github.com/pytorch/pytorch/issues/27820
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27966

Differential Revision: D17929633

Pulled By: ezyang

fbshipit-source-id: ff3e0fb7f998b0771183288200c0859eb5f381dd",DuckSoft,https://api.github.com/repos/pytorch/pytorch/git/commits/498ca083a6595569497540c77ea0c7de074c2a32
70838ad08b90dc01380bf25f26efa5cfdfe4f0f4,"Fix typo in TransformerEncoder and TransformerEncoderLayer documentation (#26230)

Summary:
Fixes a few small typos in the documentation, changing ""endocder"" to ""encoder"" and ""sequnce"" to ""sequence""
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26230

Differential Revision: D17910820

Pulled By: vincentqb

fbshipit-source-id: 58c63f8dbbd8e2079201d4485a0d4ef323ecfb49",ljjb,https://api.github.com/repos/pytorch/pytorch/git/commits/70838ad08b90dc01380bf25f26efa5cfdfe4f0f4
3397d41b8af2824495faaefc649c3c7e47f03bfb,"Wrapping namespace Reduction in namespace at (#26606) (#27422)

Summary:
1) Wrapped namespace `Reduction` in namespace `at`
2) Prefixed `at::` wherever `Reduction::` is used
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27422

Differential Revision: D17913759

Pulled By: yf225

fbshipit-source-id: 8f00ca01cad2e7f673d316b128abf59c026e216c",divyanshsinghvi,https://api.github.com/repos/pytorch/pytorch/git/commits/3397d41b8af2824495faaefc649c3c7e47f03bfb
5797f5dd27a5a2734f197cecb0e57d71e30dc944,"Update 'einsum' docstring to conform to PEP-484 (#27563)

Summary:
[PEP-484](https://www.python.org/dev/peps/pep-0484/#arbitrary-argument-lists-and-default-argument-values) specifies that arbitrary argument lists, here `*operands`, should be annotated with the type of the single arguments, i.e. not indicating that the whole thing is wrapped into a `list` (which is a Python internal anyway). The previous docstring caused problems with type checkers for IDEs such as PyCharm ([see here](https://youtrack.jetbrains.com/issue/PY-38035)).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27563

Differential Revision: D17904748

Pulled By: soumith

fbshipit-source-id: 0a7fcbbb12e388e6fc40d48bf533652a96024757",Dominik1123,https://api.github.com/repos/pytorch/pytorch/git/commits/5797f5dd27a5a2734f197cecb0e57d71e30dc944
f38beff80019daba6e8016bd3bd7ba6056412d83,"Add nn.Bilinear to C++ Frontend (#26082)

Summary:
Adds support for the Bilinear layer to the C++ frontend
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26082

Differential Revision: D17954148

Pulled By: yf225

fbshipit-source-id: 5e746bdea29b00e25969cd7a22044b8059b53687",MJ10,https://api.github.com/repos/pytorch/pytorch/git/commits/f38beff80019daba6e8016bd3bd7ba6056412d83
97b39a296f83cc8afa1281abff7acdb292eadd67,"Fix error report highlight for unmatched type annotation (#27195)

Summary:
This PR fixes https://github.com/pytorch/pytorch/issues/25801 (see there for my verbose analysis).

As an example, for the following code:

```
import torch

torch.jit.script
def f1(x):
    # type: (int, int) -> None
    pass
```

this PR will change error message from this:

```
RuntimeError:
Number of type annotations (2) did not match the number of function parameters (1):
# type: (int, int) -> None
```

to this:

```
RuntimeError:
Number of type annotations (2) did not match the number of function parameters (1):
at __scratch__/example.py:4:0
torch.jit.script
def f1(x):
~~~~~~~~ <--- HERE
    # type: (int, int) -> None
    pass
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27195

Differential Revision: D17910902

Pulled By: driazati

fbshipit-source-id: af5c6353069d005752d6c7f0bd6a0c6db8437e55",hi-ogawa,https://api.github.com/repos/pytorch/pytorch/git/commits/97b39a296f83cc8afa1281abff7acdb292eadd67
e1be08fcf5b48511b8b4546ef552d8fb472b677e,"out-variant for torch.batch_norm_elemt (#27621)

Summary:
Following dicussion with ezyang in https://github.com/pytorch/pytorch/issues/26288
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27621

Differential Revision: D17978858

Pulled By: ezyang

fbshipit-source-id: f843b691a67f1dc48b87ed6a633007d193150cf7",vadimkantorov,https://api.github.com/repos/pytorch/pytorch/git/commits/e1be08fcf5b48511b8b4546ef552d8fb472b677e
c36552c4cbc8fd9c9346d213b11654c126615d5d,"Fixing dispatch error in windows debug builds (#24360)

Summary:
nullptr initialization values for dispatch pointers were overwriting values set using the REGISTER_DISPATCH macro.

Relevant issue: https://github.com/pytorch/pytorch/issues/22681
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24360

Differential Revision: D17952241

Pulled By: ezyang

fbshipit-source-id: 4bf86dc24153e504bbeacb526c58fd8230bb972a",rje,https://api.github.com/repos/pytorch/pytorch/git/commits/c36552c4cbc8fd9c9346d213b11654c126615d5d
bd6f9e1d6c9145a2ed7df7916caab5296c15b744,"torch.nn.functional.gumbel_softmax #27078 (#28121)

Summary:
**Comments:**
* Grad check from https://github.com/pytorch/pytorch/blob/848d1ba13a3f77bb6f14656f2d84e2228e37a8ef/test/test_nn.py#L8898 not added
* Double data type as seen in     https://github.com/pytorch/pytorch/blob/848d1ba13a3f77bb6f14656f2d84e2228e37a8ef/test/test_nn.py#L8916 not tested

**Issue:**
https://github.com/pytorch/pytorch/issues/27078
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28121

Differential Revision: D18008515

Pulled By: yf225

fbshipit-source-id: 9363fe9430df0f2bfd337cc788b11ac93adaa360",Naresh1318,https://api.github.com/repos/pytorch/pytorch/git/commits/bd6f9e1d6c9145a2ed7df7916caab5296c15b744
109c467559df030d41ec162ae1157010bbed4012,"Add generate-wrapper.py with its generated wrapper files. (#28285)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28285

1. Add generate-wrapper.py to route different code path base on different platform.
2. Append all the generated wrapper files by running generate-wrapper.py, and they will be used in the next diff for buck build targets.
ghstack-source-id: 92071247

Test Plan: Will be tested in the next diff when these files are linked.

Reviewed By: dreiss

Differential Revision: D17967339

fbshipit-source-id: 8af88af9e8d2e4640bcf9d29c4daf10666aa88dc",xcheng16,https://api.github.com/repos/pytorch/pytorch/git/commits/109c467559df030d41ec162ae1157010bbed4012
1c53a74e266ec90c57f32f041ff7747b86246f47,"Fixed behavior of div_factor parameter in optim.lr_scheduler.OneCycleLR (#28217)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/28216
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28217

Differential Revision: D18070759

Pulled By: vincentqb

fbshipit-source-id: ed032190c0e3eab834fc9a8f408b75b56f0f35ec",timothyman,https://api.github.com/repos/pytorch/pytorch/git/commits/1c53a74e266ec90c57f32f041ff7747b86246f47
d081de67cf6d89ec53f30f0b60d914f86b3f655c,"fix the document of kaiming initialization (#25638)

Summary:
Based on https://github.com/pytorch/pytorch/issues/25549, I modified the comments for kaiming initialization in torch.nn.init.py
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25638

Differential Revision: D17915392

Pulled By: vincentqb

fbshipit-source-id: 40f60c65d14790696ec03d7d91c764875efd6cf1",robert780612,https://api.github.com/repos/pytorch/pytorch/git/commits/d081de67cf6d89ec53f30f0b60d914f86b3f655c
61d40b80d395ee533bf11800d0c966742d0272ae,"static initialization order with mutex (#28243)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28243

When building static libs version of pytorch 1.3 on windows (msvc v141), program crashes with bad memory reference because `fusion_backends_lock_` has not been initialized yet.

Test Plan:
sandcastle green,
tested locally on MSVC static builds that this fixes initialization.

Differential Revision: D17985919

fbshipit-source-id: ebd6178dedf5147d01c2c1754a0942a1bbbc7e34",EscapeZero,https://api.github.com/repos/pytorch/pytorch/git/commits/61d40b80d395ee533bf11800d0c966742d0272ae
dc17a2ecc5bf79b386b289ee823c1a281a8e37fb,"Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/28433

Differential Revision: D18138240

Pulled By: anjali411

fbshipit-source-id: 314e5902f103be1feb4cacde47c90204b3d353cc",anjali411,https://api.github.com/repos/pytorch/pytorch/git/commits/dc17a2ecc5bf79b386b289ee823c1a281a8e37fb
110a9317523696740336db8c3a712ac1699cb522,"Change from HTTP to HTTPS

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/28333

Differential Revision: D18143824

Pulled By: soumith

fbshipit-source-id: 613fd2219814addc850c3b9fe7ebfd8510a5e5c8",hyeongukryu,https://api.github.com/repos/pytorch/pytorch/git/commits/110a9317523696740336db8c3a712ac1699cb522
440b1920782ef09a9f1d8208e097c794376cfbd3,"Type hints: Return `Iterator` instead of `Iterable` from `__iter__` (#27445)

Summary:
`__iter__` methods are supposed to return iterators (https://docs.python.org/3/reference/datamodel.html#object.__iter__), but some of them are typed to return iterables, which is too general. This results in error messages such as `Iterable[Module[Any]]"" has no attribute ""__next__""` from Mypy. Technically this should also have caused a type error [here](https://github.com/pytorch/pytorch/blob/8f7020bbdbb5537bf1954cd252523cb17ab879b1/torch/nn/modules/container.py#L115), but due to a bug in Mypy type checking isn't working correctly in untyped methods (this will be fixed in the next release though: https://github.com/python/mypy/pull/7530).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27445

Reviewed By: lerks

Differential Revision: D18113966

Pulled By: fmassa

fbshipit-source-id: c6261ac866f86df4328e6d2fdfca0625aa2d2492",henribru,https://api.github.com/repos/pytorch/pytorch/git/commits/440b1920782ef09a9f1d8208e097c794376cfbd3
45dab5615374ce33aab371a9c08af56da79d82ea,"adding python all_gather coalesced functionality and testing. (#28634)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28634

caveat 1: this only works in sync mode.
caveat 2: this is going to go away and be replaced by c++ implementation

Test Plan: buck test caffe2/test:distributed_gloo -- test_all_gather_coalesced

Reviewed By: mrshenli

Differential Revision: D18123422

fbshipit-source-id: cfb9950d5d54c6181a5240e7cc9fed88ed47f5d9",agolynski,https://api.github.com/repos/pytorch/pytorch/git/commits/45dab5615374ce33aab371a9c08af56da79d82ea
82f31e02a396fe1aae1a0140f20feb2e0f6aeffe,"Remove the redundant calculation of derivative of power function (#28651)

Summary:
Hi, I notice that the pytorch faced the the issue as HIPS/autograd#541 .
I try to solve it, hope it can help.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28651

Reviewed By: gchanan

Differential Revision: D18137163

Pulled By: albanD

fbshipit-source-id: 888bef65c72c4c15c2acdd4b13d5041008b1354e",titaneric,https://api.github.com/repos/pytorch/pytorch/git/commits/82f31e02a396fe1aae1a0140f20feb2e0f6aeffe
0c7537c40939f7682c179813a4b7a50020f08152,"Fix obviously-broken .clang-tidy files (#28547)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28547

Pull Request resolved: https://github.com/pytorch/glow/pull/3672

See D18090864 for more background.  The issue i addressed there is more widespread, so i'm fixing all the other `.clang-tidy` files clearly not working as intended.

Perhaps this means it's time to lint the linter config :-)

Test Plan:
Here's the resulting output for `~/fbsource/fbcode/third-party-buck/platform007/build/llvm-fb/bin/clang-tidy` related to each file touched:

`fbcode/admarket/intent/.clang-tidy`: P119723794
`fbcode/caffe2/.clang-tidy`: P119723978
`fbcode/glow/glow/.clang-tidy`: P119724081
`fbcode/ice_palace/.clang-tidy`: P119724774
`fbcode/unified_graph/aggregator/.clang-tidy`: P119724375
`xplat/caffe2/.clang-tidy`: P119724464
`xplat/mcfcpp/.clang-tidy`:
```
[billfarner@devvm2187.ftw3 ~/fbsource/xplat/mcfcpp]  ~/fbsource/fbcode/third-party-buck/platform007/build/llvm-fb/bin/clang-tidy -explain-config
'readability-identifier-naming' is enabled in the /home/billfarner/fbsource/xplat/mcfcpp/.clang-tidy.
```

`xplat/wa-msys/mcfcpp/.clang-tidy`:
```
[billfarner@devvm2187.ftw3 ~/fbsource/xplat/wa-msys/mcfcpp]  ~/fbsource/fbcode/third-party-buck/platform007/build/llvm-fb/bin/clang-tidy -explain-config
'readability-identifier-naming' is enabled in the /home/billfarner/fbsource/xplat/wa-msys/mcfcpp/.clang-tidy.
```

Reviewed By: soumith

Differential Revision: D18092684

fbshipit-source-id: 951307d125c0346322cb2c636c0300004a48d7a9",wfarner,https://api.github.com/repos/pytorch/pytorch/git/commits/0c7537c40939f7682c179813a4b7a50020f08152
dfe7b25eafd88130306991299574145f3333106e,"Add nn::Flatten to C++ Frontend (#28072)

Summary:
Adds torch::nn::Flatten module support for the C++ API.

Issue: https://github.com/pytorch/pytorch/issues/25883

Reviewer: yf225
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28072

Differential Revision: D18202778

Pulled By: yf225

fbshipit-source-id: 43345dcbdf2f50d75746bf9a0ba293b84df275ab",mrsalehi,https://api.github.com/repos/pytorch/pytorch/git/commits/dfe7b25eafd88130306991299574145f3333106e
50fd20b64a97cc75a94f95c5b08cbf9d0580340c,"fix bug on setup.py to include header files on caffe2/utils/math (#28869)

Summary:
This problem is from issue [https://github.com/pytorch/pytorch/issues/28753](https://github.com/pytorch/pytorch/issues/28753)

The header files on directories`math` and `threadpool` should be included on the built package because they are included on the other header files, such as on file `torch/include/caffe2/utils/math.h`
```
#include ""caffe2/core/common.h""
#include ""caffe2/core/types.h""
#include ""caffe2/utils/math/broadcast.h""
#include ""caffe2/utils/math/elementwise.h""
#include ""caffe2/utils/math/reduce.h""
#include ""caffe2/utils/math/transpose.h""
#include ""caffe2/utils/math/utils.h""
```
But the `setup.py` doesn't include the header files on `master` branch. The header files on `utils` directory of a built `torch` package are the following:
```
> ls include/caffe2/utils
bench_utils.h  conversions.h  eigen_utils.h    map_utils.h    murmur_hash3.h   proto_wrap.h      smart_tensor_printer.h
cast.h         cpuid.h        filler.h         math-detail.h  proto_convert.h  signal_handler.h  string_utils.h
cblas.h        cpu_neon.h     fixed_divisor.h  math.h         proto_utils.h    simple_queue.h    zmq_helper.h
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28869

Differential Revision: D18226319

Pulled By: soumith

fbshipit-source-id: 51575ddc559181c069b3324aa9b2d1669310ba25",qzhong0605,https://api.github.com/repos/pytorch/pytorch/git/commits/50fd20b64a97cc75a94f95c5b08cbf9d0580340c
a465b033fdb35e746aba3df5debcacc26c44555c,"Local response norm (#28759)

Summary:
Implemented LocalResponseNorm and some initial tests for modules and functional. Reference https://github.com/pytorch/pytorch/issues/25883
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28759

Differential Revision: D18219745

Pulled By: yf225

fbshipit-source-id: e6aad568a8b1e81f54752decaefd4f9044029da9",mansoorcheema,https://api.github.com/repos/pytorch/pytorch/git/commits/a465b033fdb35e746aba3df5debcacc26c44555c
00bd9eae33058f1ef5735eb178dba242594c6c82,"Fix typo in `Dataset` and `IterableDataset` docs (#28960)

Summary:
Replaced ""overrite"" with ""overwrite"".
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28960

Differential Revision: D18246411

Pulled By: soumith

fbshipit-source-id: dc0979a44b7c621a316823061760e0358c227727",donald-pinckney,https://api.github.com/repos/pytorch/pytorch/git/commits/00bd9eae33058f1ef5735eb178dba242594c6c82
aa30176c6841aa9ea021e1620fe912c0e89ecae7,"Add C++ API clip_grad_value_ for nn:utils (#28736)

Summary:
Adds C++ API clip_grad_value_ for torch::nn:utils module.
Also, fix the for indent level error in the original test/test_nn.py.

Issue: https://github.com/pytorch/pytorch/issues/25883

Reviewer: yf225
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28736

Differential Revision: D18263807

Pulled By: yf225

fbshipit-source-id: 29282450bd2099df16925e1d0edd3d933f6eeb9b",jokerkeny,https://api.github.com/repos/pytorch/pytorch/git/commits/aa30176c6841aa9ea021e1620fe912c0e89ecae7
cd3ed4db768c27e1709551711a9907bc27092965,"Update README.md (#28971)

Summary:
Fixed some grammar.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28971

Differential Revision: D18265791

Pulled By: soumith

fbshipit-source-id: 778ab3e8a31f5f520a048c089c719c618427eaa6",aBorovtsov1994,https://api.github.com/repos/pytorch/pytorch/git/commits/cd3ed4db768c27e1709551711a9907bc27092965
31c932d9abb9e0cfb64da0286d0d7438a2999234,"fixed replicate typo in torch/nn/parallel/__init__.pyi (#29005)

Summary:
Fix for https://github.com/pytorch/pytorch/issues/29004
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29005

Differential Revision: D18264637

Pulled By: pbelevich

fbshipit-source-id: 03013f668235deca35a58f70732111b53d792de5",HanGuo97,https://api.github.com/repos/pytorch/pytorch/git/commits/31c932d9abb9e0cfb64da0286d0d7438a2999234
9e314f557f483ad7d3993ed4c420bfbf5f2d1c5e,"Fix for torch.save not saving source files (#28965)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28965

Fixed the reference to correct object

Test Plan:
Added new unit test test_serialization_save_warnings in test_torch
    Verified by running the test_torch tests

Imported from OSS

Differential Revision: D18306797

fbshipit-source-id: bbdc7a1aa59a395fcbb736bcc7c3f96db45454d3",chauhang,https://api.github.com/repos/pytorch/pytorch/git/commits/9e314f557f483ad7d3993ed4c420bfbf5f2d1c5e
7434da2c3f49b8e01dfcd2d504dba617a300c41b,"value assigned but never used in _recursive.py (#29181)

Summary:
# Description
I'm new to this project just wanted to start with small bug fixes. I found some unused local variables and I've removed them in this pr.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29181

Differential Revision: D18319893

Pulled By: suo

fbshipit-source-id: e4f9f13b6db2ca213015569deb12d3fd9beb74a8",zak-hassan-hs,https://api.github.com/repos/pytorch/pytorch/git/commits/7434da2c3f49b8e01dfcd2d504dba617a300c41b
fff4f16e45c55151f977224d5d195d6e413af938,"Clean up file opening for serialization (#29221)

Summary:
Stacked PRs
 * https://github.com/pytorch/pytorch/issues/29232 - Add zipfile serialization
 * https://github.com/pytorch/pytorch/issues/29228 - Expose miniz to Python
 * **https://github.com/pytorch/pytorch/issues/29221 - Clean up file opening for serialization**

This is a small refactor to get things started for zipfile-based serialization
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29221

Differential Revision: D18330932

Pulled By: driazati

fbshipit-source-id: ce91542faf987ae5aa6dfd322e633a0c7335e678",invalid-email-address,https://api.github.com/repos/pytorch/pytorch/git/commits/fff4f16e45c55151f977224d5d195d6e413af938
8e8a5e0664af2aaeb0c4aa0f243271bbbdf8455a,"Pruning Functionality (#24076)

Summary:
Provides implementation for feature request issue https://github.com/pytorch/pytorch/issues/20402.

Adds pruning functionalities (structured and unstructured, local and global, as well as pruning from user-provided mask).

Associated tutorial here: https://github.com/pytorch/tutorials/pull/605

cc: soumith
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24076

Differential Revision: D18400431

Pulled By: mickypaganini

fbshipit-source-id: a97bd6ca61f8600ae411da9ff6533c232aae1a51",mickypaganini,https://api.github.com/repos/pytorch/pytorch/git/commits/8e8a5e0664af2aaeb0c4aa0f243271bbbdf8455a
7f485121a6416541649ab9871775ad2a8b613d78,"Avoid MSVC _cvtsh_ss() workaround with clang-cl (#29726)

Summary:
We (me fnabulsi bmcdb) have a handful of fixes used locally to build and run with clang-cl. I am aware of https://github.com/pytorch/pytorch/issues/8784 but it has not been touched in almost a year.

It may be more practical to upstream the non-controversial fixes piecewise. For example, this one.

Here, the dummy version of `_cvtsh_ss` for MSVC is not required (and hence causes conflicts) when using clang-cl so can be #ifdef'd out.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29726

Differential Revision: D18478120

Pulled By: ezyang

fbshipit-source-id: cdcd94251e68347446f2ad1ac5a0e71089f7d0ab",jdonald,https://api.github.com/repos/pytorch/pytorch/git/commits/7f485121a6416541649ab9871775ad2a8b613d78
a9c719ba825ebbe3dacf12db938df4c23ad9fba1,"Set TORCH_CXX_FLAGS in minimal example (#29890)

Summary:
To avoid ABI issue

EDIT: After this PR, the example CMakeLists.txt will always use the `-D_GLIBCXX_USE_CXX11_ABI` value set in `share/cmake/Torch/TorchConfig.cmake`, regardless of the `-D_GLIBCXX_USE_CXX11_ABI` value passed to the `cmake` command by the user.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29890

Differential Revision: D18531391

Pulled By: yf225

fbshipit-source-id: 2db78ae7a33a4088b579e81c60b9a74861f1ccde",take-cheeze,https://api.github.com/repos/pytorch/pytorch/git/commits/a9c719ba825ebbe3dacf12db938df4c23ad9fba1
94016b153a4130814d95181014f635b33eb117df,"Fix typo in documentation

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29755

Differential Revision: D18529963

Pulled By: ezyang

fbshipit-source-id: 8d9100f00c46238fa3210944864b1d178717499f",MrTsepa,https://api.github.com/repos/pytorch/pytorch/git/commits/94016b153a4130814d95181014f635b33eb117df
e88d096321e5214c2ffaa4ae2112023ce8b7f697,"C++/Python API Parity: add AlphaDropout (#28424)

Summary:
- add `AlphaDropoutImpl` to `modules/dropout.h` and `modules/dropout.cpp`
 - add `functional/dropout.h` containing the `alpha_dropout` function
 - include `functional/dropout.h` in `nn/functional.h`
 - add functional and module tests
-  related issue https://github.com/pytorch/pytorch/issues/25883
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28424

Differential Revision: D18589162

Pulled By: yf225

fbshipit-source-id: c85734e02431a6c052515e26b11ca30ad7303644",Suyash458,https://api.github.com/repos/pytorch/pytorch/git/commits/e88d096321e5214c2ffaa4ae2112023ce8b7f697
6e4c23b02f8607011454eb1131d11d1030892dff,"Add RPC internal helper that overrides the default pickler. (#30185)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30185

To enable share_memory over RPC, add an internal helper that overrides the default RPC pickler.
Replace D18598974
ghstack-source-id: 94299660

Test Plan:
`python test/test_rpc_spawn RpcTestWithSpawn.test_use_rpc_pickler`

`buck test mode/dev-nosan //caffe2/test:rpc_spawn -- test_use_rpc_pickler`

Reviewed By: mrshenli

Differential Revision: D18621372

fbshipit-source-id: c680ef711b2c42524c47a5266e911fa8e0cd45ae",zzzwen,https://api.github.com/repos/pytorch/pytorch/git/commits/6e4c23b02f8607011454eb1131d11d1030892dff
f4b9690f2d609094a6d952a9c8d247447a6e1b2d,"Use pybind11::gil_scoped_* functions instead of AutoGIL/AutoNoGIL (#29095)

Summary:
Given that pybind11 implements these gil functions, I don't think it makes sense for Pytorch to have its own bespoke versions.

Fixes https://github.com/pytorch/pytorch/issues/29065
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29095

Differential Revision: D18301806

Pulled By: ezyang

fbshipit-source-id: 03da6a26c41ee65aaadf7b67b9f0b14d2def2a5a",alanhdu,https://api.github.com/repos/pytorch/pytorch/git/commits/f4b9690f2d609094a6d952a9c8d247447a6e1b2d
0c04763d59165078c7ffc6c1c5ea404f4ed32b45,"Changes to get inlined graph and proper names after JIT updates (#30244)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30244

This makes several small changes to the tensorboard graph parsing methods to address the recent changes to the PyTorch JIT trace/graph.
- Inline graph to get information for all nodes
- Assign and propagate scope names to GetAttr nodes
- Prune all useless GetAttr nodes (any with a ClassType output type - tensors and primitives are kept)
- Create output nodes so output tensor shape can be examined

Reviewed By: sanekmelnikov

Differential Revision: D18556323

fbshipit-source-id: b73a809bacfa554c3fe9c4ae3563525f57539874",J0Nreynolds,https://api.github.com/repos/pytorch/pytorch/git/commits/0c04763d59165078c7ffc6c1c5ea404f4ed32b45
c7f988b8c61e4e7c78b3e9f4d03eeebf8caf6bad,"transport open registration (#30167)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30167

Pull Request resolved: https://github.com/pytorch/pytorch/pull/29164

- Created GlooDeviceFactory to hide device creation details
- Added transport option while on Python interface

The reason of making the factory class is to make it easier to extend gloo transport in the future

Test Plan: Imported from OSS

Reviewed By: satgera, d4l3k

Differential Revision: D18596527

fbshipit-source-id: e8114162ee8d841c0e0769315b48356b37d6ca0a",jiayisuse,https://api.github.com/repos/pytorch/pytorch/git/commits/c7f988b8c61e4e7c78b3e9f4d03eeebf8caf6bad
7570b2798a9578179b91df9ceadf888e1d2b7b3d,"updating citation (#30267)

Summary:
NIPS -> NeurIPS
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30267

Differential Revision: D18672928

Pulled By: soumith

fbshipit-source-id: c20f26a0547f94ff39f8ee40e5f0ccc5fcc814af",juliakreutzer,https://api.github.com/repos/pytorch/pytorch/git/commits/7570b2798a9578179b91df9ceadf888e1d2b7b3d
4eff2f2007f1c963e4596d9aa901fff15d0c3770,"Fix missing closing quotes in docs

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30448

Differential Revision: D18711396

Pulled By: zou3519

fbshipit-source-id: 6e35e0779716185791273eedca7a93667a6cda90",bryant1410,https://api.github.com/repos/pytorch/pytorch/git/commits/4eff2f2007f1c963e4596d9aa901fff15d0c3770
8ee61e0be47d02a363f277a41b6d2baae9adb15c,"Fix CPU_INTEL flag error on windows (#30564)

Summary:
${CMAKE_HOST_SYSTEM_PROCESSOR} get processor name by `uname -p` on linux and `%PROCESSOR_ARCHITECTURE%` on windows
1. %PROCESSOR_ARCHITECTURE% has value in (AMD64|IA64|ARM64) for 64-bit processor, and (x86) for 32-bit processor
2. `uname -p` has value like ""(x86_64|i[3-6]+86)""
We cannot tell intel cpu from other cpus by ${CMAKE_HOST_SYSTEM_PROCESSOR}. It is the architecture, not provider.
i. e. Intel CPU i7-9700K CPU on windows get ""AMD64""

reference:
[MSDN](https://docs.microsoft.com/zh-cn/windows/win32/winprog64/wow64-implementation-details?redirectedfrom=MSDN)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30564

Differential Revision: D18763031

Pulled By: ezyang

fbshipit-source-id: 11ae20e66b4b89bde1dcf4df6177606a3374c671",liooil,https://api.github.com/repos/pytorch/pytorch/git/commits/8ee61e0be47d02a363f277a41b6d2baae9adb15c
9e3d19412bf9cdfde0db7531aada3d4a859eccf0,"Disable implicit conversion warning (#30529)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30529

We started to see build failures for multiple services with top-of-trunk LLVM compiler. The failures point to a warning that was treated as error for implicit conversion from long to double. Per discussion on D18642524, I'm disabling this warning from the containing TARGET file. T58053069 opened for code owner to track this - a proper source code fix and more unit test is needed.

Test Plan: local build, sandcastle

Reviewed By: smessmer

Differential Revision: D18668396

fbshipit-source-id: 28c0ff3258c5ba3afd41a0053f9fe1b356a496a8",WenleiHe,https://api.github.com/repos/pytorch/pytorch/git/commits/9e3d19412bf9cdfde0db7531aada3d4a859eccf0
e7fe64f6a65cd427e503491f192c14476e18033b,"Fix typos (#30606)

Summary:
Should be non-semantic.

Uses https://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings/For_machines to find likely typos.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30606

Differential Revision: D18763028

Pulled By: mrshenli

fbshipit-source-id: 896515a2156d062653408852e6c04b429fc5955c",bwignall,https://api.github.com/repos/pytorch/pytorch/git/commits/e7fe64f6a65cd427e503491f192c14476e18033b
1189595875310c9685932f59d66bbe5198b5a9fc,"Fix Tensor.argsort -> torch.argsort documentation link

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30464

Differential Revision: D18717657

Pulled By: zou3519

fbshipit-source-id: 9894f63c6cb1b5311117441e78805230d1bc09f3",willprice,https://api.github.com/repos/pytorch/pytorch/git/commits/1189595875310c9685932f59d66bbe5198b5a9fc
1d7b40f1c416f18c50c789f2e4cc88fa1c8c14b4,"Fix reading `__cuda_array_interface__` without strides (#24947)

Summary:
When converting a contiguous CuPy ndarray to Tensor via `__cuda_array_interface__`, an error occurs due to incorrect handling of default strides. This PR fixes this problem. It makes `torch.tensor(cupy_ndarray)` works for contiguous inputs.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24947

Differential Revision: D18838986

Pulled By: ezyang

fbshipit-source-id: 2d827578f54ea22836037fe9ea8735b99f2efb42",beam2d,https://api.github.com/repos/pytorch/pytorch/git/commits/1d7b40f1c416f18c50c789f2e4cc88fa1c8c14b4
5edfe9cb8095a1f9ee141f4d024e67a0aea5aba9,"add torch.square (#30719)

Summary:
fixes https://github.com/pytorch/pytorch/issues/30524
This adds an new operator `torch.square` to PyTorch

I think it is ready for the first-time review now albanD
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30719

Differential Revision: D18909268

Pulled By: albanD

fbshipit-source-id: 5626c445d8db20471a56fc1d7a3490e77812662b",TH3CHARLie,https://api.github.com/repos/pytorch/pytorch/git/commits/5edfe9cb8095a1f9ee141f4d024e67a0aea5aba9
4a751dfc2034d02a8ca1b9baef2af062afe3b6a7,"optimize MulGradient for common shapes (#19705)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19705

Optimizing for a case when there's a consecutive dims that are not broadcasted followed by another consecutive dims that are broadcasted.
For example, MulGradient([""dC"", ""A"", ""B""], [""dA"", ""dB""], broadcast=True, axis=0) where A.shape == dC.shape == [9508, 80] and B.shape == [80] .

Test Plan:
In SKL T6,

Running mul_gradient_benchmark without this optimization
Operator #0 (dA, MulGradient) 11.9119 ms/iter

After this optimization,
Operator #0 (dA, MulGradient) 0.672759 ms/iter

Need to land D15291800 before to fix the unit test error

Reviewed By: dmudiger

Differential Revision: D15075415

fbshipit-source-id: 0f97be17cf8f1dacbafa34cd637fb8bc1c5e5387",yuchenhao,https://api.github.com/repos/pytorch/pytorch/git/commits/4a751dfc2034d02a8ca1b9baef2af062afe3b6a7
0414463007cecbc987ec376e95639073d26d7d2d,"doc fix for max method: a warning about different behaviour on CPU and GPU (#31115)

Summary:
Fixes [30708](https://github.com/pytorch/pytorch/issues/30708),
Adds warning regarding different behaviour of the method depending on device type.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31115

Differential Revision: D18937365

Pulled By: zou3519

fbshipit-source-id: 7c731dd80f8b371de08d7fdfcc2196be15a593e1",nikitaved,https://api.github.com/repos/pytorch/pytorch/git/commits/0414463007cecbc987ec376e95639073d26d7d2d
f30b14dead12776981bb0a9449deb52b2fd9f17d,"Fix handling of type comments in body (#30590)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/30477. Any type comment after `# type: (...) -> ` is ignored.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30590

Differential Revision: D18887351

Pulled By: driazati

fbshipit-source-id: 162c652f6d7610d14609bbcb25aaa27cdd947a76",stante,https://api.github.com/repos/pytorch/pytorch/git/commits/f30b14dead12776981bb0a9449deb52b2fd9f17d
3694749cd1b2bfbb4537ea767fdbbe925b815923,"Detect dill version in torch.save/load (#30985)

Summary:
Fix for issue https://github.com/pytorch/pytorch/issues/28313
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30985

Differential Revision: D19142947

Pulled By: zou3519

fbshipit-source-id: 10e3a182a99e80ca8c9c8328b6f8764b27d78eb3",kurtamohler,https://api.github.com/repos/pytorch/pytorch/git/commits/3694749cd1b2bfbb4537ea767fdbbe925b815923
91eb7c26cd862d2db6c8486cef28125a86869224,"Fix Typos

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31630

Differential Revision: D19233162

Pulled By: zou3519

fbshipit-source-id: c2716a2df2b2ccfeda7718b484e9605515ecdf01",avinashsai,https://api.github.com/repos/pytorch/pytorch/git/commits/91eb7c26cd862d2db6c8486cef28125a86869224
3b7916fccdb32a6feb08b445349220f898e4bb23,"Modify the order of arguments position of torch.std and torch.std_mean in doc (#31677)

Summary:
Change log:

- [x] Change the order of arguments position of torch.std and torch.std_mean in doc.
- [x] Correct a spelling mistake of torch.std_mean in doc.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31677

Differential Revision: D19247372

Pulled By: ngimel

fbshipit-source-id: 8685f5207c39be524cdc81250430beac9d75f330",Qoo95,https://api.github.com/repos/pytorch/pytorch/git/commits/3b7916fccdb32a6feb08b445349220f898e4bb23
d770fbc1d2420419551aeb9d3e9e4311ce041441,"Some modifications to improve readability (#31352)

Summary:
In the long string, formalstring thinks it is good to have a name.

When using dict, literal is better for readability and faster than dict constructor.

I always appreciate your efforts in creating the world's best frameworks.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31352

Differential Revision: D19191967

Pulled By: ngimel

fbshipit-source-id: 21f063b163b67de8cf9761a4db5991f74318e991",marload,https://api.github.com/repos/pytorch/pytorch/git/commits/d770fbc1d2420419551aeb9d3e9e4311ce041441
3f0b330736a2af1331e49e8e12e880dffd1e5974,"corrected keyword argument name in docs for Tensor.scatter (#31617)

Summary:
See https://github.com/pytorch/pytorch/issues/31601
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31617

Differential Revision: D19268872

Pulled By: mruberry

fbshipit-source-id: 52f0213f4aab991fd549b7623556a2ced61631a6",ShnitzelKiller,https://api.github.com/repos/pytorch/pytorch/git/commits/3f0b330736a2af1331e49e8e12e880dffd1e5974
1ba1799a66b20f17657a132eab111d002b3356f3,"C++ added 3rd arg of false to BatchNorm/InstanceNorm register_parameter â€¦ (#31873)

Summary:
Fix for issue https://github.com/pytorch/pytorch/issues/31680
C++ BatchNorm & InstanceNorm attempt to register undefined tensors when affine is false.

Fixes https://github.com/pytorch/pytorch/issues/31680
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31873

Differential Revision: D19287087

Pulled By: yf225

fbshipit-source-id: 0d57f10c49083386919b703d72b520a73a8e9e7f",meganset,https://api.github.com/repos/pytorch/pytorch/git/commits/1ba1799a66b20f17657a132eab111d002b3356f3
1f2b6d632a95d4aa06b12e1b75168075d4d6bd11,"Refactor tests in pytorch's test/dist_autograd_test.py file (#31803)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31803

Refactored the following fairly similar functions:
  1. `test_context_cleanup_tensor_with_grad`
  2. `test_context_cleanup_tensor_no_grad`
  3. `test_context_cleanup_no_tensors`
by creating a helper function `context_cleanup_test_helper` that can be invoked with the appropriate arguments.

Test Plan: Verified by running tests.

Differential Revision: D19269246

fbshipit-source-id: bfb42b078ad56b97ceeecf0d68b4169768c2c453",fkhan1337,https://api.github.com/repos/pytorch/pytorch/git/commits/1f2b6d632a95d4aa06b12e1b75168075d4d6bd11
346a3491110252c3c90b559701c1767eca20c904,"Update all instances of 1.4.0 -> 1.5.0 (#31785)

Summary:
Done with:

```
â¯ sed -i 's/1\.4\.0/1.5.0/g' $(find -type f -not -path ""./third_party/*"")
```

This was previously done in separate commits, but it would be beneficial to bump all included projects within this repository at the same time.

Old bumps for reference:
* [iOS]Update Cocoapods to 1.4.0: https://github.com/pytorch/pytorch/pull/30326
* [android] Change nightly builds version to 1.4.0-SNAPSHOT: https://github.com/pytorch/pytorch/pull/27381
* Roll master to 1.4.0: https://github.com/pytorch/pytorch/pull/27374

Signed-off-by: Eli Uriegas <eliuriegas@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31785

Differential Revision: D19277925

Pulled By: seemethere

fbshipit-source-id: f72ad082f0566004858c9374879f4b1bee169f9c",seemethere,https://api.github.com/repos/pytorch/pytorch/git/commits/346a3491110252c3c90b559701c1767eca20c904
20c5dd59bda182f526e2302623db89cad934c235,"Add stub for transformer.py and MultiheadAttention Class. (#28396)

Summary:
Add stub for `transformer.py` and `class MultiheadAttention`. Add import for `transformer.py`  and `class MultiheadAttention` in `__init__.pyi.in`. I've tested the code hint in PyCharm and all works file.
Relate issue: [https://github.com/pytorch/pytorch/issues/27842](https://github.com/pytorch/pytorch/issues/27842)
ezyang
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28396

Differential Revision: D19300287

Pulled By: ezyang

fbshipit-source-id: 1a79d6518b5edd4643892c46a959108385c739ad",yyb1995,https://api.github.com/repos/pytorch/pytorch/git/commits/20c5dd59bda182f526e2302623db89cad934c235
bc68a8745f1838f4e519f2f456593f8189ba23ee,"Spelling fix in transformer docs

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31973

Differential Revision: D19330660

Pulled By: zou3519

fbshipit-source-id: 29ea1e790a34f0241cb7aba85110f087cdc069ba",jlquinn,https://api.github.com/repos/pytorch/pytorch/git/commits/bc68a8745f1838f4e519f2f456593f8189ba23ee
e74a215adeec1791cff85376f056c6e04828bcfb,"Changed clip_grad_norm_ total_norm calculation (#32020)

Summary:
Redefines the computation of the total_norm to increase performance as shown in https://github.com/pytorch/pytorch/issues/31474.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32020

Differential Revision: D19353309

Pulled By: ngimel

fbshipit-source-id: bf7530dcd39f56614a211b5f21445864d4f2e875",Enealor,https://api.github.com/repos/pytorch/pytorch/git/commits/e74a215adeec1791cff85376f056c6e04828bcfb
05088da8e907fbf07eeb3546bcd670778a3f40f4,"[pytorch][PR] Fixed error in sample code of documentation (#31682)

Summary:
""in_features"" and ""out_features"" are not defined. Possibly a typo. They should be ""input_features"" and ""output_features"" instead
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31682

Differential Revision: D19251685

Pulled By: zou3519

fbshipit-source-id: ac9e524e792a1853a16e8876d76b908495d8f35e",vamshichowdary,https://api.github.com/repos/pytorch/pytorch/git/commits/05088da8e907fbf07eeb3546bcd670778a3f40f4
0392e8384bfbaaa2ad9f594fa7d95b8a06b5c146,"Fix simple typo: whos -> whose (#31288)

Summary:
Closes https://github.com/pytorch/pytorch/issues/31287
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31288

Differential Revision: D19166753

Pulled By: zou3519

fbshipit-source-id: da31ad323b8fafa7cbc502fda4e2eb6e02facfb6",timgates42,https://api.github.com/repos/pytorch/pytorch/git/commits/0392e8384bfbaaa2ad9f594fa7d95b8a06b5c146
3363ca20a76f7c7be2aa00b59e0d98b5e49b9d27,"example_outputs Doc Edit (#31826)

Summary:
torch.onnx.export docs contain two descriptions for 'example_outputs' arg.
So combined the information for it with the description with the parameters.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31826

Differential Revision: D19274928

Pulled By: zou3519

fbshipit-source-id: cbcce0a79c51784c1d7aa8981aab8aac118ca9b4",Chetank99,https://api.github.com/repos/pytorch/pytorch/git/commits/3363ca20a76f7c7be2aa00b59e0d98b5e49b9d27
851a7e861be120c74787ef533ebf7a20e1e0b2c2,"Add CAFFE2_API to video decoding functions (#31187)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/31132
Also closes old issue https://github.com/pytorch/pytorch/issues/11735
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31187

Differential Revision: D19147172

Pulled By: pbelevich

fbshipit-source-id: e959058eec3489061f431fbecc99ded0d4dc1704",h6197627,https://api.github.com/repos/pytorch/pytorch/git/commits/851a7e861be120c74787ef533ebf7a20e1e0b2c2
f94aab45fd7b1e7e32d8ea4324da6d3632a0d412,"Logical condition reduction (#32201)

Summary:
x || ( !x  &&  y )  <=>  to x || y
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32201

Differential Revision: D19429334

Pulled By: ezyang

fbshipit-source-id: 044dc46c2d9a7e180aa1795703c0097b0c7c3585",gaurav1086,https://api.github.com/repos/pytorch/pytorch/git/commits/f94aab45fd7b1e7e32d8ea4324da6d3632a0d412
a2641e6005d29d22e982a4525d64fd0f89cfaccf,"Make type of `Tensor.type()` more specific (#32353)

Summary:
Fixes the following issue:

```
$ cat test.py
import torch

t = torch.tensor(1.5)
t.type(torch.float32)[None]

$ mypy test.py
test.py:4: error: Invalid index type ""None"" for ""Union[str, Tensor]""; expected type ""Union[int, slice]""
Found 1 error in 1 file (checked 1 source file)
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32353

Differential Revision: D19499388

Pulled By: ezyang

fbshipit-source-id: 715111e934aea020b20f850d27e32c4f70b82572",bartosz319,https://api.github.com/repos/pytorch/pytorch/git/commits/a2641e6005d29d22e982a4525d64fd0f89cfaccf
49cd83d7356aa40b3223866adab8a7d823f2c9f4,"no more build_pytorch_libs.sh/.bat (#32319)

Summary:
https://github.com/pytorch/pytorch/issues/12918
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32319

Differential Revision: D19544272

Pulled By: soumith

fbshipit-source-id: dd32fa61efa78af908f21c7e54cb6484bf895e54",denfromufa,https://api.github.com/repos/pytorch/pytorch/git/commits/49cd83d7356aa40b3223866adab8a7d823f2c9f4
320d1a15735bdba57997343131956ee63d144085,"Fix wrong typing (torch/nn/parameter.pyi) (#32617)

Summary:
A constructor of `nn.Parameter` has default values on `data` and `requires_grad`, but in type stub, there are no default values.

Resolve https://github.com/pytorch/pytorch/issues/32481
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32617

Differential Revision: D19571397

Pulled By: ngimel

fbshipit-source-id: fd14298aa472b7575221229cecf5a56f8c84f531",jeongukjae,https://api.github.com/repos/pytorch/pytorch/git/commits/320d1a15735bdba57997343131956ee63d144085
5fd037ce4450d2a7bb477fa0f58677d7b256fdfe,"Fix MagmaInitializesCorrectly_CUDA by using an invertible matrix (#32547)

Summary:
This test case had been using the tensor

```
1  2  3  4
5  6  7  8
9  10 11 12
13 14 15 16
```

which is not an invertible tensor and causes the test case to fail, even if magma gets initialized just fine. This change uses a tensor that is invertible, and the inverse doesn't include any elements that are close to zero to avoid floating point rounding errors.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32547

Differential Revision: D19572316

Pulled By: ngimel

fbshipit-source-id: 1baf3f8601b2ba69fdd6678d7a3d86772d01edbe",charleshofer,https://api.github.com/repos/pytorch/pytorch/git/commits/5fd037ce4450d2a7bb477fa0f58677d7b256fdfe
8e4161517e112478a1c1f0290fedb91965f95aff,"div_kernel: throw when dividing by integer zero (#32629)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/327
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32629

Differential Revision: D19595782

Pulled By: ezyang

fbshipit-source-id: f5bbb298f150efe63a698e8a0b53a84871d16560",Baranowski,https://api.github.com/repos/pytorch/pytorch/git/commits/8e4161517e112478a1c1f0290fedb91965f95aff
9a2691f2fc948b9792686085b493c61793c2de30,"Fix spelling errors

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32673

Differential Revision: D19597118

Pulled By: pietern

fbshipit-source-id: f88c1da7548fcee141ed248f5f49d25c1d639955",hjung4,https://api.github.com/repos/pytorch/pytorch/git/commits/9a2691f2fc948b9792686085b493c61793c2de30
c7bf4d22fe7f0de239feeb084682827c3b9a759f,"added exception args to the returned error message (#32693)

Summary:
addresses https://github.com/pytorch/pytorch/issues/32692
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32693

Differential Revision: D19606757

Pulled By: mrshenli

fbshipit-source-id: 79fc09f8bb6a33e1b73ce0bbc45387544c7adc1b",Coderx7,https://api.github.com/repos/pytorch/pytorch/git/commits/c7bf4d22fe7f0de239feeb084682827c3b9a759f
8ead65a94647cae21984f791f8c81ed3f1259fd2,"[PyTorch][TorchScript] Add support for join on List of strings in TorchScript

Summary: Add support for join on List of strings in TorchScript.

Test Plan:
(pytorch) smummadi@smummadi-mbp pytorch % python test/test_jit_string.py
Fail to import hypothesis in common_utils, tests are not derandomized
.
----------------------------------------------------------------------
Ran 1 test in 1.090s

OK

Differential Revision: D19611800

fbshipit-source-id: cef66356abc14dfd100a806d25dd1a8bc9af0a11",mvsampath,https://api.github.com/repos/pytorch/pytorch/git/commits/8ead65a94647cae21984f791f8c81ed3f1259fd2
bc2e05a3984be43fda1925ec465c9734a98fd18a,"Update Docs for building PyTorch for Android.

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32578

Reviewed By: ljk53

Differential Revision: D19588904

Pulled By: dreiss

fbshipit-source-id: 2934752b9c5b94f2f141417669d8385be44d703b",hovhannesgithub,https://api.github.com/repos/pytorch/pytorch/git/commits/bc2e05a3984be43fda1925ec465c9734a98fd18a
7b65acdf9e8345d2ffbbc3b85ae6667f7260407e,"Solves Issue #32750 - torch.prod now works fine with FP16 Input Tensor and FP32 Output Tensor (#32831)

Summary:
This PR solves Issue https://github.com/pytorch/pytorch/issues/32750.

- Changes function prod_kernel_impl to use `out_t` argument instead of `scalar_t` (which caused the garbage output for FP16 input and FP32 output tensor type).
- Adds test case for `torch.prod` (for CUDA): tests both `torch.prod` and `torch.tensor.prod`. Checks all the combinations for dtypes: `torch.float16` and `torch.float32`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32831

Differential Revision: D19664666

Pulled By: ngimel

fbshipit-source-id: c275363355c832899f10325043535949cd12b2f8",krshrimali,https://api.github.com/repos/pytorch/pytorch/git/commits/7b65acdf9e8345d2ffbbc3b85ae6667f7260407e
ad78c0f4fc880c0da5064ccb85290c38dbe252ab,"Fixed the flaky test_rref_context_debug_info (#32749)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32749

The test was flaky since the message from owner RRef confirming fork would arrive after the test checked whether the pending User RRefs map was empty - leading to an assertion error. This diff creates a utility function that should be used by any test to wait for this message to complete processing before doing any assertions related to the pending User RRefs map.

GitHub Issue: https://github.com/pytorch/pytorch/issues/30988

Test Plan: Stress tested `test_rref_context_debug_info` 200 times.

Differential Revision: D19612289

fbshipit-source-id: 57a7c19b1cf792b94c263d3efbbbb6da60c07d07",osalpekar,https://api.github.com/repos/pytorch/pytorch/git/commits/ad78c0f4fc880c0da5064ccb85290c38dbe252ab
e87887ccb42e916cd3397600e5411ff65566e337,"Update type hints for torch.optim.optimizer.Optimizer (#32900)

Summary:
This PR fixes type hints for `torch.optim.optimizer.Optimizer` object, issue also reported in https://github.com/pytorch/pytorch/issues/23731

To test things I used following optimiser implementation, that is fully covered with type hints:

```python
from typing import Optional, Callable, Union, Iterable

from torch import Tensor
from torch.optim.optimizer import Optimizer

OptClosure = Optional[Callable[[], float]]
_params_t = Union[Iterable[Tensor], Iterable[dict]]

class SGD(Optimizer):
    def __init__(self, params: _params_t, lr: float = 0.1) -> None:
        defaults = dict(lr=lr)
        super(SGD, self).__init__(params, defaults)

    def __setstate__(self, state: dict) -> None:
        super(SGD, self).__setstate__(state)

    def step(self, closure: OptClosure = None) -> Optional[float]:
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue
                d_p = p.grad.data
                p.data.add_(-group['lr'], d_p)
        return loss
```

Without fix `mypy` reports bunch of inconsistencies in types and missing properties:

```bash
$ mypy  torch_optimizer/sgd.py
torch_optimizer/sgd.py:14: error: Too many arguments for ""__init__"" of ""Optimizer""
torch_optimizer/sgd.py:17: error: ""__setstate__"" undefined in superclass
torch_optimizer/sgd.py:19: error: Return type ""Optional[float]"" of ""step"" incompatible with return type ""None"" in supertype ""Optimizer""
torch_optimizer/sgd.py:24: error: ""SGD"" has no attribute ""param_groups""
Found 4 errors in 1 file (checked 1 source file)
```

with fix not issues:
```bash
$ mypy  torch_optimizer/sgd.py
Success: no issues found in 1 source file
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32900

Differential Revision: D19697175

Pulled By: ezyang

fbshipit-source-id: d5e2b3c421f69da3df8c32b3d53b4b6d15d61a41",jettify,https://api.github.com/repos/pytorch/pytorch/git/commits/e87887ccb42e916cd3397600e5411ff65566e337
c841ab403c8f6d0c85694a750970e0c5a0da56d2,"add missing method annotations to torch.Tensor (#30576)

Summary:
Looks like some of the tensor methods defined in https://github.com/pytorch/pytorch/blob/master/torch/tensor.py#L393 were missing.

Also add missing self object to `map_`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30576

Differential Revision: D19698355

Pulled By: ezyang

fbshipit-source-id: 6df99f17d5de11715dbe89aecb292612405c08ac",christopher-hesse,https://api.github.com/repos/pytorch/pytorch/git/commits/c841ab403c8f6d0c85694a750970e0c5a0da56d2
37953d92d198b6d8941d8120218d86d6e3813eee,"raise when jit-load.ing a folder (#27836)

Summary:
Very similar to https://github.com/pytorch/pytorch/issues/16267 but handling directories.

Stoked to contribute!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27836

Differential Revision: D19698398

Pulled By: ezyang

fbshipit-source-id: eabc3a44d258124f860babb47ab91e22c2c3d6cc",fenollp,https://api.github.com/repos/pytorch/pytorch/git/commits/37953d92d198b6d8941d8120218d86d6e3813eee
00c6b903278d574d94171ebbf01986d698e22716,"Fix in documentation of convolutional modules (#30079)

Summary:
I noticed the description of the initialization of convolutional modules is inconsistent with the actual implementation. There are two such cases:

1) `k` in the initialization of ConvTranspose modules is not dependent on the input channels but on the output channels (`kaiming_uniform_` uses the size of the second dimension of `weight` which is transposed in the first two dimensions).

2) Both the normal convolutions and the transposed ones use `k` divided by `groups`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30079

Differential Revision: D19698511

Pulled By: ezyang

fbshipit-source-id: 1ba938fbbd97663eaf29fd1245872179d2761fff",davda54,https://api.github.com/repos/pytorch/pytorch/git/commits/00c6b903278d574d94171ebbf01986d698e22716
612e621da06f9fdd1acd00cb875e0134f3472aff,"Improve CHECK_OP macro (#29539)

Summary:
- Show values in question like glog.
- Handle expressions with logical operators properly by adding
  parentheses around expressions.
- Allow outputting nullptr (some build failed without this)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29539

Reviewed By: dreiss

Differential Revision: D19698991

Pulled By: ljk53

fbshipit-source-id: e329c01622cfc386ac009904092519a4adfe94a8",shinh,https://api.github.com/repos/pytorch/pytorch/git/commits/612e621da06f9fdd1acd00cb875e0134f3472aff
18d1896ba014c8a79ed60ab40dcd7be0f96ae431,"Fix confusing ""does not have GPU support"" warning message (#30721)

Summary:
Many people who use caffe2 are confused about ""does not have GPU support"" warning message.
https://github.com/facebookresearch/video-nonlocal-net/issues/6
facebookarchive/caffe2#346
facebookarchive/caffe2#1634
facebookarchive/caffe2#197

Many none GPU reasons can cause this warning message. It is better to give the error info.
![image](https://user-images.githubusercontent.com/13826327/70129721-41175e00-16ba-11ea-85df-a4b1a1690149.png)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30721

Differential Revision: D19697413

Pulled By: ezyang

fbshipit-source-id: bd24b7c814e7e677352068b9e9f77a68de080159",mpjlu,https://api.github.com/repos/pytorch/pytorch/git/commits/18d1896ba014c8a79ed60ab40dcd7be0f96ae431
d3a0bdd06b0a265903d94570d6c0b9004883ddd0,"proofreading (#29797)

Summary:
two instances of if -> it in torch.nn.modules.batchnorm.py
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29797

Differential Revision: D19698613

Pulled By: ezyang

fbshipit-source-id: 7312b2333f227113e904dfa91db90d00e525affb",nicklashansen,https://api.github.com/repos/pytorch/pytorch/git/commits/d3a0bdd06b0a265903d94570d6c0b9004883ddd0
27e1fecabd1941b70eaa54b65d921452b613de69,"let user specify CUDA_HOST_COMPILER

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32904

Differential Revision: D19729047

Pulled By: ezyang

fbshipit-source-id: c233e3924f71a025c51d25a7e3a8d728dac8730a",cyyever,https://api.github.com/repos/pytorch/pytorch/git/commits/27e1fecabd1941b70eaa54b65d921452b613de69
4f5908d5d7c7aa0aff9106cd8066e57db3b0a652,"Remove unneded TORCH_API (#32015)

Summary:
It was causing a build error when compiling on MINGW64
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32015

Differential Revision: D19697296

Pulled By: ezyang

fbshipit-source-id: 71e58783c48f8e99755c091b2027d59740dfca47",aviloria,https://api.github.com/repos/pytorch/pytorch/git/commits/4f5908d5d7c7aa0aff9106cd8066e57db3b0a652
a9141dd240505d1e406d998e2fa779c2e2f80c27,"Patch `Half.h` for compiling CUDA with clang (#29027)

Summary:
Following discussion: https://github.com/pytorch/pytorch/issues/28417
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29027

Differential Revision: D19698745

Pulled By: ezyang

fbshipit-source-id: fab4be3bcbac8f3b334d7e0a56e6a790e2c6b6d8",johnmarkwayve,https://api.github.com/repos/pytorch/pytorch/git/commits/a9141dd240505d1e406d998e2fa779c2e2f80c27
908b451efb1b35a3ef26154bc959cae7ff29e15e,"Enabling the nccl/rccl test for ROCM environment (#32340)

Summary:
Enabling the RCCL test on rocm by adding a temporary grace period to clean up.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32340

Differential Revision: D19744459

Pulled By: xw285cornell

fbshipit-source-id: 1af3b64113a67f93e622d010ddd3020e5d6c8bc8",mpruthvikumar,https://api.github.com/repos/pytorch/pytorch/git/commits/908b451efb1b35a3ef26154bc959cae7ff29e15e
7314f1c2818978fb28b420710ffe1f3b1c0b95a1,"[torch/multiprocessing] Update documentation indicating that start_method is ignored for mp.spawn() (#33070)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33070

`start_method` parameter is intentionally ignored for `mp.spawn()`. Document this fact and point the user to `start_processes` if they want to use a different `start_method`.

Test Plan:
Warning message looks like:
```
main.py:8: UserWarning: This method only supports start_method=spawn (got: fork).
To use a different start_method use:
         torch.multiprocessing.start_process(...)
  warnings.warn(msg)
```

Reviewed By: ailzhang

Differential Revision: D19780235

fbshipit-source-id: 4599cd18c3ba6cc401810efe4f390290ffa8023b",kiukchung,https://api.github.com/repos/pytorch/pytorch/git/commits/7314f1c2818978fb28b420710ffe1f3b1c0b95a1
9d94f56ce02a814fb3c9dfe3fee675420a7e7e30,"Backward operation of torch.eig for real eigenvalues (#33090)

Summary:
Another pull request to follow up issue https://github.com/pytorch/pytorch/issues/32531.
Here I implemented the backward operation for `torch.eig` with a condition that all the eigenvalues are real.

This pull request is independent of my another pull request https://github.com/pytorch/pytorch/issues/32932, which means that there is no dependency between this PR and my another PR.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33090

Differential Revision: D19814347

Pulled By: albanD

fbshipit-source-id: 2fae30964e97987abb690544df8240aedeae56e8",mfkasim91,https://api.github.com/repos/pytorch/pytorch/git/commits/9d94f56ce02a814fb3c9dfe3fee675420a7e7e30
9e7638f7c1302a5afbde4a23776c2ff46a7d9b8f,"""batchSize"" was set but never used (#32294)

Summary:
fixes a compiler warning:
```
torch/aten/src/ATen/native/cuda/MaxUnpooling.cu.cc(402):
warning: variable ""batchSize"" was set but never used
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32294

Differential Revision: D19697277

Pulled By: ezyang

fbshipit-source-id: b9821be325826dc4785cad7994803b54f1711a0c",luxe,https://api.github.com/repos/pytorch/pytorch/git/commits/9e7638f7c1302a5afbde4a23776c2ff46a7d9b8f
09915ad570d7f9116ced9fb509928dc8f6c0bf40,"[TensorBoard] Correct typo and wrap dataformats. (#31604)

Summary:
Resolves issue https://github.com/pytorch/pytorch/issues/31603

- A minor spelling typo is corrected: ""suitible"" --> ""suitable""
- A minor quality of life improvement is added: the data format strings are better rendered as fixed width to indicate that they are string constants.  ""CHW"" --> ""`CHW`""
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31604

Differential Revision: D19697293

Pulled By: ezyang

fbshipit-source-id: ee38b0d4c9ca8a233ac9243c310d9a3b42ad6f32",kdbanman,https://api.github.com/repos/pytorch/pytorch/git/commits/09915ad570d7f9116ced9fb509928dc8f6c0bf40
2e9b7c5fe11403b26090f8e45081734d1285f7b5,"Migrate dist from TH to ATen(CPU, CUDA) (#29714)

Summary:
[https://github.com/pytorch/pytorch/issues/24691](https://github.com/pytorch/pytorch/issues/24691)
[https://github.com/pytorch/pytorch/issues/24551](https://github.com/pytorch/pytorch/issues/24551)

Benchmark:

**Speed**
```python
import time, sys
import torch
import math

inf = math.inf

torch.manual_seed(0)
devices = [""cpu"", ""cuda""]
ps = [0, 1, 2, 3, 4, inf, -inf]

# Warm up
for device in devices:
    for n in [1, 10, 100, 1000]:
        x = torch.randn(100, n, requires_grad=False, device=device)
        y = torch.randn(100, n, requires_grad=False, device=device)
        for i in range(1000):
            for p in ps:
                dist_xy = torch.dist(x, y, p)

for device in devices:
    print('On {}'.format(device))
    for n in [1, 10, 100, 1000]:
        total_time = 0
        x = torch.randn(100, n, requires_grad=False, device=device)
        y = torch.randn(100, n, requires_grad=False, device=device)
        for i in range(10000):
            for p in ps:
                t1 = time.time()
                dist_xy = torch.dist(x, y, p)
                t2 = time.time()
                total_time += (t2 - t1)
        average_time = total_time / 10000 / len(ps) * 1000
        print(""input size(100, %d) average time is %.8f (ms)."" % (n, average_time))
```

Output
Before:
```shel
On cpu
input size(100, 1) average time is 0.0079491 (ms).
input size(100, 10) average time is 0.0364167 (ms).
input size(100, 100) average time is 0.3120752 (ms).
input size(100, 1000) average time is 3.0605820 (ms).
On cuda
input size(100, 1) average time is 0.04745627 (ms).
input size(100, 10) average time is 0.04919453 (ms).
input size(100, 100) average time is 0.06601572 (ms).
input size(100, 1000) average time is 0.07849015 (ms).
```

After:
```shell
On cpu
input size(100, 1) average time is 0.0099936 (ms).
input size(100, 10) average time is 0.0340414 (ms).
input size(100, 100) average time is 0.2793379 (ms).
input size(100, 1000) average time is 0.7858076 (ms).
On cuda
input size(100, 1) average time is 0.04410237 (ms).
input size(100, 10) average time is 0.03326339 (ms).
input size(100, 100) average time is 0.03314828 (ms).
input size(100, 1000) average time is 0.03990038 (ms).
```

**Precision**

```python
for device in devices:
    torch.manual_seed(0)
    print('On {}'.format(device))
    for n in [1, 10, 100, 1000]:
        x = torch.randn(100, n, requires_grad=False).to(device)
        y = torch.randn(100, n, requires_grad=False).to(device)
        for p in ps:
            dist_xy_float = torch.dist(x, y, p)
            dist_xy_double = torch.dist(x.double(), y.double(), p)
            difference = torch.abs(dist_xy_double - dist_xy_float)
            print('input size (100, {}), p: {}, float: {}, double: {}, difference: {}'.format(n, p, dist_xy_float, dist_xy_double, difference))
```

Part of [output](https://gist.github.com/rivergold/dd95014dc7f163b22f72699d1134cdd2)
Before:
```shell
On cpu
input size (100, 100), p: 0, float: 10000.0, double: 10000.0, difference: 0.0
input size (100, 100), p: 1, float: 11474.1806640625, double: 11474.185433543797, difference: 0.00476948129653465
input size (100, 100), p: 2, float: 143.50729370117188, double: 143.5073391487937, difference: 4.5447621829453055e-05
input size (100, 100), p: 3, float: 36.045475006103516, double: 36.04550275212738, difference: 2.774602386779179e-05
input size (100, 100), p: 4, float: 18.796083450317383, double: 18.79609807865317, difference: 1.4628335787136848e-05
input size (100, 100), p: inf, float: 5.540258407592773, double: 5.5402586460113525, difference: 2.384185791015625e-07
input size (100, 100), p: -inf, float: 3.4868717193603516e-06, double: 3.4868717193603516e-06, difference: 0.0
On cuda
input size (100, 100), p: 0, float: 10000.0, double: 10000.0, difference: 0.0
input size (100, 100), p: 1, float: 11474.1865234375, double: 11474.185433543797, difference: 0.00108989370346535
input size (100, 100), p: 2, float: 143.50733947753906, double: 143.5073391487933, difference: 3.2874575595087663e-07
input size (100, 100), p: 3, float: 36.04550552368164, double: 36.045502752127405, difference: 2.7715542358919265e-06
input size (100, 100), p: 4, float: 18.796098709106445, double: 18.796098078653177, difference: 6.304532682577246e-07
input size (100, 100), p: inf, float: 5.540258407592773, double: 5.5402586460113525, difference: 2.384185791015625e-07
input size (100, 100), p: -inf, float: 3.4868717193603516e-06, double: 3.4868717193603516e-06, difference: 0.0
```

After
```shell
On cpu
input size (100, 100), p: 0, float: 10000.0, double: 10000.0, difference: 0.0
input size (100, 100), p: 1, float: 11474.1806640625, double: 11474.185433543797, difference: 0.00476948129653465
input size (100, 100), p: 2, float: 143.50729370117188, double: 143.5073391487937, difference: 4.5447621829453055e-05
input size (100, 100), p: 3, float: 36.045475006103516, double: 36.04550275212738, difference: 2.774602386779179e-05
input size (100, 100), p: 4, float: 18.796083450317383, double: 18.79609807865317, difference: 1.4628335787136848e-05
input size (100, 100), p: inf, float: 5.540258407592773, double: 5.5402586460113525, difference: 2.384185791015625e-07
input size (100, 100), p: -inf, float: 3.4868717193603516e-06, double: 3.4868717193603516e-06, difference: 0.0
On cuda
input size (100, 100), p: 0, float: 10000.0, double: 10000.0, difference: 0.0
input size (100, 100), p: 1, float: 11474.185546875, double: 11474.185433543797, difference: 0.00011333120346534997
input size (100, 100), p: 2, float: 143.50733947753906, double: 143.5073391487933, difference: 3.2874575595087663e-07
input size (100, 100), p: 3, float: 36.04550552368164, double: 36.045502752127405, difference: 2.7715542358919265e-06
input size (100, 100), p: 4, float: 18.796096801757812, double: 18.796098078653177, difference: 1.2768953645547754e-06
input size (100, 100), p: inf, float: 5.540258407592773, double: 5.5402586460113525, difference: 2.384185791015625e-07
input size (100, 100), p: -inf, float: 3.4868717193603516e-06, double: 3.4868717193603516e-06, difference: 0.0
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29714

Differential Revision: D19769518

Pulled By: albanD

fbshipit-source-id: 69b79b64f1f190b410efe884662b6601e903eccf",rivergold,https://api.github.com/repos/pytorch/pytorch/git/commits/2e9b7c5fe11403b26090f8e45081734d1285f7b5
b28a8348139f4f35b69972e281e463e34d2735c4,"[codemod][lint][fbcode] Apply google-java-format

Test Plan: Sandcastle. Visual inspection.

Reviewed By: scottrice

Differential Revision: D19878711

fbshipit-source-id: be56f70b35825140676be511903e5274d1808f25",zertosh,https://api.github.com/repos/pytorch/pytorch/git/commits/b28a8348139f4f35b69972e281e463e34d2735c4
0c93c2b142ef3ce46a74d195de0dfd26002f41cd,"Add a warning sign for anomaly detection (#33176) (#33239)

Summary:
Fixes [33176](https://github.com/pytorch/pytorch/issues/33176)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33239

Differential Revision: D19879847

Pulled By: albanD

fbshipit-source-id: 594b936c10f98c364331e782b64f42059413a741",prajjwal1,https://api.github.com/repos/pytorch/pytorch/git/commits/0c93c2b142ef3ce46a74d195de0dfd26002f41cd
e5218e3e12096780f809efb6aced72509a1b54d2,"Add missing error messages for container modules (#29991)

Summary:
Container `Module`s, including `ModuleList`, `ParameterList` and `ParameterDict`, should not be called like a regular `Module`.
This PR add error messages for these special modules.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29991

Differential Revision: D19698535

Pulled By: ezyang

fbshipit-source-id: fe156a0bbb033041086734b38f8c6fde034829bf",songyouwei,https://api.github.com/repos/pytorch/pytorch/git/commits/e5218e3e12096780f809efb6aced72509a1b54d2
0150f40ddea0aa65336e55e4ac549d400c8e180f,"dont force msvc /Ox flag which can conflict with /RTC1 in debug config (#33164)

Summary:
Relates to https://github.com/pytorch/pytorch/issues/33132

This fix doesn't add full multi-configuration support described in https://github.com/pytorch/pytorch/issues/33132 but at least avoid the error presented in the issue when `CMAKE_BUILD_TYPE=Debug` is used with MSVC.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33164

Differential Revision: D19899727

Pulled By: ezyang

fbshipit-source-id: 28a364d920c4a3fb577c6b484ccd69a133fbcf5d",fmigneault,https://api.github.com/repos/pytorch/pytorch/git/commits/0150f40ddea0aa65336e55e4ac549d400c8e180f
cfb4862673303a2725d815c0338435f6b6c699fb,"[pytorch] correct input size check for GroupNorm (#33008)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33008

Corrects D19373507 to allow valid use cases that fail now. Multiplies batch size by the number of elements in a group to get the correct number of elements over which statistics are computed.

**Details**:
The current implementation disallows GroupNorm to be applied to tensors of shape e.g. `(1, C, 1, 1)` to prevent cases where statistics are computed over 1 element and thus result in a tensor filled with zeros.
However, in GroupNorm the statistics are calculated across channels. So in case where one has an input tensor of shape `(1, 256, 1, 1)` for `GroupNorm(32, 256)`, the statistics will be computed over 8 elements and thus be meaningful.

One use case is [Atrous Spatial Pyramid Pooling (ASPPPooling)](https://github.com/pytorch/vision/blob/791c172a337d98012018f98ffde93b1020ba3ed5/torchvision/models/segmentation/deeplabv3.py#L50), where GroupNorm could be used in place of BatchNorm [here](https://github.com/pytorch/vision/blob/791c172a337d98012018f98ffde93b1020ba3ed5/torchvision/models/segmentation/deeplabv3.py#L55). However, now this is prohibited and results in failures.

Proposed solution consists in correcting the computation of the number of elements over which statistics are computed. The number of elements per group is taken into account in the batch size.

Test Plan: check that existing tests pass

Reviewed By: fmassa

Differential Revision: D19723407

fbshipit-source-id: c85c244c832e6592e9aedb279d0acc867eef8f0c",vkhalidov,https://api.github.com/repos/pytorch/pytorch/git/commits/cfb4862673303a2725d815c0338435f6b6c699fb
2c99ea86540da8dad0d2123e01d17148052f0fef,"Dirac init compatibility with group convolutions (#32825)

Summary:
Initializing weights of group-conv with init.dirac_, and applying, previously resulted in an output that makes no sense:
```
x = torch.randn([1, 3, 3, 3])
print('input:\n', x)
conv_layer = torch.nn.Conv2d(3, 3, 3, padding=1, groups=3, bias=False)
torch.nn.init.dirac_(conv_layer.weight.data)
print('\noutput (before this PR):\n',conv_layer(x))

input:
 tensor([[[[ 0.5369, -1.1428,  0.1031],
          [ 0.4638, -0.0854, -0.6553],
          [ 0.8321, -2.5926, -0.3214]],

         [[-0.2289, -0.0895,  0.4407],
          [ 1.2309, -1.2096, -1.5216],
          [-0.1798,  1.1694,  0.3469]],

         [[ 0.1905,  0.8095,  0.5490],
          [-0.4525, -0.4284, -0.1141],
          [ 1.1857, -0.9246, -0.5119]]]])

output (before this PR):
 tensor([[[[ 0.5369, -1.1428,  0.1031],
          [ 0.4638, -0.0854, -0.6553],
          [ 0.8321, -2.5926, -0.3214]],

         [[ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000]],

         [[ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000]]]], grad_fn=<MkldnnConvolutionBackward>)
````

This PR allows introducing groups to the initialization:
```
torch.nn.init.dirac_(conv_layer.weight.data, groups=3)
print('output (after this PR):\n', conv_layer(x))

output (after this PR):
 tensor([[[[ 0.5369, -1.1428,  0.1031],
          [ 0.4638, -0.0854, -0.6553],
          [ 0.8321, -2.5926, -0.3214]],

         [[-0.2289, -0.0895,  0.4407],
          [ 1.2309, -1.2096, -1.5216],
          [-0.1798,  1.1694,  0.3469]],

         [[ 0.1905,  0.8095,  0.5490],
          [-0.4525, -0.4284, -0.1141],
          [ 1.1857, -0.9246, -0.5119]]]], grad_fn=<MkldnnConvolutionBackward>)
```

When out_channels is different than input_channels, it does the natural thing which is applying identity in each group separately:

```
x = torch.randn([1, 2, 3, 3])
print('input:\n', x)
conv_layer = torch.nn.Conv2d(2, 4, 3, padding=1, groups=2, bias=False)
torch.nn.init.dirac_(conv_layer.weight.data, groups=2)
print('\noutput:\n', conv_layer(x))

input:
 tensor([[[[ 1.2205, -0.6608,  0.8640],
          [-0.5464,  1.1288,  1.4726],
          [-0.6693,  0.4000, -1.7613]],

         [[-0.8760, -0.8814, -0.4705],
          [ 0.6283, -0.5943,  0.6873],
          [-0.6852,  1.4723,  0.3325]]]])

output:
 tensor([[[[ 1.2205, -0.6608,  0.8640],
          [-0.5464,  1.1288,  1.4726],
          [-0.6693,  0.4000, -1.7613]],

         [[ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000]],

         [[-0.8760, -0.8814, -0.4705],
          [ 0.6283, -0.5943,  0.6873],
          [-0.6852,  1.4723,  0.3325]],

         [[ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000]]]], grad_fn=<MkldnnConvolutionBackward>)
```

Argument 'groups' defaults to 1 so it is backward compatible.

Tests are modified to include cases of with groups>1 but also contain groups=1 cases.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32825

Differential Revision: D19859926

Pulled By: vincentqb

fbshipit-source-id: 9dfdd24471ff14d79c442dfd28c1891aff812fdf",assafshocher,https://api.github.com/repos/pytorch/pytorch/git/commits/2c99ea86540da8dad0d2123e01d17148052f0fef
60339a38eded68eeef91f84734b4d665ddf4e080,"Fixes #33001 (#33456)

Summary:
This fixes https://github.com/pytorch/pytorch/issues/33001.

When subtracting 1 from a empty array, instead of being `-1` as seems to be expected in the later code (while loop), because `size()` seems to be unsigned, it becomes a very large number. This causes a segfault during the while loop later in the code where it tries to access a empty array.

This issue seemed to happen only on the pi with the following example code: `v = torch.FloatTensor(1, 135).fill_(0); v[0, [1]] += 2`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33456

Differential Revision: D19963711

Pulled By: ezyang

fbshipit-source-id: 1dbddd59a5df544cd7e025fc540c9efe2c4e19f4",matham,https://api.github.com/repos/pytorch/pytorch/git/commits/60339a38eded68eeef91f84734b4d665ddf4e080
1d9fcf8bd2ec0682610555320036fc4c7a573594,"Correct documentation for torch.unsqueeze (#33478)

Summary:
""out"" argument in torch.unsqueeze is not actually implemented, fixed documentation https://github.com/pytorch/pytorch/issues/29800
After: ![image](https://user-images.githubusercontent.com/33493903/74796371-6289ee00-5296-11ea-8493-e8c18ac63bdf.png)

Before: ![image](https://user-images.githubusercontent.com/33493903/74796444-96651380-5296-11ea-816c-2adacfa79e35.png)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33478

Differential Revision: D19978477

Pulled By: yf225

fbshipit-source-id: 42337326c1ec04975307366c94591ee32a11b091",matthew-haines,https://api.github.com/repos/pytorch/pytorch/git/commits/1d9fcf8bd2ec0682610555320036fc4c7a573594
5e80ca12bb68c7fd34bab944f777a855d62c498c,"[pt][fbgemm] Turn on USE_FBGEMM on Windows env (#297)

Summary:
Pull Request resolved: https://github.com/pytorch/FBGEMM/pull/297

Pull Request resolved: https://github.com/pytorch/pytorch/pull/33250

As Title says. FBGEMM has recently added the support for Windows.

ghstack-source-id: 97932881

Test Plan: CI

Reviewed By: jspark1105

Differential Revision: D19738268

fbshipit-source-id: e7f3c91f033018f6355edeaf6003bd2803119df4",shz0116,https://api.github.com/repos/pytorch/pytorch/git/commits/5e80ca12bb68c7fd34bab944f777a855d62c498c
bd3c6e8e91cc8eca58b1a113a081e7701a1d595f,"avoid large vector copy when query per_channel q_params (#31040)

Summary:
The quantizer use std::vector to save per_channel scales and zero_points, but when query scales(zero_points), it requires to return tensor. These lead to use std::vector to initialize tensors and it dose cost lots of time. So I change quantizer to save per_channel scales and zero_points by using tensor directly.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31040

Differential Revision: D19701070

Pulled By: jerryzh168

fbshipit-source-id: 9043f16c44b74dd8289b8474e540171765a7f92a",zhuhaozhe,https://api.github.com/repos/pytorch/pytorch/git/commits/bd3c6e8e91cc8eca58b1a113a081e7701a1d595f
e77abb9a5ba45f0ac322d543b89cf368f3c713bf,"Normalize reward-to-go in C++ actor-critic (#33550)

Summary:
Comparing to the [Python implementation](https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py), it seems like the tensor of normalized reward-to-go is computed but never used. Even if it's just an integration test, this PR switches to the normalized version for better convergence.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33550

Differential Revision: D20024393

Pulled By: yf225

fbshipit-source-id: ebcf0fee14ff39f65f6744278fb0cbf1fc92b919",nicolov,https://api.github.com/repos/pytorch/pytorch/git/commits/e77abb9a5ba45f0ac322d543b89cf368f3c713bf
293fa5fc448656b056d56ab89bf82078c252ace7,"[Documentation] Fix minor typo in torch.serialization (#33549)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33549

Differential Revision: D20002545

Pulled By: albanD

fbshipit-source-id: 46fe2002329e5250c009eb066432909b71ecd74d",zz-xx,https://api.github.com/repos/pytorch/pytorch/git/commits/293fa5fc448656b056d56ab89bf82078c252ace7
9e384f9ce4c28b4127b23f9caf75c45f16a0e7e8,"Remove duplicate header include. (#33656)

Summary:
The same header `<torch/nn/functional/conv.h>` is included twice.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33656

Differential Revision: D20056913

Pulled By: yf225

fbshipit-source-id: b1563035c9821731b99c26eec130ff0b9cc627a7",h3lio5,https://api.github.com/repos/pytorch/pytorch/git/commits/9e384f9ce4c28b4127b23f9caf75c45f16a0e7e8
2b404de3477803179119d5b228ab67695ef6901a,"[scripts] Add script to fetch clang-format binary from AWS S3 (#33644)

Summary:
**Summary**
This commit adds a script that fetches a platform-appropriate `clang-format` binary
from S3 for use during PyTorch development. The goal is for everyone to use the exact
same `clang-format` binary so that there are no formatting conflicts.

**Testing**
Ran the script.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33644

Differential Revision: D20076598

Pulled By: SplitInfinity

fbshipit-source-id: cd837076fd30e9c7a8280665c0d652a33b559047",SplitInfinity,https://api.github.com/repos/pytorch/pytorch/git/commits/2b404de3477803179119d5b228ab67695ef6901a
5bac7febad036f8a9e124036f6c2509a540dd588,"removed padding and dilation from LPPool2d Doc (#33714)

Summary:
removed padding and dilation from LPPool2d Doc as the function dose not support padding and dilation
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33714

Differential Revision: D20097021

Pulled By: ngimel

fbshipit-source-id: fc1c2d918b32f4b45c7e6e6bd93f018e867a628f",joerg-de,https://api.github.com/repos/pytorch/pytorch/git/commits/5bac7febad036f8a9e124036f6c2509a540dd588
a836c4ca78b72ecc8e0664e1b684af64ce83be42,"Skip manual backward for `cdist` with case `p=2` (#31167)

Summary:
Fixes an issue with `cdist` backward calculation for large inputs for the euclidean case.

The grid size when launching the kernel exceeded the 2^16 limit for the second dimension, resulting in `RuntimeError: CUDA error: invalid configuration argument`

Code to reproduce:

```
h, w, d = 800, 1216, 12
n = 133
A = torch.randn(n, d).cuda()
B = torch.randn(h, w, d).cuda()
A.requires_grad = True
B.requires_grad = True

B = B.reshape(-1, d).contiguous()
dist = torch.cdist(A, B)
loss = dist.sum()
loss.backward()
```

Thanks to tkerola for the bug report, reproduction and suggesting a solution.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31167

Differential Revision: D20035605

Pulled By: ngimel

fbshipit-source-id: ae28ba4b549ee07a8bd937bb1de2438dc24eaa17",emcastillo,https://api.github.com/repos/pytorch/pytorch/git/commits/a836c4ca78b72ecc8e0664e1b684af64ce83be42
c1dd70688a28d2bc91ac1e10dcda62d4c7bbebce,"Fix deprecated python ""add"" calls (#33428)

Summary:
This PR fixed those python ""add"" calls using deprecated signature `add(Scalar, Tensor)`. The alternative signature `add(Tensor, alpha = Scalar)` is used.

cc csarofeen zasdfgbnm ptrblck ngimel
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33428

Differential Revision: D20002534

Pulled By: vincentqb

fbshipit-source-id: 81f2dd6170a47a9b53a17e5817c26e70d8afa130",xwang233,https://api.github.com/repos/pytorch/pytorch/git/commits/c1dd70688a28d2bc91ac1e10dcda62d4c7bbebce
edd5c009f78c405f80b7ab2bc03ed7bd5f61658f,"fix docs mistakes in lr_scheduler.MultiplicativeLR (#33805)

Summary:
This PR is referenced to an issue: [The docs of `MultiplicativeLR` use `LambdaLR` as example](https://github.com/pytorch/pytorch/issues/33752#issue-570374087)

https://github.com/pytorch/pytorch/issues/33752
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33805

Differential Revision: D20121314

Pulled By: mruberry

fbshipit-source-id: 5afa63bbe83d35ce4e55705b8cbd96326a907651",HearyShen,https://api.github.com/repos/pytorch/pytorch/git/commits/edd5c009f78c405f80b7ab2bc03ed7bd5f61658f
524dad13a8f785cda8d11f878fffed58249e5773,"Add device to the test tensor. Default device type is CPU, in pytorchâ€¦ (#33635)

Summary:
â€¦/xla this will result in a failure since it is comparing a XLA tensor with a CPU tensor.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33635

Differential Revision: D20043517

Pulled By: ailzhang

fbshipit-source-id: d84038ea675e4d4a9c02e7a8b0924bdb12f40501",JackCaoG,https://api.github.com/repos/pytorch/pytorch/git/commits/524dad13a8f785cda8d11f878fffed58249e5773
746e5218e7e2877f4a68042a12cfb9e19a606550,"Mistake in MSELoss documentation (#33836)

Summary:
Replaced `sum` with `mean` in [line 392](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/loss.py#L392)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33836

Differential Revision: D20142053

Pulled By: ailzhang

fbshipit-source-id: 2bfe19944ffc5534902dd9087023e70ddf5746c3",simaiden,https://api.github.com/repos/pytorch/pytorch/git/commits/746e5218e7e2877f4a68042a12cfb9e19a606550
38b6cb479bffffa93969f6be021bf78a4b7e1197,"Check fuser results when profiling (#33944)

Summary:
With the profiling executor enabled the fuser won't be invoked until the second pass over a script function, so some of these tests weren't correctly comparing the fused output with the interpreter output.  I've used the `checkScript` method where applicable, which seems to do the right thing.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33944

Test Plan: Locally inject obvious errors into the fuser and verify that the updated tests fail when they're supposed to.

Differential Revision: D20162320

Pulled By: bertmaher

fbshipit-source-id: 4a2f3f2d2ff1d81f23db504dc8cd0d5417bdcc50",bertmaher,https://api.github.com/repos/pytorch/pytorch/git/commits/38b6cb479bffffa93969f6be021bf78a4b7e1197
f4532d7542cc819786af9a3feb747be60d3de2b9,"Fix typo (#33925)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33925

Differential Revision: D20171970

Pulled By: vincentqb

fbshipit-source-id: 5c1a8553760f74cecebaea7e88463b767ab81211",silvasean,https://api.github.com/repos/pytorch/pytorch/git/commits/f4532d7542cc819786af9a3feb747be60d3de2b9
45c45195cd6526fbb64773abd316a24bbe49f50d,"Remove warning about building from source to use the NCCL backend (#34051)

Summary:
I think this warning isn't true anymore, and the NCCL backend works without PyTorch needing to be built from source.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/34051

Differential Revision: D20195310

Pulled By: ezyang

fbshipit-source-id: 14f879a8c43ea5efdbdf0f638792ea2b90011f4a",ankeshanand,https://api.github.com/repos/pytorch/pytorch/git/commits/45c45195cd6526fbb64773abd316a24bbe49f50d
2ce9d26809febe3a11edf99d50708783316975a7,"Support cdf for mixture_same_family distribution (#33408)

Summary:
The new added mixture_same_family should support cdf if the family has cdf implemented.

This is very useful for flow models where cdf of mixture of gassian/logistic is used to model flow
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33408

Differential Revision: D20191552

Pulled By: ezyang

fbshipit-source-id: 0bfd7973aa335c162919398a12ddec7425712297",buoyancy99,https://api.github.com/repos/pytorch/pytorch/git/commits/2ce9d26809febe3a11edf99d50708783316975a7
a23e8099ddf47de9719852ba2836df79db050a43,"Fix typo (#34008)

Summary:
This PR removes apparently unnecessary dots in the documentation of `torch.t`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/34008

Differential Revision: D20195084

Pulled By: ezyang

fbshipit-source-id: a34022de6b7a32d05a0bb3da197ee3507f4b8d8e",momohatt,https://api.github.com/repos/pytorch/pytorch/git/commits/a23e8099ddf47de9719852ba2836df79db050a43
